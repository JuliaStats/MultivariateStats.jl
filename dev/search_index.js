var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"Development","title":"API Reference","text":"","category":"section"},{"location":"api/#Current","page":"Development","title":"Current","text":"","category":"section"},{"location":"api/","page":"Development","title":"Development","text":"Table of the package models and corresponding function names used by these models.","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"Function \\ Model CCA WHT ICA LDA FA PPCA PCA KPCA MDS\nfit x x x x x x x x x\ntransform x x x x x x x x x\npredict    x     \nindim  x x x x x x x x\noutdim x x x x x x x x x\nmean x x x x x x x ? \nvar     x x ? ? ?\ncov     x ?   \ncor x        \nprojection x    x x x x x\nreconstruct     x x x x \nloadings ?   ? x x ? ? ?\neigvals     ? ? ? ? x\neigvecs     ? ? ? ? ?\nlength         \nsize         ","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"Note: ? refers to a possible implementation that is missing or called differently.","category":"page"},{"location":"api/#New","page":"Development","title":"New","text":"","category":"section"},{"location":"api/","page":"Development","title":"Development","text":"Function \\ Model WHT CCA LDA MC-LDA SS-LDA ICA FA PPCA PCA KPCA MDS\nfit x x x x x x x x x x x\ntransform x x - - - x x x x x x\npredict   x + +      \nindim -   - - x x x x x x\noutdim - x  - - x x x x x x\nmean x x  x x x x x x ? \nvar       x x ? ? ?\ncov       x ?   \ncor  x         \nprojection ? x  x x  x x x x x\nreconstruct       x x x x \nloadings  ?     x x ? ? ?\neigvals     +  ? ? ? ? x\neigvecs       ? ? ? ? ?\nlength +  + + +      \nsize +   + +      \n           ","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"StatsBase.AbstractDataTransform\nWhitening\nInterface: fit, transfrom\nNew: length, mean, size\nStatsBase.RegressionModel\nLinearDiscriminant\nMethods:\nInterface: fit, predict, coef, dof, weights\nNew: evaluate, length\nMulticlassLDA\nMethods: fit, predict, size, mean, projection\nNew: length\nSubspaceLDA\nMethods: fit, predict, size, mean, projection\nNew: length, eigvals\nCCA\nMethods: fit, transfrom, indim, outdim, mean\nSubtypes:\nAbstractDimensionalityReduction\nMethods: projection, var, reconstruct, loadings\nSubtypes:\nLinearDimensionalityReduction\nMethods: ICA, PCA\nNonlinearDimensionalityReduction\nMethods: KPCA, MDS\nLatentVariableModel or LatentVariableDimensionalityReduction\nMethods: FA, PPCA\nMethods: cov","category":"page"},{"location":"lda/#Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Linear Discriminant Analysis (LDA) are statistical analysis methods to find a linear combination of features for separating observations in two classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Note: Please refer to MulticlassLDA for methods that can discriminate between multiple classes.","category":"page"},{"location":"lda/#Overview-of-LDA","page":"Linear Discriminant Analysis","title":"Overview of LDA","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Suppose the samples in the positive and negative classes respectively with means: boldsymbolmu_p and boldsymbolmu_n, and covariances mathbfC_p and mathbfC_n. Then based on Fisher's Linear Discriminant Criteria, the optimal projection direction can be expressed as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfw = alpha cdot (mathbfC_p + mathbfC_n)^-1 (boldsymbolmu_p - boldsymbolmu_n)","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here alpha is an arbitrary non-negative coefficient.","category":"page"},{"location":"lda/#Linear-Discriminant-Analysis-2","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This package uses the LinearDiscriminant type to capture a linear discriminant functional:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"LinearDiscriminant","category":"page"},{"location":"lda/#MultivariateStats.LinearDiscriminant","page":"Linear Discriminant Analysis","title":"MultivariateStats.LinearDiscriminant","text":"A linear discriminant functional can be written as\n\n    f(mathbfx) = mathbfw^T mathbfx + b\n\nHere, w is the coefficient vector, and b is the bias constant.\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This type comes with several methods where f be an instance of  LinearDiscriminant.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{LinearDiscriminant}, Xp::DenseMatrix{T}, Xn::DenseMatrix{T}; kwargs) where T<:Real\nevaluate(::LinearDiscriminant, ::AbstractVector)\nevaluate(::LinearDiscriminant, ::AbstractMatrix)\npredict(::LinearDiscriminant, ::AbstractVector)\npredict(::LinearDiscriminant, ::AbstractMatrix)\ncoef(::LinearDiscriminant)\ndof(::LinearDiscriminant)\nweights(::LinearDiscriminant)\nlength(::LinearDiscriminant)","category":"page"},{"location":"lda/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{LinearDiscriminant}, DenseMatrix{T}, DenseMatrix{T}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsBase.fit","text":"fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\n\nPerforms LDA given both positive and negative samples. The function accepts follwing parameters:\n\nParameters\n\nXp: The sample matrix of the positive class.\nXn: The sample matrix of the negative class.\n\nKeyword arguments:\n\ncovestimator: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#). Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.evaluate-Tuple{LinearDiscriminant, AbstractVector{T} where T}","page":"Linear Discriminant Analysis","title":"MultivariateStats.evaluate","text":"evaluate(f, x::AbstractVector)\n\nEvaluate the linear discriminant value, i.e wx + b, it returns a real value.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.evaluate-Tuple{LinearDiscriminant, AbstractMatrix{T} where T}","page":"Linear Discriminant Analysis","title":"MultivariateStats.evaluate","text":"evaluate(f, X::AbstractMatrix)\n\nEvaluate the linear discriminant value, i.e wx + b, for each sample in columns of X. The function returns a vector of length size(X, 2).\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{LinearDiscriminant, AbstractVector{T} where T}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(f, x::AbstractVector)\n\nMake prediction for the vector x. It returns true iff evaluate(f, x) is positive.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{LinearDiscriminant, AbstractMatrix{T} where T}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(f, X::AbstractMatrix)\n\nMake predictions for the matrix X.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.coef-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsBase.coef","text":"coef(f::LinearDiscriminant)\n\nReturn the coefficients of the linear discriminant model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.dof-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsBase.dof","text":"dof(f::LinearDiscriminant)\n\nReturn the number of degrees of freedom in the linear discriminant model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.weights-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsBase.weights","text":"weights(f::LinearDiscriminant)\n\nReturn the linear discriminant model coefficient vector.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"Base.length","text":"Get the length of the coefficient vector.\n\n\n\n\n\n","category":"method"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Additional functionality:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"ldacov","category":"page"},{"location":"lda/#MultivariateStats.ldacov","page":"Linear Discriminant Analysis","title":"MultivariateStats.ldacov","text":"ldacov(C, μp, μn)\n\nPerforms LDA given a covariance matrix C and both mean vectors μp & μn.  Returns a linear discriminant functional of type LinearDiscriminant.\n\nParameters\n\nC: The pooled covariane matrix (i.e (Cp + Cn)2)\nμp: The mean vector of the positive class.\nμn: The mean vector of the negative class.\n\n\n\n\n\nldacov(Cp, Cn, μp, μn)\n\nPerforms LDA given covariances and mean vectors. Returns a linear discriminant functional of type LinearDiscriminant.\n\nParameters\n\nCp: The covariance matrix of the positive class.\nCn: The covariance matrix of the negative class.\nμp: The mean vector of the positive class.\nμn: The mean vector of the negative class.\n\nNote: The coefficient vector is scaled such that wμp + b = 1 and wμn + b = -1.\n\n\n\n\n\n","category":"function"},{"location":"lda/#Multi-class-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Multi-class Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Multi-class LDA is a generalization of standard two-class LDA that can handle arbitrary number of classes.","category":"page"},{"location":"lda/#Overview","page":"Linear Discriminant Analysis","title":"Overview","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Multi-class LDA is based on the analysis of two scatter matrices: within-class scatter matrix and between-class scatter matrix.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Given a set of samples mathbfx_1 ldots mathbfx_n, and their class labels y_1 ldots y_n:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The within-class scatter matrix is defined as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_w = sum_i=1^n (mathbfx_i - boldsymbolmu_y_i) (mathbfx_i - boldsymbolmu_y_i)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here, boldsymbolmu_k is the sample mean of the k-th class.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The between-class scatter matrix is defined as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b = sum_k=1^m n_k (boldsymbolmu_k - boldsymbolmu) (boldsymbolmu_k - boldsymbolmu)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here, m is the number of classes, boldsymbolmu is the overall sample mean, and n_k is the number of samples in the k-th class.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Then, multi-class LDA can be formulated as an optimization problem to find a set of linear combinations (with coefficients mathbfw) that maximizes the ratio of the between-class scattering to the within-class scattering, as","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"hatmathbfw = mathopmathrmargmax_mathbfw\n    fracmathbfw^T mathbfS_b mathbfwmathbfw^T mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The solution is given by the following generalized eigenvalue problem:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b mathbfw = lambda mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Generally, at most m - 1 generalized eigenvectors are useful to discriminate between m classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"When the dimensionality is high, it may not be feasible to construct the scatter matrices explicitly. In such cases, see SubspaceLDA below.","category":"page"},{"location":"lda/#Normalization-by-number-of-observations","page":"Linear Discriminant Analysis","title":"Normalization by number of observations","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"An alternative definition of the within- and between-class scatter matrices normalizes for the number of observations in each group:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_w^* = n sum_k=1^m frac1n_k sum_i mid y_i=k (mathbfx_i - boldsymbolmu_k) (mathbfx_i - boldsymbolmu_k)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b^* = n sum_k=1^m (boldsymbolmu_k - boldsymbolmu^*) (boldsymbolmu_k - boldsymbolmu^*)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"where","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"boldsymbolmu^* = frac1k sum_k=1^m boldsymbolmu_k","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This definition can sometimes be more useful when looking for directions which discriminate among clusters containing widely-varying numbers of observations.","category":"page"},{"location":"lda/#Multi-class-LDA","page":"Linear Discriminant Analysis","title":"Multi-class LDA","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package defines a MulticlassLDA type to represent a multi-class LDA model, as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"MulticlassLDA","category":"page"},{"location":"lda/#MultivariateStats.MulticlassLDA","page":"Linear Discriminant Analysis","title":"MultivariateStats.MulticlassLDA","text":"A multi-class linear discriminant model type has following fields:\n\nproj is the projection matrix\npmeans is the projected means of all classes\nstats is an instance of MulticlassLDAStats type that captures all statistics computed to train the model (which we will discuss later).\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Several methods are provided to access properties of the LDA model. Let M be an instance of MulticlassLDA:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{MulticlassLDA}, ::Int, ::DenseMatrix{T}, ::AbstractVector{Int}; kwargs...) where T<:Real\npredict(::MulticlassLDA, ::AbstractVecOrMat{<:Real})\nmean(::MulticlassLDA)\nsize(::MulticlassLDA)\nlength(::MulticlassLDA)","category":"page"},{"location":"lda/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{MulticlassLDA}, Int64, DenseMatrix{T}, AbstractVector{Int64}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsBase.fit","text":"fit(MulticlassLDA, nc, X, y; ...)\n\nPerform multi-class LDA over a given data set X and collecttion of labels y.\n\nThis function returns the resultant multi-class LDA model as an instance of MulticlassLDA.\n\nParameters\n\nnc:  the number of classes\nX:   the matrix of input samples, of size (d, n). Each column in X is an observation.\ny:   the vector of class labels, of length n. Each element of y must be an integer between 1 and nc.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:gevd: based on generalized eigenvalue decomposition (default).\n:whiten: first derive a whitening transform from Sw and then solve the problem based on eigenvalue\ndecomposition of the whiten Sb.\noutdim: The output dimension, i.e. dimension of the transformed space min(d, nc-1)\nregcoef: The regularization coefficient (default: 1.0e-6). A positive value regcoef * eigmax(Sw)   is added to the diagonal of Sw to improve numerical stability.\ncovestimator_between: Custom covariance estimator for between-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\ncovestimator_within:  Custom covariance estimator for within-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n\nNotes:\n\nThe resultant projection matrix P satisfies:\n\nmathbfP^T (mathbfS_w + kappa mathbfI) mathbfP = mathbfI\n\nHere, kappa equals regcoef * eigmax(Sw). The columns of P are arranged in descending order of the corresponding generalized eigenvalues.\n\nNote that MulticlassLDA does not currently support the normalized version using mathbfS_w^* and mathbfS_b^* (see SubspaceLDA).\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{MulticlassLDA, AbstractVecOrMat{var\"#s1\"} where var\"#s1\"<:Real}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(M, x)\n\nTransform input sample(s) in x to the output space of MC-LDA model M. Here, x can be either a sample vector or a matrix comprised of samples in columns.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Statistics.mean-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Statistics.mean","text":"mean(M::MulticlassLDA)\n\nGet the overall sample mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.size-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Base.size","text":"size(M)\n\nGet the input (i.e the dimension of the observation space) and output (i.e the dimension of the transformed features) dimensions of the model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(M)\n\nGet the sample dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Subspace-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Subspace Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package also defines a SubspaceLDA type to represent a multi-class LDA model for high-dimensional spaces. MulticlassLDA, because it stores the scatter matrices, is not well-suited for high-dimensional data. For example, if you are performing LDA on images, and each image has 10^6 pixels, then the scatter matrices would contain 10^12 elements, far too many to store directly. SubspaceLDA calculates the projection direction without the intermediary of the scatter matrices, by focusing on the subspace that lies within the span of the within-class scatter. This also serves to regularize the computation.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"SubspaceLDA supports all the same methods as MulticlassLDA, with the exception of the functions that return a scatter matrix.  The overall projection is represented as a factorization P*L, where P*x projects data points to the subspace spanned by the within-class scatter, and L is the LDA projection in the subspace.  The projection directions w (the columns of projection(M)) satisfy the equation","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"   mathbfP^T mathbfS_b mathbfw = lambda mathbfP^T mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"When P is of full rank (e.g., if there are more data points than dimensions), then this equation guarantees that mathbfS_b mathbfw = lambda mathbfS_w mathbfw will also hold.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package defines a SubspaceLDA type to represent a multi-class LDA model, as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"SubspaceLDA","category":"page"},{"location":"lda/#MultivariateStats.SubspaceLDA","page":"Linear Discriminant Analysis","title":"MultivariateStats.SubspaceLDA","text":"Subspace LDA model type has following fields:\n\nprojw: the projection matrix of the subspace spanned by the between-class scatter\nprojLDA: the projection matrix of the subspace spanned by the within-class scatter\nλ: the projection eigenvalues\ncmeans: the class centroids\ncweights: the class weights\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Several methods are provided to access properties of the LDA model. Let M be an instance of SubspaceLDA:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{SubspaceLDA}, ::DenseMatrix{T}, ::AbstractVector{Int}, ::Int; kwargs...) where T<:Real\npredict(::SubspaceLDA, ::AbstractVecOrMat{<:Real})\nmean(::SubspaceLDA)\nsize(::SubspaceLDA)\nlength(::SubspaceLDA)\neigvals(::SubspaceLDA)","category":"page"},{"location":"lda/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{SubspaceLDA}, DenseMatrix{T}, AbstractVector{Int64}, Int64}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsBase.fit","text":"fit(SubspaceLDA, X, labels; normalize=true)\n\nFit an subspace projection of LDA model using the equivalent of mathbfS_w^* and mathbfS_b^*`.\n\nNote: Subspace LDA also supports the normalized version of LDA via the normalize keyword.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{SubspaceLDA, AbstractVecOrMat{var\"#s1\"} where var\"#s1\"<:Real}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(M, x)\n\nTransform input sample(s) in x to the output space of LDA model M. Here, x can be either a sample vector or a matrix comprised of samples in columns.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.size-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Base.size","text":"size(M)\n\nGet the input (i.e the dimension of the observation space) and output (i.e the dimension of the subspace projection) dimensions of the model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(M)\n\nGet dimension of the LDA model.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Data-Transformation","page":"Data Transformation","title":"Data Transformation","text":"","category":"section"},{"location":"whiten/#Whitening","page":"Data Transformation","title":"Whitening","text":"","category":"section"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"A whitening transformation is a decorrelation transformation that transforms a set of random variables into a set of new random variables with identity covariance (uncorrelated with unit variances).","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"In particular, suppose a random vector has covariance mathbfC, then a whitening transform mathbfW is one that satisfy:","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"   mathbfW^T mathbfC mathbfW = mathbfI","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Note that mathbfW is generally not unique. In particular, if mathbfW is a whitening transform, so is any of its rotation mathbfW mathbfR with mathbfR^T mathbfR = mathbfI.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"The package uses Whitening to represent a whitening transform.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Whitening","category":"page"},{"location":"whiten/#MultivariateStats.Whitening","page":"Data Transformation","title":"MultivariateStats.Whitening","text":"A whitening transform representation.\n\n\n\n\n\n","category":"type"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Whitening transformation can be fitted to data using the fit method.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"fit(::Type{Whitening}, X::AbstractMatrix{T}; kwargs...) where {T<:Real}\nMultivariateStats.transform(::Whitening, ::AbstractVecOrMat{<:Real})\nlength(::Whitening)\nmean(::Whitening)\nsize(::Whitening)","category":"page"},{"location":"whiten/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{Whitening}, AbstractMatrix{T}}} where T<:Real","page":"Data Transformation","title":"StatsBase.fit","text":"fit(Whitening, X::AbstractMatrix{T}; kwargs...)\n\nEstimate a whitening transform from the data given in X.\n\nThis function returns an instance of Whitening\n\nKeyword Arguments:\n\nregcoef: The regularization coefficient. The covariance will be regularized as follows when regcoef is positive C + (eigmax(C) * regcoef) * eye(d). Default values is zero(T).\ndims: if 1 the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if 2 the transformation calculated from the column samples. The default is nothing, which is equivalent to dims=2 with a deprecation warning.\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNote: This function internally relies on cov_whitening to derive the transformation W.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#MultivariateStats.transform-Tuple{Whitening, AbstractVecOrMat{var\"#s1\"} where var\"#s1\"<:Real}","page":"Data Transformation","title":"MultivariateStats.transform","text":"transform(f, x)\n\nApply the whitening transform f to a vector or a matrix x with samples in columns, as mathbfW^T (mathbfx - boldsymbolmu).\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Base.length-Tuple{Whitening}","page":"Data Transformation","title":"Base.length","text":"length(f)\n\nGet the dimension of the  whitening transform f.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Statistics.mean-Tuple{Whitening}","page":"Data Transformation","title":"Statistics.mean","text":"mean(f)\n\nGet the mean vector of the whitening transformation f.\n\nNote: if mean is empty, this function returns a zero vector of length(f).\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Base.size-Tuple{Whitening}","page":"Data Transformation","title":"Base.size","text":"size(f)\n\nDimensions of the coefficient matrix of the whitening transform f.\n\n\n\n\n\n","category":"method"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Additional methods","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"cov_whitening\ncov_whitening!","category":"page"},{"location":"whiten/#MultivariateStats.cov_whitening","page":"Data Transformation","title":"MultivariateStats.cov_whitening","text":"cov_whitening(C)\n\nDerive the whitening transform coefficient matrix W given the covariance matrix C. Here, C can be either a square matrix, or an instance of Cholesky.\n\nInternally, this function solves the whitening transform using Cholesky factorization. The rationale is as follows: let mathbfC = mathbfU^T mathbfU and mathbfW = mathbfU^-1, then mathbfW^T mathbfC mathbfW = mathbfI.\n\nNote: The return matrix W is an upper triangular matrix.\n\n\n\n\n\ncov_whitening(C, regcoef)\n\nDerive a whitening transform based on a regularized covariance, as C + (eigmax(C) * regcoef) * eye(d).\n\n\n\n\n\n","category":"function"},{"location":"whiten/#MultivariateStats.cov_whitening!","page":"Data Transformation","title":"MultivariateStats.cov_whitening!","text":"cov_whitening!(C)\n\nIn-place version of cov_whitening(C), in which the input matrix C will be overwritten during computation. This can be more efficient when C is no longer used.\n\n\n\n\n\ncov_whitening!(C, regcoef)\n\nIn-place version of cov_whitening(C, regcoef), in which the input matrix C will be overwritten during computation. This can be more efficient when C is no longer used.\n\n\n\n\n\n","category":"function"},{"location":"#MultivariateStats.jl-Documentation","page":"Home","title":"MultivariateStats.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MultivariateStats\nDocTestSetup = quote\n    using Statistics\n    using Random\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultivariateStats.jl is a Julia package for multivariate statistical analysis. It provides a rich set of useful analysis techniques, such as PCA, CCA, LDA, ICA, etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"whiten.md\", \"lda.md\", \"api.md\"]\nDepth = 2","category":"page"},{"location":"","page":"Home","title":"Home","text":"Notes: All methods implemented in this package adopt the column-major convention of JuliaStats: in a data matrix, each column corresponds to a sample/observation, while each row corresponds to a feature (variable or attribute).","category":"page"}]
}
