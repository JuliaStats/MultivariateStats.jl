var documenterSearchIndex = {"docs":
[{"location":"pca/#Principal-Component-Analysis","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Principal Component Analysis (PCA) derives an orthogonal projection to convert a given set of observations to linearly uncorrelated variables, called principal components.","category":"page"},{"location":"pca/#Example","page":"Principal Component Analysis","title":"Example","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"using Plots\ngr(fmt=:svg)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Performing PCA on Iris data set:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"using MultivariateStats, RDatasets, Plots\n\n# load iris dataset\niris = dataset(\"datasets\", \"iris\")\n\n# split half to training set\nXtr = Matrix(iris[1:2:end,1:4])'\nXtr_labels = Vector(iris[1:2:end,5])\n\n# split other half to testing set\nXte = Matrix(iris[2:2:end,1:4])'\nXte_labels = Vector(iris[2:2:end,5])\nnothing # hide","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Suppose Xtr and Xte are training and testing data matrix, with each observation in a column. We train a PCA model, allowing up to 3 dimensions:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"M = fit(PCA, Xtr; maxoutdim=3)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Then, apply PCA model to the testing set","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Yte = predict(M, Xte)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"And, reconstruct testing observations (approximately) to the original space","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Xr = reconstruct(M, Yte)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Now, we group results by testing set labels for color coding and visualize first 3 principal components in 3D plot","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"setosa = Yte[:,Xte_labels.==\"setosa\"]\nversicolor = Yte[:,Xte_labels.==\"versicolor\"]\nvirginica = Yte[:,Xte_labels.==\"virginica\"]\n\np = scatter(setosa[1,:],setosa[2,:],setosa[3,:],marker=:circle,linewidth=0)\nscatter!(versicolor[1,:],versicolor[2,:],versicolor[3,:],marker=:circle,linewidth=0)\nscatter!(virginica[1,:],virginica[2,:],virginica[3,:],marker=:circle,linewidth=0)\nplot!(p,xlabel=\"PC1\",ylabel=\"PC2\",zlabel=\"PC3\")","category":"page"},{"location":"pca/#Linear-Principal-Component-Analysis","page":"Principal Component Analysis","title":"Linear Principal Component Analysis","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This package uses the PCA type to define a linear PCA model:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"PCA","category":"page"},{"location":"pca/#MultivariateStats.PCA","page":"Principal Component Analysis","title":"MultivariateStats.PCA","text":"Linear Principal Component Analysis\n\n\n\n\n\n","category":"type"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This type comes with several methods where M be an instance of  PCA, d be the dimension of observations, and p be the output dimension (i.e the dimension of the principal subspace).","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"fit(::Type{PCA}, ::AbstractMatrix{T}; kwargs) where {T<:Real}\npredict(::PCA, ::AbstractVecOrMat{T}) where {T<:Real}\nreconstruct(::PCA, ::AbstractVecOrMat{T}) where {T<:Real}\nsize(::PCA)\nmean(::PCA)\nprojection(::PCA)\nvar(::PCA)\nprincipalvars(::PCA)\ntprincipalvar(::PCA)\ntresidualvar(::PCA)\nr2(::PCA)\nloadings(::PCA)\neigvals(::PCA)\neigvecs(::PCA)","category":"page"},{"location":"pca/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{PCA}, AbstractMatrix{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsAPI.fit","text":"fit(PCA, X; ...)\n\nPerform PCA over the data given in a matrix X. Each column of X is an observation.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:auto: use :cov when d < n or :svd otherwise (default).\n:cov: based on covariance matrix decomposition.\n:svd: based on SVD of the input data.\nmaxoutdim: The output dimension, i.e. dimension of the transformed space (min(d, nc-1))\npratio: The ratio of variances preserved in the principal subspace (0.99)\nmean: The mean vector, which can be either of\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNotes:\n\nThe output dimension p depends on both maxoutdim and pratio, as follows. Suppose the first k principal components preserve at least pratio of the total variance, while the first k-1 preserves less than pratio, then the actual output dimension will be min(k maxoutdim).\nThis function calls pcacov or pcasvd internally, depending on the choice of method.\n\n\n\n\n\n","category":"method"},{"location":"pca/#StatsAPI.predict-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsAPI.predict","text":"predict(M::PCA, x::AbstractVecOrMat{<:Real})\n\nGiven a PCA model M, transform observations x into principal components space, as\n\nmathbfy = mathbfP^T (mathbfx - boldsymbolmu)\n\nHere, x can be either a vector of length d or a matrix where each column is an observation, and \\mathbf{P} is the projection matrix.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"MultivariateStats.reconstruct","text":"reconstruct(M::PCA, y::AbstractVecOrMat{<:Real})\n\nGiven a PCA model M, returns a (approximately) reconstructed observations from principal components space, as\n\ntildemathbfx = mathbfP mathbfy + boldsymbolmu\n\nHere, y can be either a vector of length p or a matrix where each column gives the principal components for an observation, and mathbfP is the projection matrix.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Base.size-Tuple{PCA}","page":"Principal Component Analysis","title":"Base.size","text":"size(M)\n\nReturns a tuple with the dimensions of input (the dimension of the observation space) and output (the dimension of the principal subspace).\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.mean-Tuple{PCA}","page":"Principal Component Analysis","title":"Statistics.mean","text":"mean(M::PCA)\n\nReturns the mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.projection-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.projection","text":"projection(M::PCA)\n\nReturns the projection matrix (of size (d, p)). Each column of the projection matrix corresponds to a principal component. The principal components are arranged in descending order of the corresponding variances.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.var-Tuple{PCA}","page":"Principal Component Analysis","title":"Statistics.var","text":"var(M::PCA)\n\nReturns the total observation variance, which is equal to tprincipalvar(M) + tresidualvar(M).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.principalvars-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.principalvars","text":"principalvars(M::PCA)\n\nReturns the variances of principal components.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.tprincipalvar-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.tprincipalvar","text":"tprincipalvar(M::PCA)\n\nReturns the total variance of principal components, which is equal to sum(principalvars(M)).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.tresidualvar-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.tresidualvar","text":"tresidualvar(M::PCA)\n\nReturns the total residual variance.\n\n\n\n\n\n","category":"method"},{"location":"pca/#StatsAPI.r2-Tuple{PCA}","page":"Principal Component Analysis","title":"StatsAPI.r2","text":"r2(M::PCA)\nprincipalratio(M::PCA)\n\nReturns the ratio of variance preserved in the principal subspace, which is equal to tprincipalvar(M) / var(M).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.loadings-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.loadings","text":"loadings(M::PCA)\n\nReturns model loadings, i.e. the weights for each original variable when calculating the principal component.\n\n\n\n\n\n","category":"method"},{"location":"pca/#LinearAlgebra.eigvals-Tuple{PCA}","page":"Principal Component Analysis","title":"LinearAlgebra.eigvals","text":"eigvals(M::PCA)\n\nGet the eigenvalues of the PCA model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/#LinearAlgebra.eigvecs-Tuple{PCA}","page":"Principal Component Analysis","title":"LinearAlgebra.eigvecs","text":"eigvecs(M::PCA)\n\nGet the eigenvalues of the PCA model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Auxiliary functions:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"pcacov\npcasvd","category":"page"},{"location":"pca/#MultivariateStats.pcacov","page":"Principal Component Analysis","title":"MultivariateStats.pcacov","text":"pcacov(C, mean; ...)\n\nCompute and return a PCA model based on eigenvalue decomposition of a given covariance matrix C.\n\nParameters:\n\nC: The covariance matrix of the samples.\nmean: The mean vector of original samples, which can be a vector of length d,          or an empty vector Float64[] indicating a zero mean.\n\nNote: This function accepts two keyword arguments: maxoutdim and pratio.\n\n\n\n\n\n","category":"function"},{"location":"pca/#MultivariateStats.pcasvd","page":"Principal Component Analysis","title":"MultivariateStats.pcasvd","text":"pcasvd(Z, mean, tw; ...)\n\nCompute and return a PCA model based on singular value decomposition of a centralized sample matrix Z.\n\nParameters:\n\nZ: a matrix of centralized samples.\nmean: The mean vector of the original samples, which can be a vector of length d,         or an empty vector Float64[] indicating a zero mean.\nn: a number of samples.\n\nNote: This function accepts two keyword arguments: maxoutdim and pratio.\n\n\n\n\n\n","category":"function"},{"location":"pca/#Kernel-Principal-Component-Analysis","page":"Principal Component Analysis","title":"Kernel Principal Component Analysis","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Kernel Principal Component Analysis (kernel PCA) is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This package defines a KernelPCA type to represent a kernel PCA model.","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"KernelPCA","category":"page"},{"location":"pca/#MultivariateStats.KernelPCA","page":"Principal Component Analysis","title":"MultivariateStats.KernelPCA","text":"This type contains kernel PCA model parameters.\n\n\n\n\n\n","category":"type"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"The package provides a set of methods to access the properties of the kernel PCA model. Let M be an instance of KernelPCA, d be the dimension of observations, and p be the output dimension (i.e the dimension of the principal subspace).","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"fit(::Type{KernelPCA}, ::AbstractMatrix{T}; kwargs...) where {T<:Real}\npredict(::KernelPCA)\npredict(::KernelPCA, ::AbstractVecOrMat{T}) where {T<:Real}\nreconstruct(::KernelPCA, ::AbstractVecOrMat{T}) where {T<:Real}\nsize(::KernelPCA)\nprojection(::KernelPCA)\neigvals(::KernelPCA)\neigvecs(::KernelPCA)","category":"page"},{"location":"pca/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{KernelPCA}, AbstractMatrix{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsAPI.fit","text":"fit(KernelPCA, X; ...)\n\nPerform kernel PCA over the data given in a matrix X. Each column of X is an observation.\n\nThis method returns an instance of KernelPCA.\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\nkernel: The kernel function. This functions accepts two vector arguments x and y,\n\nand returns a scalar value (default: (x,y)->x'y). If set to nothing, the matrix X is the pre-computed symmetric kernel (Gram) matrix.\n\nsolver: The choice of solver:\n:eig: uses LinearAlgebra.eigen (default)\n:eigs: uses Arpack.eigs (always used for sparse data)\nmaxoutdim:  Maximum output dimension (default min(d, n))\ninverse: Whether to perform calculation for inverse transform for non-precomputed kernels (default false)\nβ: Hyperparameter of the ridge regression that learns the inverse transform (default 1 when inverse is true).\ntol: Convergence tolerance for eigs solver (default 0.0)\nmaxiter: Maximum number of iterations for eigs solver (default 300)\n\n\n\n\n\n","category":"method"},{"location":"pca/#StatsAPI.predict-Tuple{KernelPCA}","page":"Principal Component Analysis","title":"StatsAPI.predict","text":"predict(M::KernelPCA)\n\nTransform the data fitted to the model M to a kernel space of the model.\n\n\n\n\n\n","category":"method"},{"location":"pca/#StatsAPI.predict-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsAPI.predict","text":"predict(M::KernelPCA, x)\n\nTransform out-of-sample transformation of x into a kernel space of the model M.\n\nHere, x can be either a vector of length d or a matrix where each column is an observation.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"MultivariateStats.reconstruct","text":"reconstruct(M::KernelPCA, y)\n\nApproximately reconstruct observations, given in y, to the original space using the kernel PCA model M.\n\nHere, y can be either a vector of length p or a matrix where each column gives the principal components for an observation.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Base.size-Tuple{KernelPCA}","page":"Principal Component Analysis","title":"Base.size","text":"size(M::KernelPCA)\n\nReturns a tuple with the input dimension d, i.e the dimension of the observation space, and the output dimension p, i.e the dimension of the principal subspace.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.projection-Tuple{KernelPCA}","page":"Principal Component Analysis","title":"MultivariateStats.projection","text":"projection(M::KernelPCA)\n\nReturn the projection matrix (of size n times p). Each column of the projection matrix corresponds to an eigenvector, and n is a number of observations.\n\nThe principal components are arranged in descending order of the corresponding eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"pca/#LinearAlgebra.eigvals-Tuple{KernelPCA}","page":"Principal Component Analysis","title":"LinearAlgebra.eigvals","text":"eigvals(M::KernelPCA)\n\nReturn eigenvalues of the kernel matrix of the model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/#LinearAlgebra.eigvecs-Tuple{KernelPCA}","page":"Principal Component Analysis","title":"LinearAlgebra.eigvecs","text":"eigvecs(M::KernelPCA)\n\nReturn eigenvectors of the kernel matrix of the model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Kernels","page":"Principal Component Analysis","title":"Kernels","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"List of the commonly used kernels:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"function description\n(x,y)->x'y Linear\n(x,y)->(x'y+c)^d Polynomial\n(x,y)->exp(-γ*norm(x-y)^2.0) Radial basis function (RBF)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This package has a separate interface for adjusting kernel matrices.","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"MultivariateStats.KernelCenter\nfit(::Type{MultivariateStats.KernelCenter}, ::AbstractMatrix{<:Real})\nMultivariateStats.transform!(::MultivariateStats.KernelCenter, ::AbstractMatrix{<:Real})","category":"page"},{"location":"pca/#MultivariateStats.KernelCenter","page":"Principal Component Analysis","title":"MultivariateStats.KernelCenter","text":"Center a kernel matrix\n\n\n\n\n\n","category":"type"},{"location":"pca/#StatsAPI.fit-Tuple{Type{MultivariateStats.KernelCenter}, AbstractMatrix{<:Real}}","page":"Principal Component Analysis","title":"StatsAPI.fit","text":"Fit KernelCenter object\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.transform!-Tuple{MultivariateStats.KernelCenter, AbstractMatrix{<:Real}}","page":"Principal Component Analysis","title":"MultivariateStats.transform!","text":"Center kernel matrix.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Probabilistic-Principal-Component-Analysis","page":"Principal Component Analysis","title":"Probabilistic Principal Component Analysis","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Probabilistic Principal Component Analysis (PPCA) represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. It is expressed as the maximum likelihood solution of a probabilistic latent variable model[1].","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This package defines a PPCA type to represent a probabilistic PCA model, and provides a set of methods to access the properties.","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"PPCA","category":"page"},{"location":"pca/#MultivariateStats.PPCA","page":"Principal Component Analysis","title":"MultivariateStats.PPCA","text":"This type contains probabilistic PCA model parameters.\n\n\n\n\n\n","category":"type"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Let M be an instance of PPCA, d be the dimension of observations, and p be the output dimension (i.e the dimension of the principal subspace).","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"fit\nsize(::PPCA)\nmean(::PPCA)\nvar(::PPCA)\ncov(::PPCA)\nprojection(::PPCA)\nloadings(::PPCA)","category":"page"},{"location":"pca/#StatsAPI.fit","page":"Principal Component Analysis","title":"StatsAPI.fit","text":"fit(Whitening, X::AbstractMatrix{T}; kwargs...)\n\nEstimate a whitening transform from the data given in X.\n\nThis function returns an instance of Whitening\n\nKeyword Arguments:\n\nregcoef: The regularization coefficient. The covariance will be regularized as follows when regcoef is positive C + (eigmax(C) * regcoef) * eye(d). Default values is zero(T).\ndims: if 1 the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if 2 the transformation calculated from the column samples. The default is nothing, which is equivalent to dims=2 with a deprecation warning.\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNote: This function internally relies on cov_whitening to derive the transformation W.\n\n\n\n\n\nfit(PCA, X; ...)\n\nPerform PCA over the data given in a matrix X. Each column of X is an observation.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:auto: use :cov when d < n or :svd otherwise (default).\n:cov: based on covariance matrix decomposition.\n:svd: based on SVD of the input data.\nmaxoutdim: The output dimension, i.e. dimension of the transformed space (min(d, nc-1))\npratio: The ratio of variances preserved in the principal subspace (0.99)\nmean: The mean vector, which can be either of\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNotes:\n\nThe output dimension p depends on both maxoutdim and pratio, as follows. Suppose the first k principal components preserve at least pratio of the total variance, while the first k-1 preserves less than pratio, then the actual output dimension will be min(k maxoutdim).\nThis function calls pcacov or pcasvd internally, depending on the choice of method.\n\n\n\n\n\nfit(PPCA, X; ...)\n\nPerform probabilistic PCA over the data given in a matrix X. Each column of X is an observation. This method returns an instance of PPCA.\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\nmethod: The choice of methods:\n:ml: use maximum likelihood version of probabilistic PCA (default)\n:em: use EM version of probabilistic PCA\n:bayes: use Bayesian PCA\nmaxoutdim: Maximum output dimension (default d-1)\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\ntol: Convergence tolerance (default 1.0e-6)\nmaxiter: Maximum number of iterations (default 1000)\n\nNotes: This function calls ppcaml, ppcaem or bayespca internally, depending on the choice of method.\n\n\n\n\n\nFit KernelCenter object\n\n\n\n\n\nfit(KernelPCA, X; ...)\n\nPerform kernel PCA over the data given in a matrix X. Each column of X is an observation.\n\nThis method returns an instance of KernelPCA.\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\nkernel: The kernel function. This functions accepts two vector arguments x and y,\n\nand returns a scalar value (default: (x,y)->x'y). If set to nothing, the matrix X is the pre-computed symmetric kernel (Gram) matrix.\n\nsolver: The choice of solver:\n:eig: uses LinearAlgebra.eigen (default)\n:eigs: uses Arpack.eigs (always used for sparse data)\nmaxoutdim:  Maximum output dimension (default min(d, n))\ninverse: Whether to perform calculation for inverse transform for non-precomputed kernels (default false)\nβ: Hyperparameter of the ridge regression that learns the inverse transform (default 1 when inverse is true).\ntol: Convergence tolerance for eigs solver (default 0.0)\nmaxiter: Maximum number of iterations for eigs solver (default 300)\n\n\n\n\n\nfit(CCA, X, Y; ...)\n\nPerform CCA over the data given in matrices X and Y. Each column of X and Y is an observation.\n\nX and Y should have the same number of columns (denoted by n below).\n\nThis method returns an instance of CCA.\n\nKeyword arguments:\n\nmethod: The choice of methods:\n:cov: based on covariance matrices\n:svd: based on SVD of the input data (default)\noutdim: The output dimension, i.e dimension of the common space (default: min(dx, dy, n))\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNotes: This function calls ccacov or ccasvd internally, depending on the choice of method.\n\n\n\n\n\nfit(MDS, X; kwargs...)\n\nCompute an embedding of X points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument distances:\n\nmds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\n\nwhere X is the data matrix. Distances between pairs of columns of X are computed using the Euclidean norm. This is equivalent to performing PCA on X.\n\nmds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\n\nwhere D is a symmetric matrix D of distances between points.\n\n\n\n\n\nfit(MetricMDS, X; kwargs...)\n\nCompute an embedding of X points by (non)metric multidimensional scaling (MDS).\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\ndistances: The choice of input (required):\nfalse: use X to calculate dissimilarity matrix using Euclidean distance\ntrue: use X input as precomputed dissimilarity symmetric matrix (distances)\nmaxoutdim: Maximum output dimension (default d-1)\nmetric : a function for calculation of disparity values\nnothing: use dissimilarity values as the disparities to perform the metric MDS (default)\nisotonic: converts dissimilarity values to ordinal disparities to perform non-metric MDS\nany two parameter disparity transformation function, where the first parameter is a vector of proximities (i.e. dissimilarities) and the second parameter is a vector of distances, e.g. (p,d)->b*p for some b is a transformation function for ratio MDS.\ntol: Convergence tolerance (default 1.0e-3)\nmaxiter: Maximum number of iterations (default 300)\ninitial: an initial reduced space point configuration\nnothing: then an initial configuration is randomly generated (default)\npre-defined matrix\nweights: a weight matrix\nnothing: then weights are set to one, w_ij = 1 (default)\npre-defined matrix\n\nNote: if the algorithm is unable to converge then ConvergenceException is thrown.\n\n\n\n\n\nfit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\n\nPerforms LDA given both positive and negative samples. The function accepts following parameters:\n\nParameters\n\nXp: The sample matrix of the positive class.\nXn: The sample matrix of the negative class.\n\nKeyword arguments:\n\ncovestimator: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#). Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.\n\n\n\n\n\nfit(MulticlassLDA, X, y; ...)\n\nPerform multi-class LDA over a given data set X with corresponding labels y with nc number of classes.\n\nThis function returns the resultant multi-class LDA model as an instance of MulticlassLDA.\n\nParameters\n\nX:   the matrix of input samples, of size (d, n). Each column in X is an observation.\ny:   the vector of class labels, of length n.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:gevd: based on generalized eigenvalue decomposition (default).\n:whiten: first derive a whitening transform from Sw and then solve the problem based on eigenvalue\ndecomposition of the whiten Sb.\noutdim: The output dimension, i.e. dimension of the transformed space min(d, nc-1)\nregcoef: The regularization coefficient (default: 1.0e-6). A positive value regcoef * eigmax(Sw)   is added to the diagonal of Sw to improve numerical stability.\ncovestimator_between: Custom covariance estimator for between-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\ncovestimator_within:  Custom covariance estimator for within-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n\nNotes:\n\nThe resultant projection matrix P satisfies:\n\nmathbfP^T (mathbfS_w + kappa mathbfI) mathbfP = mathbfI\n\nHere, kappa equals regcoef * eigmax(Sw). The columns of mathbfP are arranged in descending order of the corresponding generalized eigenvalues.\n\nNote that MulticlassLDA does not currently support the normalized version using mathbfS_w^* and mathbfS_b^* (see SubspaceLDA).\n\n\n\n\n\nfit(SubspaceLDA, X, y; normalize=true)\n\nFit an subspace projection of LDA model over a given data set X with corresponding labels y using the equivalent of mathbfS_w^* and mathbfS_b^*`.\n\nNote: Subspace LDA also supports the normalized version of LDA via the normalize keyword.\n\n\n\n\n\nfit(ICA, X, k; ...)\n\nPerform ICA over the data set given in X.\n\nParameters: -X: The data matrix, of size (m n). Each row corresponds to a mixed signal, while each column corresponds to an observation (e.g all signal value at a particular time step). -k: The number of independent components to recover.\n\nKeyword Arguments:\n\nalg: The choice of algorithm (default :fastica)\nfun: The approx neg-entropy functor (default Tanh)\ndo_whiten: Whether to perform pre-whitening (default true)\nmaxiter: Maximum number of iterations (default 100)\ntol: Tolerable change of W at convergence (default 1.0e-6)\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\nwinit: Initial guess of W, which should be either of:\nempty matrix: the function will perform random initialization (default)\na matrix of size (k k) (when do_whiten)\na matrix of size (m k) (when !do_whiten)\n\nReturns the resultant ICA model, an instance of type ICA.\n\nNote: If do_whiten is true, the return W satisfies mathbfW^T mathbfC mathbfW = mathbfI, otherwise W is orthonormal, i.e mathbfW^T mathbfW = mathbfI.\n\n\n\n\n\nfit(FactorAnalysis, X; ...)\n\nPerform factor analysis over the data given in a matrix X. Each column of X is an observation. This method returns an instance of FactorAnalysis.\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\nmethod: The choice of methods:\n:em: use EM version of factor analysis\n:cm: use CM version of factor analysis (default)\nmaxoutdim: Maximum output dimension (default d-1)\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\ntol: Convergence tolerance (default 1.0e-6)\nmaxiter: Maximum number of iterations (default 1000)\nη: Variance low bound (default 1.0e-6)\n\nNotes: This function calls facm or faem internally, depending on the choice of method.\n\n\n\n\n\n","category":"function"},{"location":"pca/#Base.size-Tuple{PPCA}","page":"Principal Component Analysis","title":"Base.size","text":"size(M::PPCA)\n\nReturns a tuple with values of the input dimension d, i.e the dimension of the observation space, and the output dimension p, i.e the dimension of the principal subspace.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.mean-Tuple{PPCA}","page":"Principal Component Analysis","title":"Statistics.mean","text":"mean(M::PPCA)\n\nGet the mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.var-Tuple{PPCA}","page":"Principal Component Analysis","title":"Statistics.var","text":"var(M::PPCA)\n\nReturns the total residual variance of the model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.cov-Tuple{PPCA}","page":"Principal Component Analysis","title":"Statistics.cov","text":"cov(M::PPCA)\n\nReturns the covariance of the model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.projection-Tuple{PPCA}","page":"Principal Component Analysis","title":"MultivariateStats.projection","text":"projection(M::PPCA)\n\nReturns the projection matrix (of size (d p)). Each column of the projection matrix corresponds to a principal component.\n\nThe principal components are arranged in descending order of the corresponding variances.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.loadings-Tuple{PPCA}","page":"Principal Component Analysis","title":"MultivariateStats.loadings","text":"loadings(M::PPCA)\n\nReturns the factor loadings matrix (of size (d p)) of the model M.\n\n\n\n\n\n","category":"method"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Given a probabilistic PCA model M, one can use it to transform observations into latent variables, as","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"\\mathbf{z} = (\\mathbf{W}^T \\mathbf{W} + \\sigma^2 \\mathbf{I}) \\mathbf{W}^T (\\mathbf{x} - \\boldsymbol{\\mu})","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"or use it to reconstruct (approximately) the observations from latent variables, as","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"\\tilde{\\mathbf{x}} = \\mathbf{W} \\mathbb{E}[\\mathbf{z}] + \\boldsymbol{\\mu}","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Here, mathbfW is the factor loadings or weight matrix.","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"predict(::PPCA, ::AbstractVecOrMat{T}) where {T<:Real}\nreconstruct(::PPCA, ::AbstractVecOrMat{T}) where {T<:Real}","category":"page"},{"location":"pca/#StatsAPI.predict-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsAPI.predict","text":"predict(M::PPCA, x)\n\nTransform observations x into latent variables. Here, x can be either a vector of length d or a matrix where each column is an observation.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"MultivariateStats.reconstruct","text":"reconstruct(M::PPCA, z)\n\nApproximately reconstruct observations from the latent variable given in z. Here, z can be either a vector of length p or a matrix where each column gives the latent variables for an observation.\n\n\n\n\n\n","category":"method"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Auxiliary functions:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"ppcaml\nppcaem\nbayespca","category":"page"},{"location":"pca/#MultivariateStats.ppcaml","page":"Principal Component Analysis","title":"MultivariateStats.ppcaml","text":"ppcaml(Z, mean; ...)\n\nCompute probabilistic PCA using on maximum likelihood formulation for a centralized sample matrix Z.\n\nParameters:\n\nZ: a centralized samples matrix\nmean: The mean vector of the original samples, which can be a vector of\n\nlength d, or an empty vector indicating a zero mean.\n\nReturns the resultant PPCA model.\n\nNote: This function accepts two keyword arguments: maxoutdim and tol.\n\n\n\n\n\n","category":"function"},{"location":"pca/#MultivariateStats.ppcaem","page":"Principal Component Analysis","title":"MultivariateStats.ppcaem","text":"ppcaem(S, mean, n; ...)\n\nCompute probabilistic PCA based on expectation-maximization algorithm for a given sample covariance matrix S.\n\nParameters:\n\nS: The sample covariance matrix.\nmean: The mean vector of original samples, which can be a vector of length d,\n\nor an empty vector indicating a zero mean.\n\nn: The number of observations.\n\nReturns the resultant PPCA model.\n\nNote: This function accepts three keyword arguments: maxoutdim, tol, and maxiter.\n\n\n\n\n\n","category":"function"},{"location":"pca/#MultivariateStats.bayespca","page":"Principal Component Analysis","title":"MultivariateStats.bayespca","text":"bayespca(S, mean, n; ...)\n\nCompute probabilistic PCA using a Bayesian algorithm for a given sample covariance matrix S.\n\nParameters:\n\nS: The sample covariance matrix.\nmean: The mean vector of original samples, which can be a vector of length d,\n\nor an empty vector indicating a zero mean.\n\nn: The number of observations.\n\nReturns the resultant PPCA model.\n\nNotes:\n\nThis function accepts three keyword arguments: maxoutdim, tol, and maxiter.\nFunction uses the maxoutdim parameter as an upper boundary when it automatically\n\ndetermines the latent space dimensionality.\n\n\n\n\n\n","category":"function"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"","category":"page"},{"location":"pca/#References","page":"Principal Component Analysis","title":"References","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"[1]: Bishop, C. M. Pattern Recognition and Machine Learning, 2006.","category":"page"},{"location":"fa/#Factor-Analysis","page":"Factor Analysis","title":"Factor Analysis","text":"","category":"section"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"Factor Analysis (FA) is a linear-Gaussian latent variable model that is closely related to probabilistic PCA. In contrast to the probabilistic PCA model, the covariance of conditional distribution of the observed variable  given the latent variable is diagonal rather than isotropic[1].","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"This package defines a FactorAnalysis type to represent a factor analysis model, and provides a set of methods to access the properties.","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"FactorAnalysis","category":"page"},{"location":"fa/#MultivariateStats.FactorAnalysis","page":"Factor Analysis","title":"MultivariateStats.FactorAnalysis","text":"This type contains factor analysis model parameters.\n\n\n\n\n\n","category":"type"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"The package provides a set of methods to access the properties of the factor analysis model. Let M be an instance of FactorAnalysis, d be the dimension of observations, and p be the output dimension (i.e the dimension of the principal subspace).","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"fit(::Type{FactorAnalysis}, ::AbstractMatrix{T}) where {T<:Real}\nsize(::FactorAnalysis)\nmean(::FactorAnalysis)\nvar(::FactorAnalysis)\ncov(::FactorAnalysis)\nprojection(::FactorAnalysis)\nloadings(::FactorAnalysis)","category":"page"},{"location":"fa/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{FactorAnalysis}, AbstractMatrix{T}}} where T<:Real","page":"Factor Analysis","title":"StatsAPI.fit","text":"fit(FactorAnalysis, X; ...)\n\nPerform factor analysis over the data given in a matrix X. Each column of X is an observation. This method returns an instance of FactorAnalysis.\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\nmethod: The choice of methods:\n:em: use EM version of factor analysis\n:cm: use CM version of factor analysis (default)\nmaxoutdim: Maximum output dimension (default d-1)\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\ntol: Convergence tolerance (default 1.0e-6)\nmaxiter: Maximum number of iterations (default 1000)\nη: Variance low bound (default 1.0e-6)\n\nNotes: This function calls facm or faem internally, depending on the choice of method.\n\n\n\n\n\n","category":"method"},{"location":"fa/#Base.size-Tuple{FactorAnalysis}","page":"Factor Analysis","title":"Base.size","text":"size(M::FactorAnalysis)\n\nReturns a tuple with values of the input dimension d, i.e the dimension of the observation space, and the output dimension p, i.e the dimension of the principal subspace.\n\n\n\n\n\n","category":"method"},{"location":"fa/#Statistics.mean-Tuple{FactorAnalysis}","page":"Factor Analysis","title":"Statistics.mean","text":"mean(M::FactorAnalysis)\n\nGet the mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"fa/#Statistics.var-Tuple{FactorAnalysis}","page":"Factor Analysis","title":"Statistics.var","text":"var(M::FactorAnalysis)\n\nReturns the variance of the model M.\n\n\n\n\n\n","category":"method"},{"location":"fa/#Statistics.cov-Tuple{FactorAnalysis}","page":"Factor Analysis","title":"Statistics.cov","text":"cov(M::FactorAnalysis)\n\nReturns the covariance of the model M.\n\n\n\n\n\n","category":"method"},{"location":"fa/#MultivariateStats.projection-Tuple{FactorAnalysis}","page":"Factor Analysis","title":"MultivariateStats.projection","text":"projection(M::FactorAnalysis)\n\nRecovers principle components from the weight matrix of the model M.\n\n\n\n\n\n","category":"method"},{"location":"fa/#MultivariateStats.loadings-Tuple{FactorAnalysis}","page":"Factor Analysis","title":"MultivariateStats.loadings","text":"loadings(M::FactorAnalysis)\n\nReturns the factor loadings matrix of the model M.\n\n\n\n\n\n","category":"method"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"Given a factor analysis model M, one can use it to transform observations into latent variables, as","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"\\mathbf{z} =  \\mathbf{W}^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"or use it to reconstruct (approximately) the observations from latent variables, as","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"\\tilde{\\mathbf{x}} = \\mathbf{\\Sigma} \\mathbf{W} (\\mathbf{W}^T \\mathbf{W})^{-1} \\mathbf{z} + \\boldsymbol{\\mu}","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"Here, mathbfW is the factor loadings or weight matrix, mathbfSigma = mathbfPsi + mathbfW^T mathbfW is the covariance matrix.","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"The package provides methods to do so:","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"predict(::FactorAnalysis, ::AbstractVecOrMat{T}) where {T<:Real}\nreconstruct(::FactorAnalysis, ::AbstractVecOrMat{T}) where {T<:Real}","category":"page"},{"location":"fa/#StatsAPI.predict-Union{Tuple{T}, Tuple{FactorAnalysis, AbstractVecOrMat{T}}} where T<:Real","page":"Factor Analysis","title":"StatsAPI.predict","text":"predict(M::FactorAnalysis, x)\n\nTransform observations x into latent variables. Here, x can be either a vector of length d or a matrix where each column is an observation.\n\n\n\n\n\n","category":"method"},{"location":"fa/#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{FactorAnalysis, AbstractVecOrMat{T}}} where T<:Real","page":"Factor Analysis","title":"MultivariateStats.reconstruct","text":"reconstruct(M::FactorAnalysis, z)\n\nApproximately reconstruct observations from the latent variable given in z. Here, z can be either a vector of length p or a matrix where each column gives the latent variables for an observation.\n\n\n\n\n\n","category":"method"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"Auxiliary functions:","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"faem\nfacm","category":"page"},{"location":"fa/#MultivariateStats.faem","page":"Factor Analysis","title":"MultivariateStats.faem","text":"faem(S, mean, n; ...)\n\nPerforms factor analysis using an expectation-maximization algorithm for a given sample covariance matrix S[2].\n\nParameters\n\nS: The sample covariance matrix.\nmean: The mean vector of original samples, which can be a vector of length d,\n\nor an empty vector indicating a zero mean.\n\nn: The number of observations.\n\nReturns the resultant FactorAnalysis model.\n\nNote: This function accepts two keyword arguments: maxoutdim,tol, and maxiter.\n\n\n\n\n\n","category":"function"},{"location":"fa/#MultivariateStats.facm","page":"Factor Analysis","title":"MultivariateStats.facm","text":"facm(S, mean, n; ...)\n\nPerforms factor analysis using a fast conditional maximization algorithm for a given sample covariance matrix S[3].\n\nParameters\n\nS: The sample covariance matrix.\nmean: The mean vector of original samples, which can be a vector of length d,\n\nor an empty vector indicating a zero mean.\n\nn: The number of observations.\n\nReturns the resultant FactorAnalysis model.\n\nNote: This function accepts two keyword arguments: maxoutdim,tol, maxiter, and η.\n\n\n\n\n\n","category":"function"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"","category":"page"},{"location":"fa/#References","page":"Factor Analysis","title":"References","text":"","category":"section"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"[1]: Bishop, C. M. Pattern Recognition and Machine Learning, 2006.","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"[2]: Rubin, Donald B., and Dorothy T. Thayer. EM algorithms for ML factor analysis. Psychometrika 47.1, 69-76, 1982.","category":"page"},{"location":"fa/","page":"Factor Analysis","title":"Factor Analysis","text":"[3]: Zhao, J-H., Philip LH Yu, and Qibao Jiang. ML estimation for factor analysis: EM or non-EM?. Statistics and computing 18.2, 109-123, 2008.","category":"page"},{"location":"api/#API-Reference","page":"Development","title":"API Reference","text":"","category":"section"},{"location":"api/#Current","page":"Development","title":"Current","text":"","category":"section"},{"location":"api/","page":"Development","title":"Development","text":"Table of the package models and corresponding function names used by these models.","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"Function \\ Model CCA WHT ICA LDA FA PPCA PCA KPCA MDS\nfit x x x x x x x x x\ntransform x x x x x x x x x\npredict    x     \nindim  x x x x x x x x\noutdim x x x x x x x x x\nmean x x x x x x x ? \nvar     x x ? ? ?\ncov     x ?   \ncor x        \nprojection x    x x x x x\nreconstruct     x x x x \nloadings ?   ? x x ? ? ?\neigvals     ? ? ? ? x\neigvecs     ? ? ? ? ?\nlength         \nsize         ","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"Note: ? refers to a possible implementation that is missing or called differently.","category":"page"},{"location":"api/#New","page":"Development","title":"New","text":"","category":"section"},{"location":"api/","page":"Development","title":"Development","text":"Function \\ Model WHT CCA LDA MC-LDA SS-LDA ICA FA PPCA PCA KPCA MDS\nfit x x x x x x x x x x x\ntransform x - - - - - x x - - -\npredict  + x + + +   + + +\nindim -   - - - x x - - -\noutdim - -  - - - x x - - -\nmean x x  x x x x x x  \nvar       x x x  ?\ncov       x x   \ncor  +         \nprojection ? x  x x  x x x x x\nreconstruct       x x x x \nloadings       x x x  +\neigvals     +  ? ? x x x\neigvecs       ? ? x + +\nlength +  + + +      \nsize + +  + + +   x + +\n           ","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"StatsBase.AbstractDataTransform\nWhitening\nInterface: fit, transform\nNew: length, mean, size\nStatsBase.RegressionModel\nInterface: fit, predict\nLinearDiscriminant\nFunctions: coef, dof, weights, evaluate, length\nMulticlassLDA\nFunctions: size, mean, projection, length\nSubspaceLDA\nFunctions: size, mean, projection, length, eigvals\nCCA\nFunctions: size, mean, projection, predict, cor\nSubtypes:\nAbstractDimensionalityReduction\nInterface: projection, var, reconstruct, loadings\nInterface: projection == weights\nSubtypes:\nLinearDimensionalityReduction\nMethods: ICA, PCA\nNonlinearDimensionalityReduction\nMethods: KPCA, MDS\nFunctions: modelmatrix(X),\nLatentVariableModel or LatentVariableDimensionalityReduction\nMethods: FA, PPCA\nFunctions: cov","category":"page"},{"location":"lda/#Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Linear Discriminant Analysis (LDA) are statistical analysis methods to find a linear combination of features for separating observations in two classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Note: Please refer to MulticlassLDA for methods that can discriminate between multiple classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Suppose the samples in the positive and negative classes respectively with means: boldsymbolmu_p and boldsymbolmu_n, and covariances mathbfC_p and mathbfC_n. Then based on Fisher's Linear Discriminant Criteria, the optimal projection direction can be expressed as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfw = alpha cdot (mathbfC_p + mathbfC_n)^-1 (boldsymbolmu_p - boldsymbolmu_n)","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here alpha is an arbitrary non-negative coefficient.","category":"page"},{"location":"lda/#Example","page":"Linear Discriminant Analysis","title":"Example","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"using Plots\ngr(fmt=:svg)","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Let's compare LDA and PCA 2D projection of Iris dataset. Principal Component Analysis identifies the directions that explain the most variance in the data. ","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"using MultivariateStats, RDatasets\n\niris = dataset(\"datasets\", \"iris\")\n\nX = Matrix(iris[1:2:end,1:4])'\nX_labels = Vector(iris[1:2:end,5])\n\npca = fit(PCA, X; maxoutdim=2)\nYpca = predict(pca, X)\nnothing # hide","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"In contrast, Linear Discriminant Analysis attempts to identify the attributes that account for the most variance between classes. Therefore, with LDA, you must use known class labels.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"lda = fit(MulticlassLDA, X, X_labels; outdim=2)\nYlda = predict(lda, X)\n\np = plot(layout=(1,2), size=(800,300))\n\nfor s in [\"setosa\", \"versicolor\", \"virginica\"]\n\n    points = Ypca[:,X_labels.==s]\n    scatter!(p[1], points[1,:],points[2,:], label=s, legend=:bottomleft)\n    points = Ylda[:,X_labels.==s]\n    scatter!(p[2], points[1,:],points[2,:], label=s, legend=:bottomleft)\n\nend\nplot!(p[1], title=\"PCA\")\nplot!(p[2], title=\"LDA\")","category":"page"},{"location":"lda/#Two-class-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Two-class Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This package uses the LinearDiscriminant type to capture a linear discriminant functional:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"LinearDiscriminant","category":"page"},{"location":"lda/#MultivariateStats.LinearDiscriminant","page":"Linear Discriminant Analysis","title":"MultivariateStats.LinearDiscriminant","text":"A linear discriminant functional can be written as\n\n    f(mathbfx) = mathbfw^T mathbfx + b\n\nHere, mathbfw is the coefficient vector, and b is the bias constant.\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This type comes with several methods where f be an instance of  LinearDiscriminant.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{LinearDiscriminant}, Xp::DenseMatrix{T}, Xn::DenseMatrix{T}; kwargs) where T<:Real\nevaluate(::LinearDiscriminant, ::AbstractVector)\nevaluate(::LinearDiscriminant, ::AbstractMatrix)\npredict(::LinearDiscriminant, ::AbstractVector)\npredict(::LinearDiscriminant, ::AbstractMatrix)\ncoef(::LinearDiscriminant)\ndof(::LinearDiscriminant)\nweights(::LinearDiscriminant)\nlength(::LinearDiscriminant)","category":"page"},{"location":"lda/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{LinearDiscriminant}, DenseMatrix{T}, DenseMatrix{T}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsAPI.fit","text":"fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\n\nPerforms LDA given both positive and negative samples. The function accepts following parameters:\n\nParameters\n\nXp: The sample matrix of the positive class.\nXn: The sample matrix of the negative class.\n\nKeyword arguments:\n\ncovestimator: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#). Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.evaluate-Tuple{LinearDiscriminant, AbstractVector}","page":"Linear Discriminant Analysis","title":"MultivariateStats.evaluate","text":"evaluate(f, x::AbstractVector)\n\nEvaluate the linear discriminant value, i.e mathbfwx + b, it returns a real value.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.evaluate-Tuple{LinearDiscriminant, AbstractMatrix}","page":"Linear Discriminant Analysis","title":"MultivariateStats.evaluate","text":"evaluate(f, X::AbstractMatrix)\n\nEvaluate the linear discriminant value, i.e mathbfwx + b, for each sample in columns of X. The function returns a vector of length size(X, 2).\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.predict-Tuple{LinearDiscriminant, AbstractVector}","page":"Linear Discriminant Analysis","title":"StatsAPI.predict","text":"predict(f, x::AbstractVector)\n\nMake prediction for the vector x. It returns true iff evaluate(f, x) is positive.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.predict-Tuple{LinearDiscriminant, AbstractMatrix}","page":"Linear Discriminant Analysis","title":"StatsAPI.predict","text":"predict(f, X::AbstractMatrix)\n\nMake predictions for the matrix X.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.coef-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsAPI.coef","text":"coef(f::LinearDiscriminant)\n\nReturn the coefficients of the linear discriminant model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.dof-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsAPI.dof","text":"dof(f::LinearDiscriminant)\n\nReturn the number of degrees of freedom in the linear discriminant model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.weights-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsAPI.weights","text":"weights(f::LinearDiscriminant)\n\nReturn the linear discriminant model coefficient vector.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(f::LinearDiscriminant)\n\nGet the length of the coefficient vector.\n\n\n\n\n\n","category":"method"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Additional functionality:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"ldacov","category":"page"},{"location":"lda/#MultivariateStats.ldacov","page":"Linear Discriminant Analysis","title":"MultivariateStats.ldacov","text":"ldacov(C, μp, μn)\n\nPerforms LDA given a covariance matrix C and both mean vectors μp & μn.  Returns a linear discriminant functional of type LinearDiscriminant.\n\nParameters\n\nC: The pooled covariance matrix (i.e (mathbfC_p + mathbfC_n)2)\nμp: The mean vector of the positive class.\nμn: The mean vector of the negative class.\n\n\n\n\n\nldacov(Cp, Cn, μp, μn)\n\nPerforms LDA given covariances and mean vectors. Returns a linear discriminant functional of type LinearDiscriminant.\n\nParameters\n\nCp: The covariance matrix of the positive class.\nCn: The covariance matrix of the negative class.\nμp: The mean vector of the positive class.\nμn: The mean vector of the negative class.\n\nNote: The coefficient vector is scaled such that mathbfwboldsymbolμ_p + b = 1 and mathbfwboldsymbolμ_n + b = -1.\n\n\n\n\n\n","category":"function"},{"location":"lda/#Multi-class-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Multi-class Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Multi-class LDA is a generalization of standard two-class LDA that can handle arbitrary number of classes.","category":"page"},{"location":"lda/#Overview","page":"Linear Discriminant Analysis","title":"Overview","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Multi-class LDA is based on the analysis of two scatter matrices: within-class scatter matrix and between-class scatter matrix.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Given a set of samples mathbfx_1 ldots mathbfx_n, and their class labels y_1 ldots y_n:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The within-class scatter matrix is defined as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_w = sum_i=1^n (mathbfx_i - boldsymbolmu_y_i) (mathbfx_i - boldsymbolmu_y_i)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here, boldsymbolmu_k is the sample mean of the k-th class.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The between-class scatter matrix is defined as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b = sum_k=1^m n_k (boldsymbolmu_k - boldsymbolmu) (boldsymbolmu_k - boldsymbolmu)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here, m is the number of classes, boldsymbolmu is the overall sample mean, and n_k is the number of samples in the k-th class.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Then, multi-class LDA can be formulated as an optimization problem to find a set of linear combinations (with coefficients mathbfw) that maximizes the ratio of the between-class scattering to the within-class scattering, as","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"hatmathbfw = mathopmathrmargmax_mathbfw\n    fracmathbfw^T mathbfS_b mathbfwmathbfw^T mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The solution is given by the following generalized eigenvalue problem:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b mathbfw = lambda mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Generally, at most m - 1 generalized eigenvectors are useful to discriminate between m classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"When the dimensionality is high, it may not be feasible to construct the scatter matrices explicitly. In such cases, see SubspaceLDA below.","category":"page"},{"location":"lda/#Normalization-by-number-of-observations","page":"Linear Discriminant Analysis","title":"Normalization by number of observations","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"An alternative definition of the within- and between-class scatter matrices normalizes for the number of observations in each group:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_w^* = n sum_k=1^m frac1n_k sum_i mid y_i=k (mathbfx_i - boldsymbolmu_k) (mathbfx_i - boldsymbolmu_k)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b^* = n sum_k=1^m (boldsymbolmu_k - boldsymbolmu^*) (boldsymbolmu_k - boldsymbolmu^*)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"where","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"boldsymbolmu^* = frac1k sum_k=1^m boldsymbolmu_k","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This definition can sometimes be more useful when looking for directions which discriminate among clusters containing widely-varying numbers of observations.","category":"page"},{"location":"lda/#Multi-class-LDA","page":"Linear Discriminant Analysis","title":"Multi-class LDA","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package defines a MulticlassLDA type to represent a multi-class LDA model, as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"MulticlassLDA\nMulticlassLDAStats","category":"page"},{"location":"lda/#MultivariateStats.MulticlassLDA","page":"Linear Discriminant Analysis","title":"MultivariateStats.MulticlassLDA","text":"A multi-class linear discriminant model type has following fields:\n\nproj is the projection matrix\npmeans is the projected means of all classes\nstats is an instance of MulticlassLDAStats type that captures all statistics computed to train the model (which we will discuss later).\n\n\n\n\n\n","category":"type"},{"location":"lda/#MultivariateStats.MulticlassLDAStats","page":"Linear Discriminant Analysis","title":"MultivariateStats.MulticlassLDAStats","text":"Resulting statistics of the multi-class LDA evaluation.\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Several methods are provided to access properties of the LDA model. Let M be an instance of MulticlassLDA:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{MulticlassLDA}, ::AbstractMatrix{T}, ::AbstractVector; kwargs...) where T<:Real\npredict(::MulticlassLDA, ::AbstractVecOrMat{T}) where {T<:Real}\nmean(::MulticlassLDA)\nsize(::MulticlassLDA)\nlength(::MulticlassLDA)\nclassmeans(::MulticlassLDA)\nclassweights(::MulticlassLDA)\nwithclass_scatter(::MulticlassLDA)\nbetweenclass_scatter(::MulticlassLDA)\nprojection(::MulticlassLDA)","category":"page"},{"location":"lda/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{MulticlassLDA}, AbstractMatrix{T}, AbstractVector}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsAPI.fit","text":"fit(MulticlassLDA, X, y; ...)\n\nPerform multi-class LDA over a given data set X with corresponding labels y with nc number of classes.\n\nThis function returns the resultant multi-class LDA model as an instance of MulticlassLDA.\n\nParameters\n\nX:   the matrix of input samples, of size (d, n). Each column in X is an observation.\ny:   the vector of class labels, of length n.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:gevd: based on generalized eigenvalue decomposition (default).\n:whiten: first derive a whitening transform from Sw and then solve the problem based on eigenvalue\ndecomposition of the whiten Sb.\noutdim: The output dimension, i.e. dimension of the transformed space min(d, nc-1)\nregcoef: The regularization coefficient (default: 1.0e-6). A positive value regcoef * eigmax(Sw)   is added to the diagonal of Sw to improve numerical stability.\ncovestimator_between: Custom covariance estimator for between-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\ncovestimator_within:  Custom covariance estimator for within-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n\nNotes:\n\nThe resultant projection matrix P satisfies:\n\nmathbfP^T (mathbfS_w + kappa mathbfI) mathbfP = mathbfI\n\nHere, kappa equals regcoef * eigmax(Sw). The columns of mathbfP are arranged in descending order of the corresponding generalized eigenvalues.\n\nNote that MulticlassLDA does not currently support the normalized version using mathbfS_w^* and mathbfS_b^* (see SubspaceLDA).\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.predict-Union{Tuple{T}, Tuple{MulticlassLDA, AbstractVecOrMat{T}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsAPI.predict","text":"predict(M::MulticlassLDA, x)\n\nTransform input sample(s) in x to the output space of MC-LDA model M. Here, x can be either a sample vector or a matrix comprised of samples in columns.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Statistics.mean-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Statistics.mean","text":"mean(M::MulticlassLDA)\n\nGet the overall sample mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.size-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Base.size","text":"size(M::MulticlassLDA)\n\nGet the input (i.e the dimension of the observation space) and output (i.e the dimension of the transformed features) dimensions of the model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(M::MulticlassLDA)\n\nGet the sample dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.classmeans-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"MultivariateStats.classmeans","text":"classmeans(M)\n\nGet the matrix comprised of class-specific means as columns (of size d  m).\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.classweights-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"MultivariateStats.classweights","text":"classweights(M)\n\nGet the weights of individual classes (a vector of length m). If the samples are not weighted, the weight equals the number of samples of each class.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.withclass_scatter-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"MultivariateStats.withclass_scatter","text":"withinclass_scatter(M)\n\nGet the within-class scatter matrix (of size d  d).\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.betweenclass_scatter-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"MultivariateStats.betweenclass_scatter","text":"betweenclass_scatter(M)\n\nGet the between-class scatter matrix (of size d  d).\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.projection-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"MultivariateStats.projection","text":"projection(M::MulticlassLDA)\n\nGet the projection matrix (of size d  p).\n\n\n\n\n\n","category":"method"},{"location":"lda/#Subspace-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Subspace Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package also defines a SubspaceLDA type to represent a multi-class LDA model for high-dimensional spaces. MulticlassLDA, because it stores the scatter matrices, is not well-suited for high-dimensional data. For example, if you are performing LDA on images, and each image has 10^6 pixels, then the scatter matrices would contain 10^12 elements, far too many to store directly. SubspaceLDA calculates the projection direction without the intermediary of the scatter matrices, by focusing on the subspace that lies within the span of the within-class scatter. This also serves to regularize the computation.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"SubspaceLDA supports all the same methods as MulticlassLDA, with the exception of the functions that return a scatter matrix.  The overall projection is represented as a factorization P*L, where P*x projects data points to the subspace spanned by the within-class scatter, and L is the LDA projection in the subspace.  The projection directions w (the columns of projection(M)) satisfy the equation","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"   mathbfP^T mathbfS_b mathbfw = lambda mathbfP^T mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"When P is of full rank (e.g., if there are more data points than dimensions), then this equation guarantees that mathbfS_b mathbfw = lambda mathbfS_w mathbfw will also hold.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package defines a SubspaceLDA type to represent a multi-class LDA model, as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"SubspaceLDA","category":"page"},{"location":"lda/#MultivariateStats.SubspaceLDA","page":"Linear Discriminant Analysis","title":"MultivariateStats.SubspaceLDA","text":"Subspace LDA model type has following fields:\n\nprojw: the projection matrix of the subspace spanned by the between-class scatter\nprojLDA: the projection matrix of the subspace spanned by the within-class scatter\nλ: the projection eigenvalues\ncmeans: the class centroids\ncweights: the class weights\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Several methods are provided to access properties of the LDA model. Let M be an instance of SubspaceLDA:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{SubspaceLDA}, ::AbstractMatrix{T}, ::AbstractVector; kwargs...) where T<:Real\npredict(::SubspaceLDA, ::AbstractVecOrMat{T}) where {T<:Real}\nmean(::SubspaceLDA)\nprojection(::SubspaceLDA)\nsize(::SubspaceLDA)\nlength(::SubspaceLDA)\neigvals(::SubspaceLDA)","category":"page"},{"location":"lda/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{SubspaceLDA}, AbstractMatrix{T}, AbstractVector}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsAPI.fit","text":"fit(SubspaceLDA, X, y; normalize=true)\n\nFit an subspace projection of LDA model over a given data set X with corresponding labels y using the equivalent of mathbfS_w^* and mathbfS_b^*`.\n\nNote: Subspace LDA also supports the normalized version of LDA via the normalize keyword.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsAPI.predict-Union{Tuple{T}, Tuple{SubspaceLDA, AbstractVecOrMat{T}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsAPI.predict","text":"predict(M::SubspaceLDA, x)\n\nTransform input sample(s) in x to the output space of LDA model M. Here, x can be either a sample vector or a matrix comprised of samples in columns.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Statistics.mean-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Statistics.mean","text":"mean(M::SubspaceLDA)\n\nReturns the mean vector of the subspace LDA model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.projection-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"MultivariateStats.projection","text":"projection(M)\n\nGet the projection matrix.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.size-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Base.size","text":"size(M)\n\nGet the input (i.e the dimension of the observation space) and output (i.e the dimension of the subspace projection) dimensions of the model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(M)\n\nGet dimension of the LDA model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#LinearAlgebra.eigvals-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"LinearAlgebra.eigvals","text":"eigvals(M::SubspaceLDA)\n\nGet the eigenvalues of the subspace LDA model M.\n\n\n\n\n\n","category":"method"},{"location":"cca/#Canonical-Correlation-Analysis","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"","category":"section"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"Canonical Correlation Analysis(CCA) is a statistical analysis technique to identify correlations between two sets of variables. Given two vector variables X and Y, it finds two projections, one for each, to transform them to a common space with maximum correlations.","category":"page"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"The package defines a CCA type to represent a CCA model, and provides a set of methods to access the properties.","category":"page"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"CCA","category":"page"},{"location":"cca/#MultivariateStats.CCA","page":"Canonical Correlation Analysis","title":"MultivariateStats.CCA","text":"Canonical Correlation Analysis Model\n\n\n\n\n\n","category":"type"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"Let M be an instance of CCA, dx be the dimension of X, dy the dimension of Y, and p the output dimension (i.e the dimension of the common space).","category":"page"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"fit(::Type{CCA}, ::AbstractMatrix{T}, ::AbstractMatrix{T}) where {T<:Real}\nsize(::CCA)\nmean(::CCA, ::Symbol)\nprojection(::CCA, ::Symbol)\ncor(::CCA)\npredict(::CCA, ::AbstractVecOrMat{<:Real}, ::Symbol)","category":"page"},{"location":"cca/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{CCA}, AbstractMatrix{T}, AbstractMatrix{T}}} where T<:Real","page":"Canonical Correlation Analysis","title":"StatsAPI.fit","text":"fit(CCA, X, Y; ...)\n\nPerform CCA over the data given in matrices X and Y. Each column of X and Y is an observation.\n\nX and Y should have the same number of columns (denoted by n below).\n\nThis method returns an instance of CCA.\n\nKeyword arguments:\n\nmethod: The choice of methods:\n:cov: based on covariance matrices\n:svd: based on SVD of the input data (default)\noutdim: The output dimension, i.e dimension of the common space (default: min(dx, dy, n))\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNotes: This function calls ccacov or ccasvd internally, depending on the choice of method.\n\n\n\n\n\n","category":"method"},{"location":"cca/#Base.size-Tuple{CCA}","page":"Canonical Correlation Analysis","title":"Base.size","text":"size(M:CCA)\n\nReturn a tuple with the dimension of X, Y, and the output dimension.\n\n\n\n\n\n","category":"method"},{"location":"cca/#Statistics.mean-Tuple{CCA, Symbol}","page":"Canonical Correlation Analysis","title":"Statistics.mean","text":"mean(M::CCA, c::Symbol)\n\nGet the mean vector for the component c of the model M. The component parameter can be :x or :y.\n\n\n\n\n\n","category":"method"},{"location":"cca/#MultivariateStats.projection-Tuple{CCA, Symbol}","page":"Canonical Correlation Analysis","title":"MultivariateStats.projection","text":"projection(M::CCA, c::Symbol)\n\nGet the projection matrix for the component c of the model M. The component parameter can be :x or :y.\n\n\n\n\n\n","category":"method"},{"location":"cca/#Statistics.cor-Tuple{CCA}","page":"Canonical Correlation Analysis","title":"Statistics.cor","text":"cor(M::CCA)\n\nThe correlations of the projected components (a vector of length p).\n\n\n\n\n\n","category":"method"},{"location":"cca/#StatsAPI.predict-Tuple{CCA, AbstractVecOrMat{<:Real}, Symbol}","page":"Canonical Correlation Analysis","title":"StatsAPI.predict","text":"predict(M::CCA, Z::AbstractVecOrMat{<:Real}, c::Symbol)\n\nGiven a CCA model, one can transform observations into both spaces into a common space, as\n\nmathbfz_x = mathbfP_x^T (mathbfx - boldsymbolmu_x) \nmathbfz_y = mathbfP_y^T (mathbfy - boldsymbolmu_y)\n\nHere, mathbfP_x and mathbfP_y are projection matrices for X and Y; boldsymbolmu_x and boldsymbolmu_y are mean vectors.\n\nParameter Z can be either a vector of length dx, dy, or a matrix where each column is an observation. The component parameter c can be :x or :y.\n\n\n\n\n\n","category":"method"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"Auxiliary functions:","category":"page"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"ccacov\nccasvd","category":"page"},{"location":"cca/#MultivariateStats.ccacov","page":"Canonical Correlation Analysis","title":"MultivariateStats.ccacov","text":"ccacov(Cxx, Cyy, Cxy, xmean, ymean, p)\n\nCompute CCA based on analysis of the given covariance matrices, using generalized eigenvalue decomposition, and return CCA model.\n\nParameters:\n\nCxx: The covariance matrix of X.\nCyy: The covariance matrix of Y.\nCxy: The covariance matrix between X and Y.\nxmean: The mean vector of the original samples of X, which can be\n\na vector of length dx, or an empty vector indicating a zero mean.\n\nymean: The mean vector of the original samples of Y, which can be\n\na vector of length dy, or an empty vector indicating a zero mean.\n\np: The output dimension, i.e the dimension of the common space.\n\n\n\n\n\n","category":"function"},{"location":"cca/#MultivariateStats.ccasvd","page":"Canonical Correlation Analysis","title":"MultivariateStats.ccasvd","text":"ccasvd(Zx, Zy, xmean, ymean, p)\n\nCompute CCA based on singular value decomposition of centralized sample matrices Zx and Zy, and return CCA model[1].\n\nParameters:\n\nZx: The centralized sample matrix for X.\nZy: The centralized sample matrix for Y.\nxmean: The mean vector of the original samples of X, which can be\n\na vector of length dx, or an empty vector indicating a zero mean.\n\nymean: The mean vector of the original samples of Y, which can be\n\na vector of length dy, or an empty vector indicating a zero mean.\n\np: The output dimension, i.e the dimension of the common space.\n\n\n\n\n\n","category":"function"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"","category":"page"},{"location":"cca/#References","page":"Canonical Correlation Analysis","title":"References","text":"","category":"section"},{"location":"cca/","page":"Canonical Correlation Analysis","title":"Canonical Correlation Analysis","text":"[1]: David Weenink, Canonical Correlation Analysis, Institute of Phonetic Sciences, Univ. of Amsterdam, Proceedings 25, 81-99, 2003.","category":"page"},{"location":"ica/#Independent-Component-Analysis","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"","category":"section"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"Independent Component Analysis (ICA) is a computational technique for separating a multivariate signal into additive subcomponents, with the assumption that the subcomponents are non-Gaussian and independent from each other.","category":"page"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"There are multiple algorithms for ICA. Currently, this package implements the Fast ICA algorithm.","category":"page"},{"location":"ica/#FastICA","page":"Independent Component Analysis","title":"FastICA","text":"","category":"section"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"This package implements the FastICA algorithm[1]. The package uses the ICA type to define a FastICA model:","category":"page"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"ICA","category":"page"},{"location":"ica/#MultivariateStats.ICA","page":"Independent Component Analysis","title":"MultivariateStats.ICA","text":"This type contains ICA model parameters: mean and component matrix W.\n\nNote: Each column of the component matrix W corresponds to an independent component.\n\n\n\n\n\n","category":"type"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"Several methods are provided to work with ICA. Let M be an instance of ICA:","category":"page"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"fit(::Type{ICA}, ::AbstractMatrix{T}, ::Int) where {T<:Real}\nsize(::ICA)\nmean(::ICA)\npredict(::ICA, ::AbstractVecOrMat{<:Real})","category":"page"},{"location":"ica/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{ICA}, AbstractMatrix{T}, Int64}} where T<:Real","page":"Independent Component Analysis","title":"StatsAPI.fit","text":"fit(ICA, X, k; ...)\n\nPerform ICA over the data set given in X.\n\nParameters: -X: The data matrix, of size (m n). Each row corresponds to a mixed signal, while each column corresponds to an observation (e.g all signal value at a particular time step). -k: The number of independent components to recover.\n\nKeyword Arguments:\n\nalg: The choice of algorithm (default :fastica)\nfun: The approx neg-entropy functor (default Tanh)\ndo_whiten: Whether to perform pre-whitening (default true)\nmaxiter: Maximum number of iterations (default 100)\ntol: Tolerable change of W at convergence (default 1.0e-6)\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\nwinit: Initial guess of W, which should be either of:\nempty matrix: the function will perform random initialization (default)\na matrix of size (k k) (when do_whiten)\na matrix of size (m k) (when !do_whiten)\n\nReturns the resultant ICA model, an instance of type ICA.\n\nNote: If do_whiten is true, the return W satisfies mathbfW^T mathbfC mathbfW = mathbfI, otherwise W is orthonormal, i.e mathbfW^T mathbfW = mathbfI.\n\n\n\n\n\n","category":"method"},{"location":"ica/#Base.size-Tuple{ICA}","page":"Independent Component Analysis","title":"Base.size","text":"size(M::ICA)\n\nReturns a tuple with the input dimension, i.e the number of observed mixtures, and the output dimension, i.e the number of independent components.\n\n\n\n\n\n","category":"method"},{"location":"ica/#Statistics.mean-Tuple{ICA}","page":"Independent Component Analysis","title":"Statistics.mean","text":"mean(M::ICA)\n\nReturns the mean vector.\n\n\n\n\n\n","category":"method"},{"location":"ica/#StatsAPI.predict-Tuple{ICA, AbstractVecOrMat{<:Real}}","page":"Independent Component Analysis","title":"StatsAPI.predict","text":"predict(M::ICA, x)\n\nTransform x to the output space to extract independent components, as mathbfW^T (mathbfx - boldsymbolmu), given the model M.\n\n\n\n\n\n","category":"method"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"The package also exports functions of the core algorithms. Sometimes, it can be more efficient to directly invoke them instead of going through the fit interface.","category":"page"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"fastica!","category":"page"},{"location":"ica/#MultivariateStats.fastica!","page":"Independent Component Analysis","title":"MultivariateStats.fastica!","text":"fastica!(W, X, fun, maxiter, tol, verbose)\n\nInvoke the Fast ICA algorithm[1].\n\nParameters:\n\nW: The initial un-mixing matrix, of size (m k). The function updates this matrix inplace.\nX: The data matrix, of size (m n). This matrix is input only, and won't be modified.\nfun: The approximate neg-entropy functor of type ICAGDeriv.\nmaxiter: Maximum number of iterations.\ntol: Tolerable change of W at convergence.\n\nReturns the updated W.\n\nNote: The number of components is inferred from W as size(W, 2).\n\n\n\n\n\n","category":"function"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"The FastICA method requires a first derivative of a functor g to approximate negative entropy. The package implements an following interface for defining derivative value estimation:","category":"page"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"MultivariateStats.ICAGDeriv\nMultivariateStats.Tanh\nMultivariateStats.Gaus","category":"page"},{"location":"ica/#MultivariateStats.ICAGDeriv","page":"Independent Component Analysis","title":"MultivariateStats.ICAGDeriv","text":"The abstract type for all g (derivative) functions.\n\nLet g be an instance of such type, then update!(g, U, E) given\n\nU = w'x\n\nreturns updated in-place U and E, s.t.\n\ng(w'x) --> U and E{g'(w'x)} --> E\n\n\n\n\n\n","category":"type"},{"location":"ica/#MultivariateStats.Tanh","page":"Independent Component Analysis","title":"MultivariateStats.Tanh","text":"Derivative for (1a_1)logcosh a_1 u\n\n\n\n\n\n","category":"type"},{"location":"ica/#MultivariateStats.Gaus","page":"Independent Component Analysis","title":"MultivariateStats.Gaus","text":"Derivative for -e^frac-u^22\n\n\n\n\n\n","category":"type"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"","category":"page"},{"location":"ica/#References","page":"Independent Component Analysis","title":"References","text":"","category":"section"},{"location":"ica/","page":"Independent Component Analysis","title":"Independent Component Analysis","text":"[1]: Aapo Hyvarinen and Erkki Oja, Independent Component Analysis: Algorithms and Applications. Neural Network 13(4-5), 2000.","category":"page"},{"location":"whiten/#Data-Transformation","page":"Data Transformation","title":"Data Transformation","text":"","category":"section"},{"location":"whiten/#Whitening","page":"Data Transformation","title":"Whitening","text":"","category":"section"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"A whitening transformation is a decorrelation transformation that transforms a set of random variables into a set of new random variables with identity covariance (uncorrelated with unit variances).","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"In particular, suppose a random vector has covariance mathbfC, then a whitening transform mathbfW is one that satisfy:","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"   mathbfW^T mathbfC mathbfW = mathbfI","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Note that mathbfW is generally not unique. In particular, if mathbfW is a whitening transform, so is any of its rotation mathbfW mathbfR with mathbfR^T mathbfR = mathbfI.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"The package uses Whitening to represent a whitening transform.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Whitening","category":"page"},{"location":"whiten/#MultivariateStats.Whitening","page":"Data Transformation","title":"MultivariateStats.Whitening","text":"A whitening transform representation.\n\n\n\n\n\n","category":"type"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Whitening transformation can be fitted to data using the fit method.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"fit(::Type{Whitening}, X::AbstractMatrix{T}; kwargs...) where {T<:Real}\nMultivariateStats.transform(::Whitening, ::AbstractVecOrMat{<:Real})\nlength(::Whitening)\nmean(::Whitening)\nsize(::Whitening)","category":"page"},{"location":"whiten/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{Whitening}, AbstractMatrix{T}}} where T<:Real","page":"Data Transformation","title":"StatsAPI.fit","text":"fit(Whitening, X::AbstractMatrix{T}; kwargs...)\n\nEstimate a whitening transform from the data given in X.\n\nThis function returns an instance of Whitening\n\nKeyword Arguments:\n\nregcoef: The regularization coefficient. The covariance will be regularized as follows when regcoef is positive C + (eigmax(C) * regcoef) * eye(d). Default values is zero(T).\ndims: if 1 the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if 2 the transformation calculated from the column samples. The default is nothing, which is equivalent to dims=2 with a deprecation warning.\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNote: This function internally relies on cov_whitening to derive the transformation W.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#MultivariateStats.transform-Tuple{Whitening, AbstractVecOrMat{<:Real}}","page":"Data Transformation","title":"MultivariateStats.transform","text":"transform(f, x)\n\nApply the whitening transform f to a vector or a matrix x with samples in columns, as mathbfW^T (mathbfx - boldsymbolmu).\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Base.length-Tuple{Whitening}","page":"Data Transformation","title":"Base.length","text":"length(f)\n\nGet the dimension of the  whitening transform f.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Statistics.mean-Tuple{Whitening}","page":"Data Transformation","title":"Statistics.mean","text":"mean(f)\n\nGet the mean vector of the whitening transformation f.\n\nNote: if mean is empty, this function returns a zero vector of length(f).\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Base.size-Tuple{Whitening}","page":"Data Transformation","title":"Base.size","text":"size(f)\n\nDimensions of the coefficient matrix of the whitening transform f.\n\n\n\n\n\n","category":"method"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Additional methods","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"cov_whitening\ncov_whitening!","category":"page"},{"location":"whiten/#MultivariateStats.cov_whitening","page":"Data Transformation","title":"MultivariateStats.cov_whitening","text":"cov_whitening(C)\n\nDerive the whitening transform coefficient matrix W given the covariance matrix C. Here, C can be either a square matrix, or an instance of Cholesky.\n\nInternally, this function solves the whitening transform using Cholesky factorization. The rationale is as follows: let mathbfC = mathbfU^T mathbfU and mathbfW = mathbfU^-1, then mathbfW^T mathbfC mathbfW = mathbfI.\n\nNote: The return matrix W is an upper triangular matrix.\n\n\n\n\n\ncov_whitening(C, regcoef)\n\nDerive a whitening transform based on a regularized covariance, as C + (eigmax(C) * regcoef) * eye(d).\n\n\n\n\n\n","category":"function"},{"location":"whiten/#MultivariateStats.cov_whitening!","page":"Data Transformation","title":"MultivariateStats.cov_whitening!","text":"cov_whitening!(C)\n\nIn-place version of cov_whitening(C), in which the input matrix C will be overwritten during computation. This can be more efficient when C is no longer used.\n\n\n\n\n\ncov_whitening!(C, regcoef)\n\nIn-place version of cov_whitening(C, regcoef), in which the input matrix C will be overwritten during computation. This can be more efficient when C is no longer used.\n\n\n\n\n\n","category":"function"},{"location":"#MultivariateStats.jl-Documentation","page":"Home","title":"MultivariateStats.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MultivariateStats\nDocTestSetup = quote\n    using Statistics\n    using Random\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultivariateStats.jl is a Julia package for multivariate statistical analysis. It provides a rich set of useful analysis techniques, such as PCA, CCA, LDA, ICA, etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"whiten.md\", \"lreg.md\", \"lda.md\", \"pca.md\", \"ica.md\", \"cca.md\", \"fa.md\", \"mds.md\", \"api.md\"]\nDepth = 2","category":"page"},{"location":"","page":"Home","title":"Home","text":"Notes: All methods implemented in this package adopt the column-major convention of JuliaStats: in a data matrix, each column corresponds to a sample/observation, while each row corresponds to a feature (variable or attribute).","category":"page"},{"location":"lreg/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides functions to perform Linear Least Square, Ridge, and Isotonic Regression.","category":"page"},{"location":"lreg/#Examples","page":"Regression","title":"Examples","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"using Plots\ngr(fmt=:svg)","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Performing llsq regression on cars data set:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"using MultivariateStats, RDatasets, Plots\n\n# load cars dataset\ncars = dataset(\"datasets\", \"cars\")\n\n# calculate regression models\na = llsq(cars[!,:Speed], cars[!, :Dist])\nb = isotonic(cars[!,:Speed], cars[!, :Dist])\n\n# plot results\np = scatter(cars[!,:Speed], cars[!,:Dist], xlab=\"Speed\", ylab=\"Distance\",\n            leg=:topleft, lab=\"data\")\nlet xs = cars[!,:Speed]\n    plot!(p, xs, map(x->a[1]*x+a[2], xs), lab=\"llsq\")\n    plot!(p, xs, b, lab=\"isotonic\")\nend","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"For a single response vector y (without using bias):","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"# prepare data\nX = rand(1000, 3)               # feature matrix\na0 = rand(3)                    # ground truths\ny = X * a0 + 0.1 * randn(1000)  # generate response\n\n# solve using llsq\na = llsq(X, y; bias=false)\n\n# do prediction\nyp = X * a\n\n# measure the error\nrmse = sqrt(mean(abs2.(y - yp)))\nprint(\"rmse = $rmse\")","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"For a single response vector y (using bias):","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"# prepare data\nX = rand(1000, 3)\na0, b0 = rand(3), rand()\ny = X * a0 .+ b0 .+ 0.1 * randn(1000)\n\n# solve using llsq\nsol = llsq(X, y)\n\n# extract results\na, b = sol[1:end-1], sol[end]\n\n# do prediction\nyp = X * a .+ b'","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"For a matrix of column-stored regressors X and a matrix comprised of multiple columns of dependent variables Y:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"# prepare data\nX = rand(3, 1000)\nA0, b0 = rand(3, 5), rand(1, 5)\nY = (X' * A0 .+ b0) + 0.1 * randn(1000, 5)\n\n# solve using llsq\nsol = llsq(X, Y, dims=2)\n\n# extract results\nA, b = sol[1:end-1,:], sol[end,:]\n\n# do prediction\nYp = X'*A .+ b'","category":"page"},{"location":"lreg/#Linear-Least-Square","page":"Regression","title":"Linear Least Square","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Linear Least Square is to find linear combination(s) of given variables to fit the responses by minimizing the squared error between them. This can be formulated as an optimization as follows:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX mathbfa + b)^2","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Sometimes, the coefficient matrix is given in a transposed form, in which case, the optimization is modified as:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX^T mathbfa + b)^2","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides following functions to solve the above problems:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"llsq","category":"page"},{"location":"lreg/#MultivariateStats.llsq","page":"Regression","title":"MultivariateStats.llsq","text":"llsq(X, y; ...)\n\nSolve the linear least square problem.\n\nHere, y can be either a vector, or a matrix where each column is a response vector.\n\nThis function accepts two keyword arguments:\n\ndims: whether input observations are stored as rows (1) or columns (2). (default is 1)\nbias: whether to include the bias term b. (default is true)\n\nThe function results the solution a. In particular, when y is a vector (matrix), a is also a vector (matrix). If bias is true, then the returned array is augmented as [a; b].\n\n\n\n\n\n","category":"function"},{"location":"lreg/#Ridge-Regression","page":"Regression","title":"Ridge Regression","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Compared to linear least square, Ridge Regression uses an additional quadratic term to regularize the problem:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX mathbfa + b)^2 +\n    frac12 mathbfa^T mathbfQ mathbfa","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The transposed form:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"    mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX^T mathbfa + b)^2 +\n    frac12 mathbfa^T mathbfQ mathbfa","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides following functions to solve the above problems:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"ridge","category":"page"},{"location":"lreg/#MultivariateStats.ridge","page":"Regression","title":"MultivariateStats.ridge","text":"ridge(X, y, r; ...)\n\nSolve the ridge regression problem.\n\nHere, y can be either a vector, or a matrix where each column is a response vector.\n\nThe argument r gives the quadratic regularization matrix Q, which can be in either of the following forms:\n\nr is a real scalar, then Q is considered to be r * eye(n), where n is the dimension of a.\nr is a real vector, then Q is considered to be diagm(r).\nr is a real symmetric matrix, then Q is simply considered to be r.\n\nThis function accepts two keyword arguments:\n\ndims: whether input observations are stored as rows (1) or columns (2). (default is 1)\nbias: whether to include the bias term b. (default is true)\n\nThe function results the solution a. In particular, when y is a vector (matrix), a is also a vector (matrix). If bias is true, then the returned array is augmented as [a; b].\n\n\n\n\n\n","category":"function"},{"location":"lreg/#Isotonic-Regression","page":"Regression","title":"Isotonic Regression","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Isotonic regression or monotonic regression fits a sequence of observations into a fitted line that is non-decreasing (or non-increasing) everywhere. The problem defined as a weighted least-squares fit hat y_i approx y_i for all i, subject to the constraint that hat y_i leq hat y_j whenever x_i leq x_j. This gives the following quadratic program:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"min sum_i=1^n w_i(hat y_i-y_i)^2\ntext  subject to   hat y_i leq hat y_j\ntext for all  (ij) in E","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"where E=(ij)x_ileq x_j specifies the partial ordering of the observed inputs x_i.","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides following functions to solve the above problems:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"isotonic","category":"page"},{"location":"lreg/#MultivariateStats.isotonic","page":"Regression","title":"MultivariateStats.isotonic","text":"isotonic(x, y[, w])\n\nSolve the isotonic regression problem using the pool adjacent violators algorithm[1].\n\nHere x is the regressor vector, y is response vector, and w is an optional weights vector.\n\nThe function returns a prediction vector of the same size as the regressor vector x.\n\n\n\n\n\n","category":"function"},{"location":"lreg/","page":"Regression","title":"Regression","text":"","category":"page"},{"location":"lreg/#References","page":"Regression","title":"References","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"[1]: Best, M.J., Chakravarti, N. Active set algorithms for isotonic regression; A unifying framework. Mathematical Programming 47, 425–439 (1990).","category":"page"},{"location":"mds/#Multidimensional-Scaling","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"In general, Multidimensional Scaling (MDS) refers to techniques that transforms samples into lower dimensional space while preserving the inter-sample distances as well as possible.","category":"page"},{"location":"mds/#Example","page":"Multidimensional Scaling","title":"Example","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"using Plots\ngr(fmt=:svg)","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Performing MDS on Iris data set:","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"using MultivariateStats, RDatasets, Plots\n\n# load iris dataset\niris = dataset(\"datasets\", \"iris\")\n\n# take half of the dataset\nX = Matrix(iris[1:2:end,1:4])'\nX_labels = Vector(iris[1:2:end,5])\nnothing # hide","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Suppose X is our data matrix, with each observation in a column. We train a MDS model, allowing up to 3 dimensions:","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"M = fit(MDS, X; maxoutdim=3, distances=false)","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Then, apply MDS model to get an embedding of our data in 3D space:","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Y = predict(M)","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Now, we group results by testing set labels for color coding and visualize first 3 principal components in 3D interactive plot","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"setosa = Y[:,X_labels.==\"setosa\"]\nversicolor = Y[:,X_labels.==\"versicolor\"]\nvirginica = Y[:,X_labels.==\"virginica\"]\n\np = scatter(setosa[1,:],setosa[2,:],setosa[3,:],marker=:circle,linewidth=0)\nscatter!(versicolor[1,:],versicolor[2,:],versicolor[3,:],marker=:circle,linewidth=0)\nscatter!(virginica[1,:],virginica[2,:],virginica[3,:],marker=:circle,linewidth=0)","category":"page"},{"location":"mds/#Classical-Multidimensional-Scaling","page":"Multidimensional Scaling","title":"Classical Multidimensional Scaling","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"This package defines a MDS type to represent a classical MDS model[1], and provides a set of methods to access the properties.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"MDS","category":"page"},{"location":"mds/#MultivariateStats.MDS","page":"Multidimensional Scaling","title":"MultivariateStats.MDS","text":"Classical Multidimensional Scaling (MDS), also known as Principal Coordinates Analysis (PCoA), is a specific technique in this family that accomplishes the embedding in two steps:\n\nConvert the distance matrix to a Gram matrix. This conversion is based on\n\nthe following relations between a distance matrix D and a Gram matrix G:\n\nmathrmsqr(mathbfD) = mathbfg mathbf1^T + mathbf1 mathbfg^T - 2 mathbfG\n\nHere, mathrmsqr(mathbfD) indicates the element-wise square of mathbfD, and mathbfg is the diagonal elements of mathbfG. This relation is itself based on the following decomposition of squared Euclidean distance:\n\n mathbfx - mathbfy ^2 =  mathbfx ^2 +  mathbfy ^2 - 2 mathbfx^T mathbfy\n\nPerform eigenvalue decomposition of the Gram matrix to derive the coordinates.\n\nNote:  The Gramian derived from D may have non-positive or degenerate eigenvalues.  The subspace of non-positive eigenvalues is projected out of the MDS solution so that the strain function is minimized in a least-squares sense.  If the smallest remaining eigenvalue that is used for the MDS is degenerate, then the solution is not unique, as any linear combination of degenerate eigenvectors will also yield a MDS solution with the same strain value.\n\n\n\n\n\n","category":"type"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"The MDS method type comes with several methods where M be an instance of MDS, d be the dimension of observations, and p be the output dimension, i.e. the embedding dimension, and n is the number of the observations.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"fit(::Type{MDS}, ::AbstractMatrix{T}; kwargs) where {T<:Real}\npredict(::MDS{T}) where {T<:Real}\npredict(::MDS, ::AbstractVector{<:Real})\nsize(::MDS)\nprojection(M::MDS)\nloadings(M::MDS)\neigvals(M::MDS)\neigvecs(M::MDS)\nstress","category":"page"},{"location":"mds/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{MDS}, AbstractMatrix{T}}} where T<:Real","page":"Multidimensional Scaling","title":"StatsAPI.fit","text":"fit(MDS, X; kwargs...)\n\nCompute an embedding of X points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument distances:\n\nmds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\n\nwhere X is the data matrix. Distances between pairs of columns of X are computed using the Euclidean norm. This is equivalent to performing PCA on X.\n\nmds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\n\nwhere D is a symmetric matrix D of distances between points.\n\n\n\n\n\n","category":"method"},{"location":"mds/#StatsAPI.predict-Union{Tuple{MDS{T}}, Tuple{T}} where T<:Real","page":"Multidimensional Scaling","title":"StatsAPI.predict","text":"predict(M)\n\nReturns a coordinate matrix of size (p n) for the MDS model M, where each column is the coordinates for an observation in the embedding space.\n\n\n\n\n\n","category":"method"},{"location":"mds/#StatsAPI.predict-Tuple{MDS, AbstractVector{<:Real}}","page":"Multidimensional Scaling","title":"StatsAPI.predict","text":"predict(M, x::AbstractVector)\n\nCalculate the out-of-sample transformation of the observation x for the MDS model M. Here, x is a vector of length d.\n\n\n\n\n\n","category":"method"},{"location":"mds/#Base.size-Tuple{MDS}","page":"Multidimensional Scaling","title":"Base.size","text":"size(M::MDS)\n\nReturns tuple where the first value is the MDS model M input dimension, i.e the dimension of the observation space, and the second value is the output dimension, i.e the dimension of the embedding.\n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.projection-Tuple{MDS}","page":"Multidimensional Scaling","title":"MultivariateStats.projection","text":"projection(M::MDS)\n\nGet the MDS model M eigenvectors matrix (of size (n p)) of the embedding space. The eigenvectors are arranged in descending order of the corresponding eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.loadings-Tuple{MDS}","page":"Multidimensional Scaling","title":"MultivariateStats.loadings","text":"loadings(M::MDS)\n\nGet the loading of the MDS model M.\n\n\n\n\n\n","category":"method"},{"location":"mds/#LinearAlgebra.eigvals-Tuple{MDS}","page":"Multidimensional Scaling","title":"LinearAlgebra.eigvals","text":"eigvals(M::MDS)\n\nGet the eigenvalues of the MDS model M.\n\n\n\n\n\n","category":"method"},{"location":"mds/#LinearAlgebra.eigvecs-Tuple{MDS}","page":"Multidimensional Scaling","title":"LinearAlgebra.eigvecs","text":"eigvecs(M::MDS)\n\nGet the MDS model M eigenvectors matrix. \n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.stress","page":"Multidimensional Scaling","title":"MultivariateStats.stress","text":"stress(M::MDS)\n\nGet the stress of the MDS mode M.\n\n\n\n\n\nstress(M::MetricMDS)\n\nGet the stress of the MDS model M.\n\n\n\n\n\n","category":"function"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"This package provides following functions related to classical MDS.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"gram2dmat\ngram2dmat!\ndmat2gram\ndmat2gram!","category":"page"},{"location":"mds/#MultivariateStats.gram2dmat","page":"Multidimensional Scaling","title":"MultivariateStats.gram2dmat","text":"gram2dmat(G)\n\nConvert a Gram matrix G to a distance matrix.\n\n\n\n\n\n","category":"function"},{"location":"mds/#MultivariateStats.gram2dmat!","page":"Multidimensional Scaling","title":"MultivariateStats.gram2dmat!","text":"gram2dmat!(D, G)\n\nConvert a Gram matrix G to a distance matrix, and write the results to D.\n\n\n\n\n\n","category":"function"},{"location":"mds/#MultivariateStats.dmat2gram","page":"Multidimensional Scaling","title":"MultivariateStats.dmat2gram","text":"dmat2gram(D)\n\nConvert a distance matrix D to a Gram matrix.\n\n\n\n\n\n","category":"function"},{"location":"mds/#MultivariateStats.dmat2gram!","page":"Multidimensional Scaling","title":"MultivariateStats.dmat2gram!","text":"dmat2gram!(G, D)\n\nConvert a distance matrix D to a Gram matrix, and write the results to G.\n\n\n\n\n\n","category":"function"},{"location":"mds/#Metric-Multidimensional-Scaling","page":"Multidimensional Scaling","title":"Metric Multidimensional Scaling","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"This package defines a MetricMDS type to represent a (non)metric MDS model[1], and provides a set of methods to access the properties.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"MetricMDS","category":"page"},{"location":"mds/#MultivariateStats.MetricMDS","page":"Multidimensional Scaling","title":"MultivariateStats.MetricMDS","text":"(Non)Metric Multidimensional Scaling\n\nUse this class to perform (non)metric multidimensional scaling.\n\nThere are two calling options, specified via the required keyword argument distances:\n\nmds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\n\nwhere X is the data matrix. Distances between pairs of columns of X are computed using the Euclidean norm. This is equivalent to performing PCA on X.\n\nmds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\n\nwhere D is a symmetric matrix D of distances between points.\n\nIn addition, the metric parameter specifies type of MDS, By default, it is assigned with nothing value which results in performing metric MDS with dissimilarities calculated as Euclidean distances.\n\nAn arbitrary transformation function can be provided to metric parameter to perform metric MDS with transformed proximities. The function has to accept two parameters, a vector of proximities and a vector of distances, corresponding to the proximities, to calculate disparities required for stress calculation. Internally, the proximity and distance vectors are obtained from compact triangular matrix representation of proximity and distance matrices.\n\nFor ratio MDS, a following ratio transformation function can be used\n\nmds = fit(MDS, D; distances=true, metric=((p,d)->2 .* p))\n\nFor order MDS, use isotonic regression function in the metric parameter:\n\nmds = fit(MDS, D; distances=true, metric=isotonic)\n\n\n\n\n\n","category":"type"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"The metric MDS type comes with several methods where M be an instance of MetricMDS, d be the dimension of observations, and p be the output dimension, i.e. the embedding dimension, and n is the number of the observations.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"fit(::Type{MetricMDS}, ::AbstractMatrix{T}; kwargs) where {T<:Real}\npredict(::MetricMDS)\nsize(::MetricMDS)\nstress(::MetricMDS)","category":"page"},{"location":"mds/#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{MetricMDS}, AbstractMatrix{T}}} where T<:Real","page":"Multidimensional Scaling","title":"StatsAPI.fit","text":"fit(MetricMDS, X; kwargs...)\n\nCompute an embedding of X points by (non)metric multidimensional scaling (MDS).\n\nKeyword arguments:\n\nLet (d, n) = size(X) be respectively the input dimension and the number of observations:\n\ndistances: The choice of input (required):\nfalse: use X to calculate dissimilarity matrix using Euclidean distance\ntrue: use X input as precomputed dissimilarity symmetric matrix (distances)\nmaxoutdim: Maximum output dimension (default d-1)\nmetric : a function for calculation of disparity values\nnothing: use dissimilarity values as the disparities to perform the metric MDS (default)\nisotonic: converts dissimilarity values to ordinal disparities to perform non-metric MDS\nany two parameter disparity transformation function, where the first parameter is a vector of proximities (i.e. dissimilarities) and the second parameter is a vector of distances, e.g. (p,d)->b*p for some b is a transformation function for ratio MDS.\ntol: Convergence tolerance (default 1.0e-3)\nmaxiter: Maximum number of iterations (default 300)\ninitial: an initial reduced space point configuration\nnothing: then an initial configuration is randomly generated (default)\npre-defined matrix\nweights: a weight matrix\nnothing: then weights are set to one, w_ij = 1 (default)\npre-defined matrix\n\nNote: if the algorithm is unable to converge then ConvergenceException is thrown.\n\n\n\n\n\n","category":"method"},{"location":"mds/#StatsAPI.predict-Tuple{MetricMDS}","page":"Multidimensional Scaling","title":"StatsAPI.predict","text":"predict(M::MetricMDS)\n\nReturns a coordinate matrix of size (p n) for the MDS model M, where each column is the coordinates for an observation in the embedding space.\n\n\n\n\n\n","category":"method"},{"location":"mds/#Base.size-Tuple{MetricMDS}","page":"Multidimensional Scaling","title":"Base.size","text":"size(M::MetricMDS)\n\nReturns tuple where the first value is the MDS model M input dimension, i.e the dimension of the observation space, and the second value is the output dimension, i.e the dimension of the embedding.\n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.stress-Tuple{MetricMDS}","page":"Multidimensional Scaling","title":"MultivariateStats.stress","text":"stress(M::MetricMDS)\n\nGet the stress of the MDS model M.\n\n\n\n\n\n","category":"method"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"","category":"page"},{"location":"mds/#References","page":"Multidimensional Scaling","title":"References","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"[1]: Ingwer Borg and Patrick J. F. Groenen, \"Modern Multidimensional Scaling: Theory and Applications\", Springer, pp. 201–268, 2005.","category":"page"}]
}
