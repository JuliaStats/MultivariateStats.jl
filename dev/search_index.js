var documenterSearchIndex = {"docs":
[{"location":"pca/#Principal-Component-Analysis","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Principal Component Analysis (PCA) derives an orthogonal projection to convert a given set of observations to linearly uncorrelated variables, called principal components.","category":"page"},{"location":"pca/#Example","page":"Principal Component Analysis","title":"Example","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Performing PCA on Iris data set:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"using MultivariateStats, RDatasets, Plots\nplotly() # using plotly for 3D-interacive graphing\n\n# load iris dataset\niris = dataset(\"datasets\", \"iris\")\n\n# split half to training set\nXtr = Matrix(iris[1:2:end,1:4])'\nXtr_labels = Vector(iris[1:2:end,5])\n\n# split other half to testing set\nXte = Matrix(iris[2:2:end,1:4])'\nXte_labels = Vector(iris[2:2:end,5])\nnothing # hide","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Suppose Xtr and Xte are training and testing data matrix, with each observation in a column. We train a PCA model, allowing up to 3 dimensions:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"M = fit(PCA, Xtr; maxoutdim=3)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Then, apply PCA model to the testing set","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Yte = predict(M, Xte)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"And, reconstruct testing observations (approximately) to the original space","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Xr = reconstruct(M, Yte)","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Now, we group results by testing set labels for color coding and visualize first 3 principal components in 3D interactive plot","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"setosa = Yte[:,Xte_labels.==\"setosa\"]\nversicolor = Yte[:,Xte_labels.==\"versicolor\"]\nvirginica = Yte[:,Xte_labels.==\"virginica\"]\n\np = scatter(setosa[1,:],setosa[2,:],setosa[3,:],marker=:circle,linewidth=0)\nscatter!(versicolor[1,:],versicolor[2,:],versicolor[3,:],marker=:circle,linewidth=0)\nscatter!(virginica[1,:],virginica[2,:],virginica[3,:],marker=:circle,linewidth=0)\nplot!(p,xlabel=\"PC1\",ylabel=\"PC2\",zlabel=\"PC3\")","category":"page"},{"location":"pca/#Linear-Principal-Component-Analysis","page":"Principal Component Analysis","title":"Linear Principal Component Analysis","text":"","category":"section"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This package uses the PCA type to define a linear PCA model:","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"PCA","category":"page"},{"location":"pca/#MultivariateStats.PCA","page":"Principal Component Analysis","title":"MultivariateStats.PCA","text":"Linear Principal Component Analysis\n\n\n\n\n\n","category":"type"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"This type comes with several methods where M be an instance of  PCA, d be the dimension of observations, and p be the output dimension (i.e the dimension of the principal subspace).","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"fit(::Type{PCA}, ::AbstractMatrix{T}; kwargs) where {T<:Real}\npredict(::PCA, ::AbstractVecOrMat{T}) where {T<:Real}\nreconstruct(::PCA, ::AbstractVecOrMat{T}) where {T<:Real}\nsize(::PCA)\nmean(M::PCA)\nprojection(M::PCA)\nvar(M::PCA)\ntprincipalvar(M::PCA)\ntresidualvar(M::PCA)\nr2(M::PCA)\nloadings(M::PCA)\neigvals(M::PCA)\neigvecs(M::PCA)","category":"page"},{"location":"pca/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{PCA}, AbstractMatrix{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsBase.fit","text":"fit(PCA, X; ...)\n\nPerform PCA over the data given in a matrix X. Each column of X is an observation.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:auto: use :cov when d < n or :svd otherwise (default).\n:cov: based on covariance matrix decomposition.\n:svd: based on SVD of the input data.\nmaxoutdim: The output dimension, i.e. dimension of the transformed space (min(d, nc-1))\npratio: The ratio of variances preserved in the principal subspace (0.99)\nmean: The mean vector, which can be either of\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNotes:\n\nThe output dimension p depends on both maxoutdim and pratio, as follows. Suppose the first k principal components preserve at least pratio of the total variance, while the first k-1 preserves less than pratio, then the actual output dimension will be min(k maxoutdim).\nThis function calls pcacov or pcasvd internally, depending on the choice of method.\n\n\n\n\n\n","category":"method"},{"location":"pca/#StatsBase.predict-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"StatsBase.predict","text":"predict(M::PCA, x::AbstractVecOrMat{<:Real})\n\nGiven a PCA model M, retur transform observations x into principal components space, as\n\nmathbfy = mathbfP^T (mathbfx - boldsymbolmu)\n\nHere, x can be either a vector of length d or a matrix where each column is an observation, and \\mathbf{P} is the projection matrix.\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T<:Real","page":"Principal Component Analysis","title":"MultivariateStats.reconstruct","text":"reconstruct(M::PCA, y::AbstractVecOrMat{<:Real})\n\nGiven a PCA model M, returns a (approximately) reconstructed observations from principal components space, as\n\ntildemathbfx = mathbfP mathbfy + boldsymbolmu\n\nHere, y can be either a vector of length p or a matrix where each column gives the principal components for an observation, and mathbfP is the projection matrix.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Base.size-Tuple{PCA}","page":"Principal Component Analysis","title":"Base.size","text":"size(M)\n\nReturns a tuple with the dimensions of input (the dimension of the observation space) and output (the dimension of the principal subspace).\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.mean-Tuple{PCA}","page":"Principal Component Analysis","title":"Statistics.mean","text":"mean(M::PCA)\n\nReturns the mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.projection-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.projection","text":"projection(M::PCA)\n\nReturns the projection matrix (of size (d, p)). Each column of the projection matrix corresponds to a principal component. The principal components are arranged in descending order of the corresponding variances.\n\n\n\n\n\n","category":"method"},{"location":"pca/#Statistics.var-Tuple{PCA}","page":"Principal Component Analysis","title":"Statistics.var","text":"var(M::PCA)\n\nReturns the total observation variance, which is equal to tprincipalvar(M) + tresidualvar(M).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.tprincipalvar-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.tprincipalvar","text":"tprincipalvar(M::PCA)\n\nReturns the total variance of principal components, which is equal to sum(principalvars(M)).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.tresidualvar-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.tresidualvar","text":"tresidualvar(M::PCA)\n\nReturns the total residual variance.\n\n\n\n\n\n","category":"method"},{"location":"pca/#StatsBase.r2-Tuple{PCA}","page":"Principal Component Analysis","title":"StatsBase.r2","text":"r2(M::PCA)\nprincipalratio(M::PCA)\n\nReturns the ratio of variance preserved in the principal subspace, which is equal to tprincipalvar(M) / var(M).\n\n\n\n\n\n","category":"method"},{"location":"pca/#MultivariateStats.loadings-Tuple{PCA}","page":"Principal Component Analysis","title":"MultivariateStats.loadings","text":"loadings(M::PCA)\n\nReturns model loadings, i.e. the weights for each original variable when calculating the principal component.\n\n\n\n\n\n","category":"method"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"Auxiliary functions","category":"page"},{"location":"pca/","page":"Principal Component Analysis","title":"Principal Component Analysis","text":"pcacov\npcasvd","category":"page"},{"location":"pca/#MultivariateStats.pcacov","page":"Principal Component Analysis","title":"MultivariateStats.pcacov","text":"pcacov(C, mean; ...)\n\nCompute and return a PCA model based on eigenvalue decomposition of a given covariance matrix C.\n\nParameters:\n\nC: The covariance matrix of the samples.\nmean: The mean vector of original samples, which can be a vector of length d,          or an empty vector Float64[] indicating a zero mean.\n\nNote: This function accepts two keyword arguments: maxoutdim and pratio.\n\n\n\n\n\n","category":"function"},{"location":"pca/#MultivariateStats.pcasvd","page":"Principal Component Analysis","title":"MultivariateStats.pcasvd","text":"pcasvd(Z, mean, tw; ...)\n\nCompute and return a PCA model based on singular value decomposition of a centralized sample matrix Z.\n\nParameters:\n\nZ: a matrix of centralized samples.\nmean: The mean vector of the original samples, which can be a vector of length d,         or an empty vector Float64[] indicating a zero mean.\nn: a number of samples.\n\nNote: This function accepts two keyword arguments: maxoutdim and pratio.\n\n\n\n\n\n","category":"function"},{"location":"api/#API-Reference","page":"Development","title":"API Reference","text":"","category":"section"},{"location":"api/#Current","page":"Development","title":"Current","text":"","category":"section"},{"location":"api/","page":"Development","title":"Development","text":"Table of the package models and corresponding function names used by these models.","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"Function \\ Model CCA WHT ICA LDA FA PPCA PCA KPCA MDS\nfit x x x x x x x x x\ntransform x x x x x x x x x\npredict    x     \nindim  x x x x x x x x\noutdim x x x x x x x x x\nmean x x x x x x x ? \nvar     x x ? ? ?\ncov     x ?   \ncor x        \nprojection x    x x x x x\nreconstruct     x x x x \nloadings ?   ? x x ? ? ?\neigvals     ? ? ? ? x\neigvecs     ? ? ? ? ?\nlength         \nsize         ","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"Note: ? refers to a possible implementation that is missing or called differently.","category":"page"},{"location":"api/#New","page":"Development","title":"New","text":"","category":"section"},{"location":"api/","page":"Development","title":"Development","text":"Function \\ Model WHT CCA LDA MC-LDA SS-LDA ICA FA PPCA PCA KPCA MDS\nfit x x x x x x x x x x x\ntransform x x - - - x x x - x -\npredict   x + +    +  +\nindim -   - - x x x - x -\noutdim - x  - - x x x - x -\nmean x x  x x x x x x ? \nvar       x x x ? ?\ncov       x x   \ncor  x         \nprojection ? x  x x  x x x x x\nreconstruct       x x x x \nloadings  ?     x x x ? +\neigvals     +  ? ? x ? x\neigvecs       ? ? x ? +\nlength +  + + +      \nsize +   + +    x  +\n           ","category":"page"},{"location":"api/","page":"Development","title":"Development","text":"StatsBase.AbstractDataTransform\nWhitening\nInterface: fit, transform\nNew: length, mean, size\nStatsBase.RegressionModel\nInterface: fit, predict\nLinearDiscriminant\nFunctions: coef, dof, weights, evaluate, length\nMulticlassLDA\nFunctions: size, mean, projection, length\nSubspaceLDA\nFunctions: size, mean, projection, length, eigvals\nCCA\nFunctions: indim, outdim, mean\nSubtypes:\nAbstractDimensionalityReduction\nInterface: projection, var, reconstruct, loadings\nInterface: projection == weights\nSubtypes:\nLinearDimensionalityReduction\nMethods: ICA, PCA\nNonlinearDimensionalityReduction\nMethods: KPCA, MDS\nFunctions: modelmatrix(X),\nLatentVariableModel or LatentVariableDimensionalityReduction\nMethods: FA, PPCA\nFunctions: cov","category":"page"},{"location":"lda/#Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Linear Discriminant Analysis (LDA) are statistical analysis methods to find a linear combination of features for separating observations in two classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Note: Please refer to MulticlassLDA for methods that can discriminate between multiple classes.","category":"page"},{"location":"lda/#Overview-of-LDA","page":"Linear Discriminant Analysis","title":"Overview of LDA","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Suppose the samples in the positive and negative classes respectively with means: boldsymbolmu_p and boldsymbolmu_n, and covariances mathbfC_p and mathbfC_n. Then based on Fisher's Linear Discriminant Criteria, the optimal projection direction can be expressed as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfw = alpha cdot (mathbfC_p + mathbfC_n)^-1 (boldsymbolmu_p - boldsymbolmu_n)","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here alpha is an arbitrary non-negative coefficient.","category":"page"},{"location":"lda/#Linear-Discriminant-Analysis-2","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This package uses the LinearDiscriminant type to capture a linear discriminant functional:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"LinearDiscriminant","category":"page"},{"location":"lda/#MultivariateStats.LinearDiscriminant","page":"Linear Discriminant Analysis","title":"MultivariateStats.LinearDiscriminant","text":"A linear discriminant functional can be written as\n\n    f(mathbfx) = mathbfw^T mathbfx + b\n\nHere, w is the coefficient vector, and b is the bias constant.\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This type comes with several methods where f be an instance of  LinearDiscriminant.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{LinearDiscriminant}, Xp::DenseMatrix{T}, Xn::DenseMatrix{T}; kwargs) where T<:Real\nevaluate(::LinearDiscriminant, ::AbstractVector)\nevaluate(::LinearDiscriminant, ::AbstractMatrix)\npredict(::LinearDiscriminant, ::AbstractVector)\npredict(::LinearDiscriminant, ::AbstractMatrix)\ncoef(::LinearDiscriminant)\ndof(::LinearDiscriminant)\nweights(::LinearDiscriminant)\nlength(::LinearDiscriminant)","category":"page"},{"location":"lda/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{LinearDiscriminant}, DenseMatrix{T}, DenseMatrix{T}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsBase.fit","text":"fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\n\nPerforms LDA given both positive and negative samples. The function accepts follwing parameters:\n\nParameters\n\nXp: The sample matrix of the positive class.\nXn: The sample matrix of the negative class.\n\nKeyword arguments:\n\ncovestimator: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#). Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.evaluate-Tuple{LinearDiscriminant, AbstractVector}","page":"Linear Discriminant Analysis","title":"MultivariateStats.evaluate","text":"evaluate(f, x::AbstractVector)\n\nEvaluate the linear discriminant value, i.e wx + b, it returns a real value.\n\n\n\n\n\n","category":"method"},{"location":"lda/#MultivariateStats.evaluate-Tuple{LinearDiscriminant, AbstractMatrix}","page":"Linear Discriminant Analysis","title":"MultivariateStats.evaluate","text":"evaluate(f, X::AbstractMatrix)\n\nEvaluate the linear discriminant value, i.e wx + b, for each sample in columns of X. The function returns a vector of length size(X, 2).\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{LinearDiscriminant, AbstractVector}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(f, x::AbstractVector)\n\nMake prediction for the vector x. It returns true iff evaluate(f, x) is positive.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{LinearDiscriminant, AbstractMatrix}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(f, X::AbstractMatrix)\n\nMake predictions for the matrix X.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.coef-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsBase.coef","text":"coef(f::LinearDiscriminant)\n\nReturn the coefficients of the linear discriminant model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.dof-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsBase.dof","text":"dof(f::LinearDiscriminant)\n\nReturn the number of degrees of freedom in the linear discriminant model.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.weights-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"StatsBase.weights","text":"weights(f::LinearDiscriminant)\n\nReturn the linear discriminant model coefficient vector.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{LinearDiscriminant}","page":"Linear Discriminant Analysis","title":"Base.length","text":"Get the length of the coefficient vector.\n\n\n\n\n\n","category":"method"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Additional functionality:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"ldacov","category":"page"},{"location":"lda/#MultivariateStats.ldacov","page":"Linear Discriminant Analysis","title":"MultivariateStats.ldacov","text":"ldacov(C, μp, μn)\n\nPerforms LDA given a covariance matrix C and both mean vectors μp & μn.  Returns a linear discriminant functional of type LinearDiscriminant.\n\nParameters\n\nC: The pooled covariane matrix (i.e (Cp + Cn)2)\nμp: The mean vector of the positive class.\nμn: The mean vector of the negative class.\n\n\n\n\n\nldacov(Cp, Cn, μp, μn)\n\nPerforms LDA given covariances and mean vectors. Returns a linear discriminant functional of type LinearDiscriminant.\n\nParameters\n\nCp: The covariance matrix of the positive class.\nCn: The covariance matrix of the negative class.\nμp: The mean vector of the positive class.\nμn: The mean vector of the negative class.\n\nNote: The coefficient vector is scaled such that wμp + b = 1 and wμn + b = -1.\n\n\n\n\n\n","category":"function"},{"location":"lda/#Multi-class-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Multi-class Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Multi-class LDA is a generalization of standard two-class LDA that can handle arbitrary number of classes.","category":"page"},{"location":"lda/#Overview","page":"Linear Discriminant Analysis","title":"Overview","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Multi-class LDA is based on the analysis of two scatter matrices: within-class scatter matrix and between-class scatter matrix.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Given a set of samples mathbfx_1 ldots mathbfx_n, and their class labels y_1 ldots y_n:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The within-class scatter matrix is defined as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_w = sum_i=1^n (mathbfx_i - boldsymbolmu_y_i) (mathbfx_i - boldsymbolmu_y_i)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here, boldsymbolmu_k is the sample mean of the k-th class.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The between-class scatter matrix is defined as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b = sum_k=1^m n_k (boldsymbolmu_k - boldsymbolmu) (boldsymbolmu_k - boldsymbolmu)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Here, m is the number of classes, boldsymbolmu is the overall sample mean, and n_k is the number of samples in the k-th class.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Then, multi-class LDA can be formulated as an optimization problem to find a set of linear combinations (with coefficients mathbfw) that maximizes the ratio of the between-class scattering to the within-class scattering, as","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"hatmathbfw = mathopmathrmargmax_mathbfw\n    fracmathbfw^T mathbfS_b mathbfwmathbfw^T mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The solution is given by the following generalized eigenvalue problem:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b mathbfw = lambda mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Generally, at most m - 1 generalized eigenvectors are useful to discriminate between m classes.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"When the dimensionality is high, it may not be feasible to construct the scatter matrices explicitly. In such cases, see SubspaceLDA below.","category":"page"},{"location":"lda/#Normalization-by-number-of-observations","page":"Linear Discriminant Analysis","title":"Normalization by number of observations","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"An alternative definition of the within- and between-class scatter matrices normalizes for the number of observations in each group:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_w^* = n sum_k=1^m frac1n_k sum_i mid y_i=k (mathbfx_i - boldsymbolmu_k) (mathbfx_i - boldsymbolmu_k)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"mathbfS_b^* = n sum_k=1^m (boldsymbolmu_k - boldsymbolmu^*) (boldsymbolmu_k - boldsymbolmu^*)^T","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"where","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"boldsymbolmu^* = frac1k sum_k=1^m boldsymbolmu_k","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"This definition can sometimes be more useful when looking for directions which discriminate among clusters containing widely-varying numbers of observations.","category":"page"},{"location":"lda/#Multi-class-LDA","page":"Linear Discriminant Analysis","title":"Multi-class LDA","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package defines a MulticlassLDA type to represent a multi-class LDA model, as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"MulticlassLDA","category":"page"},{"location":"lda/#MultivariateStats.MulticlassLDA","page":"Linear Discriminant Analysis","title":"MultivariateStats.MulticlassLDA","text":"A multi-class linear discriminant model type has following fields:\n\nproj is the projection matrix\npmeans is the projected means of all classes\nstats is an instance of MulticlassLDAStats type that captures all statistics computed to train the model (which we will discuss later).\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Several methods are provided to access properties of the LDA model. Let M be an instance of MulticlassLDA:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{MulticlassLDA}, ::Int, ::DenseMatrix{T}, ::AbstractVector{Int}; kwargs...) where T<:Real\npredict(::MulticlassLDA, ::AbstractVecOrMat{<:Real})\nmean(::MulticlassLDA)\nsize(::MulticlassLDA)\nlength(::MulticlassLDA)","category":"page"},{"location":"lda/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{MulticlassLDA}, Int64, DenseMatrix{T}, AbstractVector{Int64}}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsBase.fit","text":"fit(MulticlassLDA, nc, X, y; ...)\n\nPerform multi-class LDA over a given data set X and collecttion of labels y.\n\nThis function returns the resultant multi-class LDA model as an instance of MulticlassLDA.\n\nParameters\n\nnc:  the number of classes\nX:   the matrix of input samples, of size (d, n). Each column in X is an observation.\ny:   the vector of class labels, of length n. Each element of y must be an integer between 1 and nc.\n\nKeyword arguments\n\nmethod: The choice of methods:\n:gevd: based on generalized eigenvalue decomposition (default).\n:whiten: first derive a whitening transform from Sw and then solve the problem based on eigenvalue\ndecomposition of the whiten Sb.\noutdim: The output dimension, i.e. dimension of the transformed space min(d, nc-1)\nregcoef: The regularization coefficient (default: 1.0e-6). A positive value regcoef * eigmax(Sw)   is added to the diagonal of Sw to improve numerical stability.\ncovestimator_between: Custom covariance estimator for between-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\ncovestimator_within:  Custom covariance estimator for within-class covariance (default: SimpleCovariance()).   The covariance matrix will be calculated as cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc)).   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n\nNotes:\n\nThe resultant projection matrix P satisfies:\n\nmathbfP^T (mathbfS_w + kappa mathbfI) mathbfP = mathbfI\n\nHere, kappa equals regcoef * eigmax(Sw). The columns of P are arranged in descending order of the corresponding generalized eigenvalues.\n\nNote that MulticlassLDA does not currently support the normalized version using mathbfS_w^* and mathbfS_b^* (see SubspaceLDA).\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{MulticlassLDA, AbstractVecOrMat{<:Real}}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(M, x)\n\nTransform input sample(s) in x to the output space of MC-LDA model M. Here, x can be either a sample vector or a matrix comprised of samples in columns.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Statistics.mean-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Statistics.mean","text":"mean(M::MulticlassLDA)\n\nGet the overall sample mean vector (of length d).\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.size-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Base.size","text":"size(M)\n\nGet the input (i.e the dimension of the observation space) and output (i.e the dimension of the transformed features) dimensions of the model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{MulticlassLDA}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(M)\n\nGet the sample dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Subspace-Linear-Discriminant-Analysis","page":"Linear Discriminant Analysis","title":"Subspace Linear Discriminant Analysis","text":"","category":"section"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package also defines a SubspaceLDA type to represent a multi-class LDA model for high-dimensional spaces. MulticlassLDA, because it stores the scatter matrices, is not well-suited for high-dimensional data. For example, if you are performing LDA on images, and each image has 10^6 pixels, then the scatter matrices would contain 10^12 elements, far too many to store directly. SubspaceLDA calculates the projection direction without the intermediary of the scatter matrices, by focusing on the subspace that lies within the span of the within-class scatter. This also serves to regularize the computation.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"SubspaceLDA supports all the same methods as MulticlassLDA, with the exception of the functions that return a scatter matrix.  The overall projection is represented as a factorization P*L, where P*x projects data points to the subspace spanned by the within-class scatter, and L is the LDA projection in the subspace.  The projection directions w (the columns of projection(M)) satisfy the equation","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"   mathbfP^T mathbfS_b mathbfw = lambda mathbfP^T mathbfS_w mathbfw","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"When P is of full rank (e.g., if there are more data points than dimensions), then this equation guarantees that mathbfS_b mathbfw = lambda mathbfS_w mathbfw will also hold.","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"The package defines a SubspaceLDA type to represent a multi-class LDA model, as:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"SubspaceLDA","category":"page"},{"location":"lda/#MultivariateStats.SubspaceLDA","page":"Linear Discriminant Analysis","title":"MultivariateStats.SubspaceLDA","text":"Subspace LDA model type has following fields:\n\nprojw: the projection matrix of the subspace spanned by the between-class scatter\nprojLDA: the projection matrix of the subspace spanned by the within-class scatter\nλ: the projection eigenvalues\ncmeans: the class centroids\ncweights: the class weights\n\n\n\n\n\n","category":"type"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"Several methods are provided to access properties of the LDA model. Let M be an instance of SubspaceLDA:","category":"page"},{"location":"lda/","page":"Linear Discriminant Analysis","title":"Linear Discriminant Analysis","text":"fit(::Type{SubspaceLDA}, ::DenseMatrix{T}, ::AbstractVector{Int}, ::Int; kwargs...) where T<:Real\npredict(::SubspaceLDA, ::AbstractVecOrMat{<:Real})\nmean(::SubspaceLDA)\nsize(::SubspaceLDA)\nlength(::SubspaceLDA)\neigvals(::SubspaceLDA)","category":"page"},{"location":"lda/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{SubspaceLDA}, DenseMatrix{T}, AbstractVector{Int64}, Int64}} where T<:Real","page":"Linear Discriminant Analysis","title":"StatsBase.fit","text":"fit(SubspaceLDA, X, labels; normalize=true)\n\nFit an subspace projection of LDA model using the equivalent of mathbfS_w^* and mathbfS_b^*`.\n\nNote: Subspace LDA also supports the normalized version of LDA via the normalize keyword.\n\n\n\n\n\n","category":"method"},{"location":"lda/#StatsBase.predict-Tuple{SubspaceLDA, AbstractVecOrMat{<:Real}}","page":"Linear Discriminant Analysis","title":"StatsBase.predict","text":"predict(M, x)\n\nTransform input sample(s) in x to the output space of LDA model M. Here, x can be either a sample vector or a matrix comprised of samples in columns.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.size-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Base.size","text":"size(M)\n\nGet the input (i.e the dimension of the observation space) and output (i.e the dimension of the subspace projection) dimensions of the model M.\n\n\n\n\n\n","category":"method"},{"location":"lda/#Base.length-Tuple{SubspaceLDA}","page":"Linear Discriminant Analysis","title":"Base.length","text":"length(M)\n\nGet dimension of the LDA model.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Data-Transformation","page":"Data Transformation","title":"Data Transformation","text":"","category":"section"},{"location":"whiten/#Whitening","page":"Data Transformation","title":"Whitening","text":"","category":"section"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"A whitening transformation is a decorrelation transformation that transforms a set of random variables into a set of new random variables with identity covariance (uncorrelated with unit variances).","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"In particular, suppose a random vector has covariance mathbfC, then a whitening transform mathbfW is one that satisfy:","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"   mathbfW^T mathbfC mathbfW = mathbfI","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Note that mathbfW is generally not unique. In particular, if mathbfW is a whitening transform, so is any of its rotation mathbfW mathbfR with mathbfR^T mathbfR = mathbfI.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"The package uses Whitening to represent a whitening transform.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Whitening","category":"page"},{"location":"whiten/#MultivariateStats.Whitening","page":"Data Transformation","title":"MultivariateStats.Whitening","text":"A whitening transform representation.\n\n\n\n\n\n","category":"type"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Whitening transformation can be fitted to data using the fit method.","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"fit(::Type{Whitening}, X::AbstractMatrix{T}; kwargs...) where {T<:Real}\nMultivariateStats.transform(::Whitening, ::AbstractVecOrMat{<:Real})\nlength(::Whitening)\nmean(::Whitening)\nsize(::Whitening)","category":"page"},{"location":"whiten/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{Whitening}, AbstractMatrix{T}}} where T<:Real","page":"Data Transformation","title":"StatsBase.fit","text":"fit(Whitening, X::AbstractMatrix{T}; kwargs...)\n\nEstimate a whitening transform from the data given in X.\n\nThis function returns an instance of Whitening\n\nKeyword Arguments:\n\nregcoef: The regularization coefficient. The covariance will be regularized as follows when regcoef is positive C + (eigmax(C) * regcoef) * eye(d). Default values is zero(T).\ndims: if 1 the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if 2 the transformation calculated from the column samples. The default is nothing, which is equivalent to dims=2 with a deprecation warning.\nmean: The mean vector, which can be either of:\n0: the input data has already been centralized\nnothing: this function will compute the mean (default)\na pre-computed mean vector\n\nNote: This function internally relies on cov_whitening to derive the transformation W.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#MultivariateStats.transform-Tuple{Whitening, AbstractVecOrMat{<:Real}}","page":"Data Transformation","title":"MultivariateStats.transform","text":"transform(f, x)\n\nApply the whitening transform f to a vector or a matrix x with samples in columns, as mathbfW^T (mathbfx - boldsymbolmu).\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Base.length-Tuple{Whitening}","page":"Data Transformation","title":"Base.length","text":"length(f)\n\nGet the dimension of the  whitening transform f.\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Statistics.mean-Tuple{Whitening}","page":"Data Transformation","title":"Statistics.mean","text":"mean(f)\n\nGet the mean vector of the whitening transformation f.\n\nNote: if mean is empty, this function returns a zero vector of length(f).\n\n\n\n\n\n","category":"method"},{"location":"whiten/#Base.size-Tuple{Whitening}","page":"Data Transformation","title":"Base.size","text":"size(f)\n\nDimensions of the coefficient matrix of the whitening transform f.\n\n\n\n\n\n","category":"method"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"Additional methods","category":"page"},{"location":"whiten/","page":"Data Transformation","title":"Data Transformation","text":"cov_whitening\ncov_whitening!","category":"page"},{"location":"whiten/#MultivariateStats.cov_whitening","page":"Data Transformation","title":"MultivariateStats.cov_whitening","text":"cov_whitening(C)\n\nDerive the whitening transform coefficient matrix W given the covariance matrix C. Here, C can be either a square matrix, or an instance of Cholesky.\n\nInternally, this function solves the whitening transform using Cholesky factorization. The rationale is as follows: let mathbfC = mathbfU^T mathbfU and mathbfW = mathbfU^-1, then mathbfW^T mathbfC mathbfW = mathbfI.\n\nNote: The return matrix W is an upper triangular matrix.\n\n\n\n\n\ncov_whitening(C, regcoef)\n\nDerive a whitening transform based on a regularized covariance, as C + (eigmax(C) * regcoef) * eye(d).\n\n\n\n\n\n","category":"function"},{"location":"whiten/#MultivariateStats.cov_whitening!","page":"Data Transformation","title":"MultivariateStats.cov_whitening!","text":"cov_whitening!(C)\n\nIn-place version of cov_whitening(C), in which the input matrix C will be overwritten during computation. This can be more efficient when C is no longer used.\n\n\n\n\n\ncov_whitening!(C, regcoef)\n\nIn-place version of cov_whitening(C, regcoef), in which the input matrix C will be overwritten during computation. This can be more efficient when C is no longer used.\n\n\n\n\n\n","category":"function"},{"location":"#MultivariateStats.jl-Documentation","page":"Home","title":"MultivariateStats.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MultivariateStats\nDocTestSetup = quote\n    using Statistics\n    using Random\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultivariateStats.jl is a Julia package for multivariate statistical analysis. It provides a rich set of useful analysis techniques, such as PCA, CCA, LDA, ICA, etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"whiten.md\", \"lreg.md\", \"lda.md\", \"pca.md\", \"mda.md\", \"api.md\"]\nDepth = 2","category":"page"},{"location":"","page":"Home","title":"Home","text":"Notes: All methods implemented in this package adopt the column-major convention of JuliaStats: in a data matrix, each column corresponds to a sample/observation, while each row corresponds to a feature (variable or attribute).","category":"page"},{"location":"lreg/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides functions to perform Linear Least Square, Ridge, and Isotonic Regression.","category":"page"},{"location":"lreg/#Examples","page":"Regression","title":"Examples","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"using Plots\ngr(fmt=:svg)","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Performing llsq regression on cars data set:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"using MultivariateStats, RDatasets, Plots\n\n# load cars dataset\ncars = dataset(\"datasets\", \"cars\")\n\n# calculate regression models\na = llsq(cars[!,:Speed], cars[!, :Dist])\nb = isotonic(cars[!,:Speed], cars[!, :Dist])\n\n# plot results\np = scatter(cars[!,:Speed], cars[!,:Dist], xlab=\"Speed\", ylab=\"Distance\",\n            leg=:topleft, lab=\"data\")\nlet xs = cars[!,:Speed]\n    plot!(p, xs, map(x->a[1]*x+a[2], xs), lab=\"llsq\")\n    plot!(p, xs, b, lab=\"isotonic\")\nend","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"For a single response vector y (without using bias):","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"# prepare data\nX = rand(1000, 3)               # feature matrix\na0 = rand(3)                    # ground truths\ny = X * a0 + 0.1 * randn(1000)  # generate response\n\n# solve using llsq\na = llsq(X, y; bias=false)\n\n# do prediction\nyp = X * a\n\n# measure the error\nrmse = sqrt(mean(abs2.(y - yp)))\nprint(\"rmse = $rmse\")","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"For a single response vector y (using bias):","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"# prepare data\nX = rand(1000, 3)\na0, b0 = rand(3), rand()\ny = X * a0 .+ b0 .+ 0.1 * randn(1000)\n\n# solve using llsq\nsol = llsq(X, y)\n\n# extract results\na, b = sol[1:end-1], sol[end]\n\n# do prediction\nyp = X * a .+ b'","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"For a matrix of column-stored regressors X and a matrix comprised of multiple columns of dependent variables Y:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"# prepare data\nX = rand(3, 1000)\nA0, b0 = rand(3, 5), rand(1, 5)\nY = (X' * A0 .+ b0) + 0.1 * randn(1000, 5)\n\n# solve using llsq\nsol = llsq(X, Y, dims=2)\n\n# extract results\nA, b = sol[1:end-1,:], sol[end,:]\n\n# do prediction\nYp = X'*A .+ b'","category":"page"},{"location":"lreg/#Linear-Least-Square","page":"Regression","title":"Linear Least Square","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Linear Least Square is to find linear combination(s) of given variables to fit the responses by minimizing the squared error between them. This can be formulated as an optimization as follows:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX mathbfa + b)^2","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Sometimes, the coefficient matrix is given in a transposed form, in which case, the optimization is modified as:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX^T mathbfa + b)^2","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides following functions to solve the above problems:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"llsq","category":"page"},{"location":"lreg/#MultivariateStats.llsq","page":"Regression","title":"MultivariateStats.llsq","text":"llsq(X, y; ...)\n\nSolve the linear least square problem.\n\nHere, y can be either a vector, or a matrix where each column is a response vector.\n\nThis function accepts two keyword arguments:\n\ndims: whether input observations are stored as rows (1) or columns (2). (default is 1)\nbias: whether to include the bias term b. (default is true)\n\nThe function results the solution a. In particular, when y is a vector (matrix), a is also a vector (matrix). If bias is true, then the returned array is augmented as [a; b].\n\n\n\n\n\n","category":"function"},{"location":"lreg/#Ridge-Regression","page":"Regression","title":"Ridge Regression","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Compared to linear least square, Ridge Regression uses an additional quadratic term to regularize the problem:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX mathbfa + b)^2 +\n    frac12 mathbfa^T mathbfQ mathbfa","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The transposed form:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"    mathopmathrmminimize_(mathbfa b) \n    frac12 mathbfy - (mathbfX^T mathbfa + b)^2 +\n    frac12 mathbfa^T mathbfQ mathbfa","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides following functions to solve the above problems:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"ridge","category":"page"},{"location":"lreg/#MultivariateStats.ridge","page":"Regression","title":"MultivariateStats.ridge","text":"ridge(X, y, r; ...)\n\nSolve the ridge regression problem.\n\nHere, y can be either a vector, or a matrix where each column is a response vector.\n\nThe argument r gives the quadratic regularization matrix Q, which can be in either of the following forms:\n\nr is a real scalar, then Q is considered to be r * eye(n), where n is the dimension of a.\nr is a real vector, then Q is considered to be diagm(r).\nr is a real symmetric matrix, then Q is simply considered to be r.\n\nThis function accepts two keyword arguments:\n\ndims: whether input observations are stored as rows (1) or columns (2). (default is 1)\nbias: whether to include the bias term b. (default is true)\n\nThe function results the solution a. In particular, when y is a vector (matrix), a is also a vector (matrix). If bias is true, then the returned array is augmented as [a; b].\n\n\n\n\n\n","category":"function"},{"location":"lreg/#Isotonic-Regression","page":"Regression","title":"Isotonic Regression","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"Isotonic regression or monotonic regression fits a sequence of observations into a fitted line that is non-decreasing (or non-increasing) everywhere. The problem defined as a weighted least-squares fit hat y_i approx y_i for all i, subject to the constraint that hat y_i leq hat y_j whenever x_i leq x_j. This gives the following quadratic program:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"min sum_i=1^n w_i(hat y_i-y_i)^2\ntext  subject to   hat y_i leq hat y_j\ntext for all  (ij) in E","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"where E=(ij)x_ileq x_j specifies the partial ordering of the observed inputs x_i.","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"The package provides following functions to solve the above problems:","category":"page"},{"location":"lreg/","page":"Regression","title":"Regression","text":"isotonic","category":"page"},{"location":"lreg/#MultivariateStats.isotonic","page":"Regression","title":"MultivariateStats.isotonic","text":"isotonic(x, y[, w])\n\nSolve the isotonic regression problem using the pool adjacent violators algorithm[1].\n\nHere x is the regressor vector, y is response vector, and w is an optional weights vector.\n\nThe function returns a prediction vector of the same size as the regressor vector x.\n\n\n\n\n\n","category":"function"},{"location":"lreg/","page":"Regression","title":"Regression","text":"","category":"page"},{"location":"lreg/#References","page":"Regression","title":"References","text":"","category":"section"},{"location":"lreg/","page":"Regression","title":"Regression","text":"[1] Best, M.J., Chakravarti, N. Active set algorithms for isotonic regression; A unifying framework. Mathematical Programming 47, 425–439 (1990).","category":"page"},{"location":"mds/#Multidimensional-Scaling","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"In general, Multidimensional Scaling (MDS) refers to techniques that transforms samples into lower dimensional space while preserving the inter-sample distances as well as possible.","category":"page"},{"location":"mds/#Example","page":"Multidimensional Scaling","title":"Example","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"using Plots\ngr(fmt=:svg)","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Performing MDS on Iris data set:","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"using MultivariateStats, RDatasets, Plots\n\n# load iris dataset\niris = dataset(\"datasets\", \"iris\")\n\n# take half of the dataset\nX = Matrix(iris[1:2:end,1:4])'\nX_labels = Vector(iris[1:2:end,5])\nnothing # hide","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Suppose X is our data matrix, with each observation in a column. We train a MDS model, allowing up to 3 dimensions:","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"M = fit(MDS, X; maxoutdim=3, distances=false)","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Then, apply MDS model to get an embedding of our data in 3D space:","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Y = predict(M)","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"Now, we group results by testing set labels for color coding and visualize first 3 principal components in 3D interactive plot","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"setosa = Y[:,X_labels.==\"setosa\"]\nversicolor = Y[:,X_labels.==\"versicolor\"]\nvirginica = Y[:,X_labels.==\"virginica\"]\n\np = scatter(setosa[1,:],setosa[2,:],setosa[3,:],marker=:circle,linewidth=0)\nscatter!(versicolor[1,:],versicolor[2,:],versicolor[3,:],marker=:circle,linewidth=0)\nscatter!(virginica[1,:],virginica[2,:],virginica[3,:],marker=:circle,linewidth=0)","category":"page"},{"location":"mds/#Classical-Multidimensional-Scaling","page":"Multidimensional Scaling","title":"Classical Multidimensional Scaling","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"This package defines a MDS type to represent a classical MDS model [1], and provides a set of methods to access the properties.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"MDS","category":"page"},{"location":"mds/#MultivariateStats.MDS","page":"Multidimensional Scaling","title":"MultivariateStats.MDS","text":"Classical Multidimensional Scaling (MDS), also known as Principal Coordinates Analysis (PCoA), is a specific technique in this family that accomplishes the embedding in two steps:\n\nConvert the distance matrix to a Gram matrix. This conversion is based on\n\nthe following relations between a distance matrix D and a Gram matrix G:\n\nmathrmsqr(mathbfD) = mathbfg mathbf1^T + mathbf1 mathbfg^T - 2 mathbfG\n\nHere, mathrmsqr(mathbfD) indicates the element-wise square of mathbfD, and mathbfg is the diagonal elements of mathbfG. This relation is itself based on the following decomposition of squared Euclidean distance:\n\n mathbfx - mathbfy ^2 =  mathbfx ^2 +  mathbfy ^2 - 2 mathbfx^T mathbfy\n\nPerform eigenvalue decomposition of the Gram matrix to derive the coordinates.\n\nNote:  The Gramian derived from D may have non-positive or degenerate eigenvalues.  The subspace of non-positive eigenvalues is projected out of the MDS solution so that the strain function is minimized in a least-squares sense.  If the smallest remaining eigenvalue that is used for the MDS is degenerate, then the solution is not unique, as any linear combination of degenerate eigenvectors will also yield a MDS solution with the same strain value.\n\n\n\n\n\n","category":"type"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"The MDS method type comes with several methods where M be an instance of MDS, d be the dimension of observations, and p be the output dimension, i.e. the embedding dimension, and n is the number of the observations.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"fit(::Type{MDS}, ::AbstractMatrix{T}; kwargs) where {T<:Real}\npredict(::MDS)\npredict(::MDS, ::AbstractVector{<:Real})\nsize(::MDS)\nprojection(M::MDS)\nloadings(M::MDS)\neigvals(M::MDS)\neigvecs(M::MDS)\nstress","category":"page"},{"location":"mds/#StatsBase.fit-Union{Tuple{T}, Tuple{Type{MDS}, AbstractMatrix{T}}} where T<:Real","page":"Multidimensional Scaling","title":"StatsBase.fit","text":"Compute an embedding of points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument distances:\n\nmds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\n\nwhere X is the data matrix. Distances between pairs of columns of X are computed using the Euclidean norm. This is equivalent to performing PCA on X.\n\nmds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\n\nwhere D is a symmetric matrix D of distances between points.\n\n\n\n\n\n","category":"method"},{"location":"mds/#StatsBase.predict-Tuple{MDS}","page":"Multidimensional Scaling","title":"StatsBase.predict","text":"predict(M)\n\nReturns a coordinate matrix of size (p n) for the MDS model M, where each column is the coordinates for an observation in the embedding space.\n\n\n\n\n\n","category":"method"},{"location":"mds/#StatsBase.predict-Tuple{MDS, AbstractVector{<:Real}}","page":"Multidimensional Scaling","title":"StatsBase.predict","text":"predict(M, x::AbstractVector)\n\nCalculate the out-of-sample transformation of the observation x for the MDS model M. Here, x is a vector of length d.\n\n\n\n\n\n","category":"method"},{"location":"mds/#Base.size-Tuple{MDS}","page":"Multidimensional Scaling","title":"Base.size","text":"size(M)\n\nReturns tuple where the first value is the MDS model M input dimension, i.e the dimension of the observation space, and the second value is the output dimension, i.e the dimension of the embedding.\n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.projection-Tuple{MDS}","page":"Multidimensional Scaling","title":"MultivariateStats.projection","text":"projection(M)\n\nGet the MDS model M eigenvectors matrix (of size (n p)) of the embedding space. The eigenvectors are arranged in descending order of the corresponding eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.loadings-Tuple{MDS}","page":"Multidimensional Scaling","title":"MultivariateStats.loadings","text":"loadings(M::MDS)\n\nGet the loading of the MDS model M.\n\n\n\n\n\n","category":"method"},{"location":"mds/#LinearAlgebra.eigvals-Tuple{MDS}","page":"Multidimensional Scaling","title":"LinearAlgebra.eigvals","text":"eigvals(M::MDS)\n\nGet the eigenvalues of the MDS model M.\n\n\n\n\n\n","category":"method"},{"location":"mds/#LinearAlgebra.eigvecs-Tuple{MDS}","page":"Multidimensional Scaling","title":"LinearAlgebra.eigvecs","text":"eigvecs(M::MDS)\n\nGet the MDS model M eigenvectors matrix. \n\n\n\n\n\n","category":"method"},{"location":"mds/#MultivariateStats.stress","page":"Multidimensional Scaling","title":"MultivariateStats.stress","text":"stress(M)\n\nGet the stress of the MDS mode M.\n\n\n\n\n\n","category":"function"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"This package provides following functions related to classical MDS.","category":"page"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"gram2dmat\ngram2dmat!\ndmat2gram\ndmat2gram!","category":"page"},{"location":"mds/#MultivariateStats.gram2dmat","page":"Multidimensional Scaling","title":"MultivariateStats.gram2dmat","text":"gram2dmat(G)\n\nConvert a Gram matrix G to a distance matrix.\n\n\n\n\n\n","category":"function"},{"location":"mds/#MultivariateStats.gram2dmat!","page":"Multidimensional Scaling","title":"MultivariateStats.gram2dmat!","text":"gram2dmat!(D, G)\n\nConvert a Gram matrix G to a distance matrix, and write the results to D.\n\n\n\n\n\n","category":"function"},{"location":"mds/#MultivariateStats.dmat2gram","page":"Multidimensional Scaling","title":"MultivariateStats.dmat2gram","text":"dmat2gram(D)\n\nConvert a distance matrix D to a Gram matrix.\n\n\n\n\n\n","category":"function"},{"location":"mds/#MultivariateStats.dmat2gram!","page":"Multidimensional Scaling","title":"MultivariateStats.dmat2gram!","text":"dmat2gram!(G, D)\n\nConvert a distance matrix D to a Gram matrix, and write the results to G.\n\n\n\n\n\n","category":"function"},{"location":"mds/#References","page":"Multidimensional Scaling","title":"References","text":"","category":"section"},{"location":"mds/","page":"Multidimensional Scaling","title":"Multidimensional Scaling","text":"[1]: Ingwer Borg and Patrick J. F. Groenen, \"Modern Multidimensional Scaling: Theory and Applications\", Springer, pp. 201–268, 2005.","category":"page"}]
}
