<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Principal Component Analysis · MultivariateStats.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultivariateStats.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../whiten/">Data Transformation</a></li><li><a class="tocitem" href="../lreg/">Regression</a></li><li><a class="tocitem" href="../lda/">Linear Discriminant Analysis</a></li><li class="is-active"><a class="tocitem" href>Principal Component Analysis</a><ul class="internal"><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Linear-Principal-Component-Analysis"><span>Linear Principal Component Analysis</span></a></li><li><a class="tocitem" href="#Kernel-Principal-Component-Analysis"><span>Kernel Principal Component Analysis</span></a></li><li><a class="tocitem" href="#Probabilistic-Principal-Component-Analysis"><span>Probabilistic Principal Component Analysis</span></a></li></ul></li><li><a class="tocitem" href="../ica/">Independent Component Analysis</a></li><li><a class="tocitem" href="../cca/">Canonical Correlation Analysis</a></li><li><a class="tocitem" href="../fa/">Factor Analysis</a></li><li><a class="tocitem" href="../mds/">Multidimensional Scaling</a></li><li><a class="tocitem" href="../api/">Development</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Principal Component Analysis</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Principal Component Analysis</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/master/docs/src/pca.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Principal-Component-Analysis">Principal Component Analysis</a><a id="Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Principal-Component-Analysis" title="Permalink"></a></h1><p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA) derives an orthogonal projection to convert a given set of observations to linearly uncorrelated variables, called <em>principal components</em>.</p><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>Performing <a href="#MultivariateStats.PCA"><code>PCA</code></a> on <em>Iris</em> data set:</p><pre><code class="language-julia hljs">using MultivariateStats, RDatasets, Plots

# load iris dataset
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)

# split half to training set
Xtr = Matrix(iris[1:2:end,1:4])&#39;
Xtr_labels = Vector(iris[1:2:end,5])

# split other half to testing set
Xte = Matrix(iris[2:2:end,1:4])&#39;
Xte_labels = Vector(iris[2:2:end,5])</code></pre><p>Suppose <code>Xtr</code> and <code>Xte</code> are training and testing data matrix, with each observation in a column. We train a PCA model, allowing up to 3 dimensions:</p><pre><code class="language-julia hljs">M = fit(PCA, Xtr; maxoutdim=3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PCA(indim = 4, outdim = 3, principalratio = 0.9957325846529407)

Pattern matrix (unstandardized loadings):
────────────────────────────────────
         PC1         PC2         PC3
────────────────────────────────────
1   0.70954    0.344711   -0.160106
2  -0.227592   0.29865     0.215417
3   1.77976   -0.0797511   0.0197705
4   0.764206  -0.0453779   0.166764
────────────────────────────────────

Importance of components:
─────────────────────────────────────────────────────────
                                PC1        PC2        PC3
─────────────────────────────────────────────────────────
SS Loadings (Eigenvalues)  4.3068    0.216437   0.100239
Variance explained         0.927532  0.0466128  0.021588
Cumulative variance        0.927532  0.974145   0.995733
Proportion explained       0.931507  0.0468125  0.0216805
Cumulative proportion      0.931507  0.978319   1.0
─────────────────────────────────────────────────────────</code></pre><p>Then, apply PCA model to the testing set</p><pre><code class="language-julia hljs">Yte = predict(M, Xte)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3×75 Matrix{Float64}:
  2.72714    2.75491    2.32396   …  -1.92047   -1.74161   -1.37706
 -0.230916  -0.406149   0.646374      0.246554   0.127625  -0.280295
 -0.253119  -0.0271266  0.230469      0.180044   0.123165   0.314992</code></pre><p>And, reconstruct testing observations (approximately) to the original space</p><pre><code class="language-julia hljs">Xr = reconstruct(M, Yte)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4×75 Matrix{Float64}:
 4.86449  4.61087   5.40782   5.00775   …  6.79346  6.58825  6.46774  5.94384
 3.04262  3.08695   3.89061   3.39069      3.20785  3.13416  3.03873  2.94737
 1.46099  1.48132   1.68656   1.48668      5.91124  5.39197  5.25542  5.02469
 0.10362  0.229519  0.421233  0.221041     2.28224  1.99665  1.91243  1.91901</code></pre><p>Now, we group results by testing set labels for color coding and visualize first 3 principal components in 3D plot</p><pre><code class="language-julia hljs">setosa = Yte[:,Xte_labels.==&quot;setosa&quot;]
versicolor = Yte[:,Xte_labels.==&quot;versicolor&quot;]
virginica = Yte[:,Xte_labels.==&quot;virginica&quot;]

p = scatter(setosa[1,:],setosa[2,:],setosa[3,:],marker=:circle,linewidth=0)
scatter!(versicolor[1,:],versicolor[2,:],versicolor[3,:],marker=:circle,linewidth=0)
scatter!(virginica[1,:],virginica[2,:],virginica[3,:],marker=:circle,linewidth=0)
plot!(p,xlabel=&quot;PC1&quot;,ylabel=&quot;PC2&quot;,zlabel=&quot;PC3&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip020">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip020)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip021">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<defs>
  <clipPath id="clip022">
    <rect x="287" y="47" width="2066" height="1377"/>
  </clipPath>
</defs>
<path clip-path="url(#clip022)" d="
M1204.31 1149.71 L1179.68 627.219 L1312.89 444.958 L1501.91 539.937 L1481.55 1029.94 L1393.41 1354.16 L1204.31 1149.71  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  862.042,1148.1 1254.23,805.522 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1254.23,805.522 1245.21,205.842 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  948.11,1182.98 1330.4,828.853 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1330.4,828.853 1331.84,222.157 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1037.4,1219.16 1408.89,852.895 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1408.89,852.895 1421.49,239.04 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1130.11,1256.73 1489.82,877.682 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1489.82,877.682 1514.31,256.522 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1226.43,1295.76 1573.29,903.249 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1573.29,903.249 1610.49,274.636 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1326.58,1336.34 1659.43,929.634 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1659.43,929.634 1710.2,293.415 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1430.78,1378.57 1748.37,956.876 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1748.37,956.876 1813.65,312.897 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  809.917,1126.98 1476.45,1397.07 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  862.042,1148.1 867.771,1143.09 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  948.11,1182.98 953.713,1177.79 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1037.4,1219.16 1042.87,1213.77 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1130.11,1256.73 1135.42,1251.13 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1226.43,1295.76 1231.57,1289.94 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1326.58,1336.34 1331.52,1330.29 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1430.78,1378.57 1435.52,1372.27 
  "/>
<path clip-path="url(#clip020)" d="M779.834 1168.11 L809.51 1168.11 L809.51 1172.04 L779.834 1172.04 L779.834 1168.11 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M833.769 1166.3 Q837.126 1167.02 839.001 1169.29 Q840.899 1171.55 840.899 1174.89 Q840.899 1180 837.38 1182.8 Q833.862 1185.61 827.38 1185.61 Q825.205 1185.61 822.89 1185.17 Q820.598 1184.75 818.144 1183.89 L818.144 1179.38 Q820.089 1180.51 822.404 1181.09 Q824.718 1181.67 827.242 1181.67 Q831.64 1181.67 833.931 1179.93 Q836.246 1178.2 836.246 1174.89 Q836.246 1171.83 834.093 1170.12 Q831.964 1168.38 828.144 1168.38 L824.117 1168.38 L824.117 1164.54 L828.33 1164.54 Q831.779 1164.54 833.607 1163.18 Q835.436 1161.79 835.436 1159.19 Q835.436 1156.53 833.538 1155.12 Q831.663 1153.68 828.144 1153.68 Q826.223 1153.68 824.024 1154.1 Q821.825 1154.52 819.186 1155.4 L819.186 1151.23 Q821.848 1150.49 824.163 1150.12 Q826.501 1149.75 828.561 1149.75 Q833.885 1149.75 836.987 1152.18 Q840.089 1154.59 840.089 1158.71 Q840.089 1161.58 838.445 1163.57 Q836.802 1165.54 833.769 1166.3 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M866.851 1202.98 L896.527 1202.98 L896.527 1206.92 L866.851 1206.92 L866.851 1202.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M910.647 1215.88 L926.967 1215.88 L926.967 1219.81 L905.022 1219.81 L905.022 1215.88 Q907.685 1213.12 912.268 1208.49 Q916.874 1203.84 918.055 1202.5 Q920.3 1199.97 921.18 1198.24 Q922.083 1196.48 922.083 1194.79 Q922.083 1192.03 920.138 1190.3 Q918.217 1188.56 915.115 1188.56 Q912.916 1188.56 910.462 1189.33 Q908.032 1190.09 905.254 1191.64 L905.254 1186.92 Q908.078 1185.78 910.532 1185.2 Q912.985 1184.63 915.022 1184.63 Q920.393 1184.63 923.587 1187.31 Q926.782 1190 926.782 1194.49 Q926.782 1196.62 925.971 1198.54 Q925.184 1200.44 923.078 1203.03 Q922.499 1203.7 919.397 1206.92 Q916.296 1210.11 910.647 1215.88 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M955.776 1239.17 L985.452 1239.17 L985.452 1243.1 L955.776 1243.1 L955.776 1239.17 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M996.354 1252.06 L1003.99 1252.06 L1003.99 1225.69 L995.683 1227.36 L995.683 1223.1 L1003.95 1221.44 L1008.62 1221.44 L1008.62 1252.06 L1016.26 1252.06 L1016.26 1256 L996.354 1256 L996.354 1252.06 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1097.02 1262.08 Q1093.41 1262.08 1091.58 1265.65 Q1089.78 1269.19 1089.78 1276.32 Q1089.78 1283.42 1091.58 1286.99 Q1093.41 1290.53 1097.02 1290.53 Q1100.66 1290.53 1102.46 1286.99 Q1104.29 1283.42 1104.29 1276.32 Q1104.29 1269.19 1102.46 1265.65 Q1100.66 1262.08 1097.02 1262.08 M1097.02 1258.38 Q1102.83 1258.38 1105.89 1262.98 Q1108.97 1267.57 1108.97 1276.32 Q1108.97 1285.04 1105.89 1289.65 Q1102.83 1294.23 1097.02 1294.23 Q1091.21 1294.23 1088.14 1289.65 Q1085.08 1285.04 1085.08 1276.32 Q1085.08 1267.57 1088.14 1262.98 Q1091.21 1258.38 1097.02 1258.38 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1185.38 1328.66 L1193.02 1328.66 L1193.02 1302.29 L1184.71 1303.96 L1184.71 1299.7 L1192.97 1298.03 L1197.65 1298.03 L1197.65 1328.66 L1205.29 1328.66 L1205.29 1332.59 L1185.38 1332.59 L1185.38 1328.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1289.11 1369.24 L1305.43 1369.24 L1305.43 1373.17 L1283.49 1373.17 L1283.49 1369.24 Q1286.15 1366.48 1290.73 1361.86 Q1295.34 1357.2 1296.52 1355.86 Q1298.77 1353.34 1299.65 1351.6 Q1300.55 1349.84 1300.55 1348.15 Q1300.55 1345.4 1298.6 1343.66 Q1296.68 1341.92 1293.58 1341.92 Q1291.38 1341.92 1288.93 1342.69 Q1286.5 1343.45 1283.72 1345 L1283.72 1340.28 Q1286.54 1339.15 1289 1338.57 Q1291.45 1337.99 1293.49 1337.99 Q1298.86 1337.99 1302.05 1340.67 Q1305.25 1343.36 1305.25 1347.85 Q1305.25 1349.98 1304.44 1351.9 Q1303.65 1353.8 1301.54 1356.39 Q1300.96 1357.06 1297.86 1360.28 Q1294.76 1363.48 1289.11 1369.24 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1402.51 1396.77 Q1405.86 1397.48 1407.74 1399.75 Q1409.64 1402.02 1409.64 1405.35 Q1409.64 1410.47 1406.12 1413.27 Q1402.6 1416.07 1396.12 1416.07 Q1393.94 1416.07 1391.63 1415.63 Q1389.34 1415.22 1386.88 1414.36 L1386.88 1409.85 Q1388.83 1410.98 1391.14 1411.56 Q1393.46 1412.14 1395.98 1412.14 Q1400.38 1412.14 1402.67 1410.4 Q1404.98 1408.66 1404.98 1405.35 Q1404.98 1402.3 1402.83 1400.59 Q1400.7 1398.85 1396.88 1398.85 L1392.85 1398.85 L1392.85 1395.01 L1397.07 1395.01 Q1400.52 1395.01 1402.34 1393.64 Q1404.17 1392.25 1404.17 1389.66 Q1404.17 1387 1402.28 1385.59 Q1400.4 1384.15 1396.88 1384.15 Q1394.96 1384.15 1392.76 1384.57 Q1390.56 1384.98 1387.92 1385.86 L1387.92 1381.7 Q1390.59 1380.96 1392.9 1380.59 Q1395.24 1380.22 1397.3 1380.22 Q1402.62 1380.22 1405.72 1382.65 Q1408.83 1385.05 1408.83 1389.17 Q1408.83 1392.04 1407.18 1394.03 Q1405.54 1396 1402.51 1396.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M947.152 1390.86 L947.152 1408.71 L955.237 1408.71 Q959.725 1408.71 962.176 1406.39 Q964.626 1404.06 964.626 1399.77 Q964.626 1395.5 962.176 1393.18 Q959.725 1390.86 955.237 1390.86 L947.152 1390.86 M940.723 1385.57 L955.237 1385.57 Q963.226 1385.57 967.3 1389.2 Q971.406 1392.8 971.406 1399.77 Q971.406 1406.8 967.3 1410.4 Q963.226 1413.99 955.237 1413.99 L947.152 1413.99 L947.152 1433.09 L940.723 1433.09 L940.723 1385.57 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1015.62 1389.23 L1015.62 1396.01 Q1012.37 1392.99 1008.68 1391.49 Q1005.02 1390 1000.88 1390 Q992.731 1390 988.402 1394.99 Q984.074 1399.96 984.074 1409.38 Q984.074 1418.77 988.402 1423.77 Q992.731 1428.73 1000.88 1428.73 Q1005.02 1428.73 1008.68 1427.24 Q1012.37 1425.74 1015.62 1422.72 L1015.62 1429.43 Q1012.24 1431.72 1008.45 1432.87 Q1004.7 1434.01 1000.5 1434.01 Q989.707 1434.01 983.501 1427.43 Q977.294 1420.81 977.294 1409.38 Q977.294 1397.92 983.501 1391.33 Q989.707 1384.71 1000.5 1384.71 Q1004.76 1384.71 1008.52 1385.86 Q1012.31 1386.97 1015.62 1389.23 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1027.23 1427.68 L1037.74 1427.68 L1037.74 1391.43 L1026.31 1393.72 L1026.31 1387.86 L1037.67 1385.57 L1044.1 1385.57 L1044.1 1427.68 L1054.61 1427.68 L1054.61 1433.09 L1027.23 1433.09 L1027.23 1427.68 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1490.23,1378.08 827.138,1112.45 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  827.138,1112.45 743.998,426.001 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1559.17,1283.03 913.901,1039.27 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  913.901,1039.27 848.525,372.4 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1622.22,1196.12 994.093,971.623 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  994.093,971.623 943.917,323.483 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1680.08,1116.34 1068.43,908.916 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1068.43,908.916 1031.32,278.662 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1733.38,1042.85 1137.54,850.624 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1137.54,850.624 1111.7,237.442 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1782.64,974.944 1201.95,796.296 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1201.95,796.296 1185.87,199.408 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1476.45,1397.07 1787.13,968.749 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1490.23,1378.08 1481.15,1374.44 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1559.17,1283.03 1550.39,1279.71 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1622.22,1196.12 1613.71,1193.07 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1680.08,1116.34 1671.84,1113.54 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1733.38,1042.85 1725.39,1040.27 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1782.64,974.944 1774.88,972.555 
  "/>
<path clip-path="url(#clip020)" d="M1518.02 1386.09 L1547.7 1386.09 L1547.7 1390.02 L1518.02 1390.02 L1518.02 1386.09 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1558.6 1398.98 L1566.24 1398.98 L1566.24 1372.61 L1557.93 1374.28 L1557.93 1370.02 L1566.19 1368.36 L1570.87 1368.36 L1570.87 1398.98 L1578.51 1398.98 L1578.51 1402.92 L1558.6 1402.92 L1558.6 1398.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1587.95 1397.04 L1592.84 1397.04 L1592.84 1402.92 L1587.95 1402.92 L1587.95 1397.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1613.02 1371.43 Q1609.41 1371.43 1607.58 1375 Q1605.78 1378.54 1605.78 1385.67 Q1605.78 1392.78 1607.58 1396.34 Q1609.41 1399.88 1613.02 1399.88 Q1616.66 1399.88 1618.46 1396.34 Q1620.29 1392.78 1620.29 1385.67 Q1620.29 1378.54 1618.46 1375 Q1616.66 1371.43 1613.02 1371.43 M1613.02 1367.73 Q1618.83 1367.73 1621.89 1372.34 Q1624.97 1376.92 1624.97 1385.67 Q1624.97 1394.4 1621.89 1399 Q1618.83 1403.59 1613.02 1403.59 Q1607.21 1403.59 1604.13 1399 Q1601.08 1394.4 1601.08 1385.67 Q1601.08 1376.92 1604.13 1372.34 Q1607.21 1367.73 1613.02 1367.73 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1586.97 1291.04 L1616.64 1291.04 L1616.64 1294.97 L1586.97 1294.97 L1586.97 1291.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1636.73 1276.38 Q1633.12 1276.38 1631.29 1279.95 Q1629.49 1283.49 1629.49 1290.62 Q1629.49 1297.72 1631.29 1301.29 Q1633.12 1304.83 1636.73 1304.83 Q1640.37 1304.83 1642.17 1301.29 Q1644 1297.72 1644 1290.62 Q1644 1283.49 1642.17 1279.95 Q1640.37 1276.38 1636.73 1276.38 M1636.73 1272.68 Q1642.54 1272.68 1645.6 1277.29 Q1648.68 1281.87 1648.68 1290.62 Q1648.68 1299.35 1645.6 1303.95 Q1642.54 1308.54 1636.73 1308.54 Q1630.92 1308.54 1627.85 1303.95 Q1624.79 1299.35 1624.79 1290.62 Q1624.79 1281.87 1627.85 1277.29 Q1630.92 1272.68 1636.73 1272.68 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1656.9 1301.98 L1661.78 1301.98 L1661.78 1307.86 L1656.9 1307.86 L1656.9 1301.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1672.01 1273.3 L1690.37 1273.3 L1690.37 1277.24 L1676.29 1277.24 L1676.29 1285.71 Q1677.31 1285.36 1678.33 1285.2 Q1679.35 1285.02 1680.37 1285.02 Q1686.16 1285.02 1689.54 1288.19 Q1692.91 1291.36 1692.91 1296.78 Q1692.91 1302.35 1689.44 1305.46 Q1685.97 1308.54 1679.65 1308.54 Q1677.48 1308.54 1675.21 1308.16 Q1672.96 1307.79 1670.55 1307.05 L1670.55 1302.35 Q1672.64 1303.49 1674.86 1304.04 Q1677.08 1304.6 1679.56 1304.6 Q1683.56 1304.6 1685.9 1302.49 Q1688.24 1300.39 1688.24 1296.78 Q1688.24 1293.16 1685.9 1291.06 Q1683.56 1288.95 1679.56 1288.95 Q1677.68 1288.95 1675.81 1289.37 Q1673.96 1289.79 1672.01 1290.66 L1672.01 1273.3 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1661.95 1189.47 Q1658.34 1189.47 1656.51 1193.03 Q1654.71 1196.58 1654.71 1203.71 Q1654.71 1210.81 1656.51 1214.38 Q1658.34 1217.92 1661.95 1217.92 Q1665.59 1217.92 1667.39 1214.38 Q1669.22 1210.81 1669.22 1203.71 Q1669.22 1196.58 1667.39 1193.03 Q1665.59 1189.47 1661.95 1189.47 M1661.95 1185.77 Q1667.76 1185.77 1670.82 1190.37 Q1673.9 1194.96 1673.9 1203.71 Q1673.9 1212.43 1670.82 1217.04 Q1667.76 1221.62 1661.95 1221.62 Q1656.14 1221.62 1653.06 1217.04 Q1650.01 1212.43 1650.01 1203.71 Q1650.01 1194.96 1653.06 1190.37 Q1656.14 1185.77 1661.95 1185.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1682.11 1215.07 L1687 1215.07 L1687 1220.95 L1682.11 1220.95 L1682.11 1215.07 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1707.18 1189.47 Q1703.57 1189.47 1701.74 1193.03 Q1699.94 1196.58 1699.94 1203.71 Q1699.94 1210.81 1701.74 1214.38 Q1703.57 1217.92 1707.18 1217.92 Q1710.82 1217.92 1712.62 1214.38 Q1714.45 1210.81 1714.45 1203.71 Q1714.45 1196.58 1712.62 1193.03 Q1710.82 1189.47 1707.18 1189.47 M1707.18 1185.77 Q1712.99 1185.77 1716.05 1190.37 Q1719.13 1194.96 1719.13 1203.71 Q1719.13 1212.43 1716.05 1217.04 Q1712.99 1221.62 1707.18 1221.62 Q1701.37 1221.62 1698.29 1217.04 Q1695.24 1212.43 1695.24 1203.71 Q1695.24 1194.96 1698.29 1190.37 Q1701.37 1185.77 1707.18 1185.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1719.82 1109.69 Q1716.21 1109.69 1714.38 1113.26 Q1712.57 1116.8 1712.57 1123.93 Q1712.57 1131.03 1714.38 1134.6 Q1716.21 1138.14 1719.82 1138.14 Q1723.45 1138.14 1725.26 1134.6 Q1727.09 1131.03 1727.09 1123.93 Q1727.09 1116.8 1725.26 1113.26 Q1723.45 1109.69 1719.82 1109.69 M1719.82 1105.99 Q1725.63 1105.99 1728.68 1110.6 Q1731.76 1115.18 1731.76 1123.93 Q1731.76 1132.66 1728.68 1137.26 Q1725.63 1141.84 1719.82 1141.84 Q1714.01 1141.84 1710.93 1137.26 Q1707.87 1132.66 1707.87 1123.93 Q1707.87 1115.18 1710.93 1110.6 Q1714.01 1105.99 1719.82 1105.99 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1739.98 1135.29 L1744.86 1135.29 L1744.86 1141.17 L1739.98 1141.17 L1739.98 1135.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1755.1 1106.61 L1773.45 1106.61 L1773.45 1110.55 L1759.38 1110.55 L1759.38 1119.02 Q1760.4 1118.67 1761.41 1118.51 Q1762.43 1118.33 1763.45 1118.33 Q1769.24 1118.33 1772.62 1121.5 Q1776 1124.67 1776 1130.09 Q1776 1135.66 1772.53 1138.77 Q1769.05 1141.84 1762.73 1141.84 Q1760.56 1141.84 1758.29 1141.47 Q1756.04 1141.1 1753.64 1140.36 L1753.64 1135.66 Q1755.72 1136.8 1757.94 1137.35 Q1760.16 1137.91 1762.64 1137.91 Q1766.65 1137.91 1768.98 1135.8 Q1771.32 1133.7 1771.32 1130.09 Q1771.32 1126.47 1768.98 1124.37 Q1766.65 1122.26 1762.64 1122.26 Q1760.77 1122.26 1758.89 1122.68 Q1757.04 1123.1 1755.1 1123.97 L1755.1 1106.61 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1761.85 1063.75 L1769.49 1063.75 L1769.49 1037.39 L1761.18 1039.05 L1761.18 1034.8 L1769.44 1033.13 L1774.11 1033.13 L1774.11 1063.75 L1781.75 1063.75 L1781.75 1067.69 L1761.85 1067.69 L1761.85 1063.75 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1791.2 1061.81 L1796.08 1061.81 L1796.08 1067.69 L1791.2 1067.69 L1791.2 1061.81 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1816.27 1036.21 Q1812.66 1036.21 1810.83 1039.77 Q1809.02 1043.31 1809.02 1050.44 Q1809.02 1057.55 1810.83 1061.11 Q1812.66 1064.66 1816.27 1064.66 Q1819.9 1064.66 1821.71 1061.11 Q1823.54 1057.55 1823.54 1050.44 Q1823.54 1043.31 1821.71 1039.77 Q1819.9 1036.21 1816.27 1036.21 M1816.27 1032.5 Q1822.08 1032.5 1825.13 1037.11 Q1828.21 1041.69 1828.21 1050.44 Q1828.21 1059.17 1825.13 1063.78 Q1822.08 1068.36 1816.27 1068.36 Q1810.46 1068.36 1807.38 1063.78 Q1804.32 1059.17 1804.32 1050.44 Q1804.32 1041.69 1807.38 1037.11 Q1810.46 1032.5 1816.27 1032.5 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1811.1 995.844 L1818.74 995.844 L1818.74 969.478 L1810.43 971.145 L1810.43 966.886 L1818.7 965.219 L1823.37 965.219 L1823.37 995.844 L1831.01 995.844 L1831.01 999.779 L1811.1 999.779 L1811.1 995.844 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1840.46 993.899 L1845.34 993.899 L1845.34 999.779 L1840.46 999.779 L1840.46 993.899 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1855.57 965.219 L1873.93 965.219 L1873.93 969.154 L1859.85 969.154 L1859.85 977.626 Q1860.87 977.279 1861.89 977.117 Q1862.91 976.932 1863.93 976.932 Q1869.71 976.932 1873.09 980.103 Q1876.47 983.275 1876.47 988.691 Q1876.47 994.27 1873 997.372 Q1869.53 1000.45 1863.21 1000.45 Q1861.03 1000.45 1858.77 1000.08 Q1856.52 999.71 1854.11 998.969 L1854.11 994.27 Q1856.2 995.404 1858.42 995.96 Q1860.64 996.515 1863.12 996.515 Q1867.12 996.515 1869.46 994.409 Q1871.8 992.302 1871.8 988.691 Q1871.8 985.08 1869.46 982.974 Q1867.12 980.867 1863.12 980.867 Q1861.24 980.867 1859.37 981.284 Q1857.52 981.7 1855.57 982.58 L1855.57 965.219 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1721.89 1297.05 L1721.89 1314.91 L1729.98 1314.91 Q1734.47 1314.91 1736.92 1312.59 Q1739.37 1310.26 1739.37 1305.97 Q1739.37 1301.7 1736.92 1299.38 Q1734.47 1297.05 1729.98 1297.05 L1721.89 1297.05 M1715.46 1291.77 L1729.98 1291.77 Q1737.97 1291.77 1742.04 1295.4 Q1746.15 1299 1746.15 1305.97 Q1746.15 1313 1742.04 1316.6 Q1737.97 1320.19 1729.98 1320.19 L1721.89 1320.19 L1721.89 1339.29 L1715.46 1339.29 L1715.46 1291.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1790.36 1295.43 L1790.36 1302.21 Q1787.11 1299.19 1783.42 1297.69 Q1779.76 1296.2 1775.62 1296.2 Q1767.47 1296.2 1763.14 1301.19 Q1758.81 1306.16 1758.81 1315.58 Q1758.81 1324.97 1763.14 1329.97 Q1767.47 1334.93 1775.62 1334.93 Q1779.76 1334.93 1783.42 1333.44 Q1787.11 1331.94 1790.36 1328.92 L1790.36 1335.63 Q1786.98 1337.92 1783.2 1339.07 Q1779.44 1340.21 1775.24 1340.21 Q1764.45 1340.21 1758.24 1333.63 Q1752.04 1327.01 1752.04 1315.58 Q1752.04 1304.12 1758.24 1297.53 Q1764.45 1290.91 1775.24 1290.91 Q1779.5 1290.91 1783.26 1292.06 Q1787.05 1293.17 1790.36 1295.43 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M1806.4 1333.88 L1828.84 1333.88 L1828.84 1339.29 L1798.66 1339.29 L1798.66 1333.88 Q1802.32 1330.09 1808.63 1323.73 Q1814.96 1317.33 1816.58 1315.48 Q1819.67 1312.01 1820.88 1309.63 Q1822.12 1307.21 1822.12 1304.88 Q1822.12 1301.1 1819.45 1298.71 Q1816.81 1296.32 1812.54 1296.32 Q1809.52 1296.32 1806.14 1297.37 Q1802.8 1298.42 1798.98 1300.56 L1798.98 1294.06 Q1802.87 1292.5 1806.24 1291.71 Q1809.61 1290.91 1812.41 1290.91 Q1819.8 1290.91 1824.19 1294.6 Q1828.58 1298.3 1828.58 1304.47 Q1828.58 1307.4 1827.47 1310.04 Q1826.39 1312.65 1823.49 1316.22 Q1822.69 1317.14 1818.43 1321.56 Q1814.16 1325.96 1806.4 1333.88 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  797.282,1026.53 1205.58,702.433 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1205.58,702.433 1797.7,873.397 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  781.053,897.528 1202.69,589.419 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1202.69,589.419 1811.22,751.513 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  763.785,760.257 1199.65,470.556 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1199.65,470.556 1825.51,622.533 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  745.374,613.899 1196.45,345.377 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1196.45,345.377 1840.67,485.819 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  725.702,457.522 1193.08,213.365 
  "/>
<polyline clip-path="url(#clip022)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1193.08,213.365 1856.76,340.654 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  809.917,1126.98 723.086,436.725 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  797.282,1026.53 803.261,1021.79 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  781.053,897.528 787.262,892.991 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  763.785,760.257 770.241,755.966 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  745.374,613.899 752.097,609.897 
  "/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  725.702,457.522 732.715,453.859 
  "/>
<path clip-path="url(#clip020)" d="M631.376 1026.98 L661.052 1026.98 L661.052 1030.92 L631.376 1030.92 L631.376 1026.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M681.144 1012.33 Q677.533 1012.33 675.704 1015.9 Q673.899 1019.44 673.899 1026.57 Q673.899 1033.67 675.704 1037.24 Q677.533 1040.78 681.144 1040.78 Q684.778 1040.78 686.584 1037.24 Q688.413 1033.67 688.413 1026.57 Q688.413 1019.44 686.584 1015.9 Q684.778 1012.33 681.144 1012.33 M681.144 1008.63 Q686.954 1008.63 690.01 1013.23 Q693.088 1017.82 693.088 1026.57 Q693.088 1035.29 690.01 1039.9 Q686.954 1044.48 681.144 1044.48 Q675.334 1044.48 672.255 1039.9 Q669.2 1035.29 669.2 1026.57 Q669.2 1017.82 672.255 1013.23 Q675.334 1008.63 681.144 1008.63 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M701.306 1037.93 L706.19 1037.93 L706.19 1043.81 L701.306 1043.81 L701.306 1037.93 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M716.422 1009.25 L734.778 1009.25 L734.778 1013.19 L720.704 1013.19 L720.704 1021.66 Q721.723 1021.31 722.741 1021.15 Q723.76 1020.97 724.778 1020.97 Q730.565 1020.97 733.945 1024.14 Q737.324 1027.31 737.324 1032.73 Q737.324 1038.3 733.852 1041.41 Q730.38 1044.48 724.061 1044.48 Q721.885 1044.48 719.616 1044.11 Q717.371 1043.74 714.963 1043 L714.963 1038.3 Q717.047 1039.44 719.269 1039.99 Q721.491 1040.55 723.968 1040.55 Q727.973 1040.55 730.31 1038.44 Q732.648 1036.34 732.648 1032.73 Q732.648 1029.11 730.31 1027.01 Q727.973 1024.9 723.968 1024.9 Q722.093 1024.9 720.218 1025.32 Q718.366 1025.73 716.422 1026.61 L716.422 1009.25 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M756.537 1012.33 Q752.926 1012.33 751.097 1015.9 Q749.292 1019.44 749.292 1026.57 Q749.292 1033.67 751.097 1037.24 Q752.926 1040.78 756.537 1040.78 Q760.171 1040.78 761.977 1037.24 Q763.806 1033.67 763.806 1026.57 Q763.806 1019.44 761.977 1015.9 Q760.171 1012.33 756.537 1012.33 M756.537 1008.63 Q762.347 1008.63 765.403 1013.23 Q768.482 1017.82 768.482 1026.57 Q768.482 1035.29 765.403 1039.9 Q762.347 1044.48 756.537 1044.48 Q750.727 1044.48 747.648 1039.9 Q744.593 1035.29 744.593 1026.57 Q744.593 1017.82 747.648 1013.23 Q750.727 1008.63 756.537 1008.63 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M616.143 897.979 L645.819 897.979 L645.819 901.915 L616.143 901.915 L616.143 897.979 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M665.911 883.327 Q662.3 883.327 660.471 886.891 Q658.666 890.433 658.666 897.563 Q658.666 904.669 660.471 908.234 Q662.3 911.776 665.911 911.776 Q669.545 911.776 671.351 908.234 Q673.18 904.669 673.18 897.563 Q673.18 890.433 671.351 886.891 Q669.545 883.327 665.911 883.327 M665.911 879.623 Q671.721 879.623 674.777 884.229 Q677.856 888.813 677.856 897.563 Q677.856 906.29 674.777 910.896 Q671.721 915.479 665.911 915.479 Q660.101 915.479 657.022 910.896 Q653.967 906.29 653.967 897.563 Q653.967 888.813 657.022 884.229 Q660.101 879.623 665.911 879.623 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M686.073 908.928 L690.957 908.928 L690.957 914.808 L686.073 914.808 L686.073 908.928 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M705.17 910.873 L721.49 910.873 L721.49 914.808 L699.545 914.808 L699.545 910.873 Q702.207 908.118 706.791 903.489 Q711.397 898.836 712.578 897.493 Q714.823 894.97 715.703 893.234 Q716.605 891.475 716.605 889.785 Q716.605 887.03 714.661 885.294 Q712.74 883.558 709.638 883.558 Q707.439 883.558 704.985 884.322 Q702.554 885.086 699.777 886.637 L699.777 881.915 Q702.601 880.78 705.054 880.202 Q707.508 879.623 709.545 879.623 Q714.915 879.623 718.11 882.308 Q721.304 884.993 721.304 889.484 Q721.304 891.614 720.494 893.535 Q719.707 895.433 717.601 898.026 Q717.022 898.697 713.92 901.915 Q710.818 905.109 705.17 910.873 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M731.351 880.248 L749.707 880.248 L749.707 884.183 L735.633 884.183 L735.633 892.655 Q736.651 892.308 737.67 892.146 Q738.689 891.961 739.707 891.961 Q745.494 891.961 748.874 895.132 Q752.253 898.303 752.253 903.72 Q752.253 909.299 748.781 912.401 Q745.309 915.479 738.989 915.479 Q736.814 915.479 734.545 915.109 Q732.3 914.739 729.892 913.998 L729.892 909.299 Q731.976 910.433 734.198 910.989 Q736.42 911.544 738.897 911.544 Q742.901 911.544 745.239 909.438 Q747.577 907.331 747.577 903.72 Q747.577 900.109 745.239 898.003 Q742.901 895.896 738.897 895.896 Q737.022 895.896 735.147 896.313 Q733.295 896.729 731.351 897.609 L731.351 880.248 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M647.648 746.056 Q644.037 746.056 642.208 749.62 Q640.402 753.162 640.402 760.292 Q640.402 767.398 642.208 770.963 Q644.037 774.505 647.648 774.505 Q651.282 774.505 653.087 770.963 Q654.916 767.398 654.916 760.292 Q654.916 753.162 653.087 749.62 Q651.282 746.056 647.648 746.056 M647.648 742.352 Q653.458 742.352 656.513 746.958 Q659.592 751.542 659.592 760.292 Q659.592 769.019 656.513 773.625 Q653.458 778.208 647.648 778.208 Q641.837 778.208 638.759 773.625 Q635.703 769.019 635.703 760.292 Q635.703 751.542 638.759 746.958 Q641.837 742.352 647.648 742.352 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M667.81 771.657 L672.694 771.657 L672.694 777.537 L667.81 777.537 L667.81 771.657 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M692.879 746.056 Q689.268 746.056 687.439 749.62 Q685.634 753.162 685.634 760.292 Q685.634 767.398 687.439 770.963 Q689.268 774.505 692.879 774.505 Q696.513 774.505 698.319 770.963 Q700.147 767.398 700.147 760.292 Q700.147 753.162 698.319 749.62 Q696.513 746.056 692.879 746.056 M692.879 742.352 Q698.689 742.352 701.745 746.958 Q704.823 751.542 704.823 760.292 Q704.823 769.019 701.745 773.625 Q698.689 778.208 692.879 778.208 Q687.069 778.208 683.99 773.625 Q680.934 769.019 680.934 760.292 Q680.934 751.542 683.99 746.958 Q687.069 742.352 692.879 742.352 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M723.041 746.056 Q719.43 746.056 717.601 749.62 Q715.795 753.162 715.795 760.292 Q715.795 767.398 717.601 770.963 Q719.43 774.505 723.041 774.505 Q726.675 774.505 728.481 770.963 Q730.309 767.398 730.309 760.292 Q730.309 753.162 728.481 749.62 Q726.675 746.056 723.041 746.056 M723.041 742.352 Q728.851 742.352 731.906 746.958 Q734.985 751.542 734.985 760.292 Q734.985 769.019 731.906 773.625 Q728.851 778.208 723.041 778.208 Q717.231 778.208 714.152 773.625 Q711.096 769.019 711.096 760.292 Q711.096 751.542 714.152 746.958 Q717.231 742.352 723.041 742.352 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M630.232 599.698 Q626.621 599.698 624.792 603.263 Q622.986 606.805 622.986 613.934 Q622.986 621.041 624.792 624.605 Q626.621 628.147 630.232 628.147 Q633.866 628.147 635.672 624.605 Q637.5 621.041 637.5 613.934 Q637.5 606.805 635.672 603.263 Q633.866 599.698 630.232 599.698 M630.232 595.994 Q636.042 595.994 639.098 600.601 Q642.176 605.184 642.176 613.934 Q642.176 622.661 639.098 627.267 Q636.042 631.851 630.232 631.851 Q624.422 631.851 621.343 627.267 Q618.287 622.661 618.287 613.934 Q618.287 605.184 621.343 600.601 Q624.422 595.994 630.232 595.994 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M650.394 625.3 L655.278 625.3 L655.278 631.179 L650.394 631.179 L650.394 625.3 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M669.491 627.244 L685.81 627.244 L685.81 631.179 L663.866 631.179 L663.866 627.244 Q666.528 624.49 671.111 619.86 Q675.718 615.207 676.898 613.865 Q679.144 611.342 680.023 609.606 Q680.926 607.846 680.926 606.156 Q680.926 603.402 678.982 601.666 Q677.06 599.93 673.958 599.93 Q671.759 599.93 669.306 600.694 Q666.875 601.457 664.097 603.008 L664.097 598.286 Q666.921 597.152 669.375 596.573 Q671.829 595.994 673.866 595.994 Q679.236 595.994 682.431 598.68 Q685.625 601.365 685.625 605.856 Q685.625 607.985 684.815 609.906 Q684.028 611.805 681.921 614.397 Q681.343 615.068 678.241 618.286 Q675.139 621.48 669.491 627.244 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M695.671 596.619 L714.028 596.619 L714.028 600.555 L699.954 600.555 L699.954 609.027 Q700.972 608.68 701.991 608.518 Q703.009 608.332 704.028 608.332 Q709.815 608.332 713.194 611.504 Q716.574 614.675 716.574 620.092 Q716.574 625.67 713.102 628.772 Q709.63 631.851 703.31 631.851 Q701.134 631.851 698.866 631.48 Q696.62 631.11 694.213 630.369 L694.213 625.67 Q696.296 626.804 698.518 627.36 Q700.741 627.916 703.218 627.916 Q707.222 627.916 709.56 625.809 Q711.898 623.703 711.898 620.092 Q711.898 616.48 709.56 614.374 Q707.222 612.268 703.218 612.268 Q701.343 612.268 699.468 612.684 Q697.616 613.101 695.671 613.98 L695.671 596.619 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M609.565 443.321 Q605.954 443.321 604.125 446.886 Q602.32 450.427 602.32 457.557 Q602.32 464.663 604.125 468.228 Q605.954 471.77 609.565 471.77 Q613.199 471.77 615.005 468.228 Q616.833 464.663 616.833 457.557 Q616.833 450.427 615.005 446.886 Q613.199 443.321 609.565 443.321 M609.565 439.617 Q615.375 439.617 618.431 444.224 Q621.509 448.807 621.509 457.557 Q621.509 466.284 618.431 470.89 Q615.375 475.474 609.565 475.474 Q603.755 475.474 600.676 470.89 Q597.62 466.284 597.62 457.557 Q597.62 448.807 600.676 444.224 Q603.755 439.617 609.565 439.617 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M629.727 468.923 L634.611 468.923 L634.611 474.802 L629.727 474.802 L629.727 468.923 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M644.842 440.242 L663.199 440.242 L663.199 444.177 L649.125 444.177 L649.125 452.65 Q650.143 452.302 651.162 452.14 Q652.18 451.955 653.199 451.955 Q658.986 451.955 662.365 455.126 Q665.745 458.298 665.745 463.714 Q665.745 469.293 662.273 472.395 Q658.801 475.474 652.481 475.474 Q650.305 475.474 648.037 475.103 Q645.791 474.733 643.384 473.992 L643.384 469.293 Q645.467 470.427 647.69 470.983 Q649.912 471.538 652.389 471.538 Q656.393 471.538 658.731 469.432 Q661.069 467.325 661.069 463.714 Q661.069 460.103 658.731 457.997 Q656.393 455.89 652.389 455.89 Q650.514 455.89 648.639 456.307 Q646.787 456.724 644.842 457.603 L644.842 440.242 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M684.958 443.321 Q681.347 443.321 679.518 446.886 Q677.713 450.427 677.713 457.557 Q677.713 464.663 679.518 468.228 Q681.347 471.77 684.958 471.77 Q688.592 471.77 690.398 468.228 Q692.226 464.663 692.226 457.557 Q692.226 450.427 690.398 446.886 Q688.592 443.321 684.958 443.321 M684.958 439.617 Q690.768 439.617 693.824 444.224 Q696.902 448.807 696.902 457.557 Q696.902 466.284 693.824 470.89 Q690.768 475.474 684.958 475.474 Q679.148 475.474 676.069 470.89 Q673.014 466.284 673.014 457.557 Q673.014 448.807 676.069 444.224 Q679.148 439.617 684.958 439.617 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M504.308 859.828 L522.164 859.828 L522.164 851.744 Q522.164 847.256 519.841 844.805 Q517.517 842.355 513.22 842.355 Q508.955 842.355 506.632 844.805 Q504.308 847.256 504.308 851.744 L504.308 859.828 M499.025 866.258 L499.025 851.744 Q499.025 843.755 502.653 839.681 Q506.25 835.575 513.22 835.575 Q520.254 835.575 523.851 839.681 Q527.448 843.755 527.448 851.744 L527.448 859.828 L546.545 859.828 L546.545 866.258 L499.025 866.258 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M502.685 791.365 L509.465 791.365 Q506.441 794.612 504.945 798.304 Q503.449 801.964 503.449 806.102 Q503.449 814.25 508.446 818.579 Q513.411 822.907 522.833 822.907 Q532.222 822.907 537.219 818.579 Q542.184 814.25 542.184 806.102 Q542.184 801.964 540.688 798.304 Q539.192 794.612 536.169 791.365 L542.885 791.365 Q545.176 794.739 546.322 798.527 Q547.468 802.282 547.468 806.484 Q547.468 817.274 540.879 823.48 Q534.259 829.687 522.833 829.687 Q511.374 829.687 504.786 823.48 Q498.165 817.274 498.165 806.484 Q498.165 802.219 499.311 798.463 Q500.425 794.675 502.685 791.365 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M520.923 761.383 Q521.91 756.768 525.029 754.19 Q528.148 751.58 532.731 751.58 Q539.765 751.58 543.617 756.418 Q547.468 761.255 547.468 770.167 Q547.468 773.159 546.863 776.342 Q546.29 779.493 545.113 782.867 L538.906 782.867 Q540.466 780.193 541.261 777.011 Q542.057 773.828 542.057 770.358 Q542.057 764.311 539.67 761.16 Q537.283 757.977 532.731 757.977 Q528.53 757.977 526.175 760.937 Q523.787 763.865 523.787 769.117 L523.787 774.655 L518.504 774.655 L518.504 768.862 Q518.504 764.12 516.626 761.606 Q514.716 759.091 511.151 759.091 Q507.491 759.091 505.55 761.701 Q503.576 764.279 503.576 769.117 Q503.576 771.759 504.149 774.783 Q504.722 777.806 505.932 781.435 L500.202 781.435 Q499.184 777.774 498.675 774.592 Q498.165 771.377 498.165 768.544 Q498.165 761.224 501.507 756.959 Q504.818 752.694 510.483 752.694 Q514.43 752.694 517.167 754.953 Q519.873 757.213 520.923 761.383 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><circle clip-path="url(#clip022)" cx="1535.23" cy="975.913" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1519.84" cy="876.359" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1623.74" cy="570.196" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1576.39" cy="799.58" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1539.57" cy="953.946" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1555.56" cy="760.143" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1542.66" cy="922.411" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1713.49" cy="407.105" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1600.49" cy="743.7" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1625.55" cy="596.201" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1610.17" cy="613.31" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1535.97" cy="767.464" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1518.08" cy="983.365" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1596.87" cy="787.005" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1525.9" cy="840.358" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1587.08" cy="808.856" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1700.21" cy="522.691" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1585.99" cy="904.832" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1610.88" cy="719.884" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1580.84" cy="816.618" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1435.53" cy="1230.27" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1562.87" cy="623.905" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1526.19" cy="931.666" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1544.03" cy="830.627" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1575.63" cy="850.576" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1263.3" cy="650.894" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1154.18" cy="985" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1170.33" cy="766.297" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1204.39" cy="959.435" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1171.24" cy="715.28" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1232.04" cy="661.194" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1191.2" cy="753.064" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1291.98" cy="773.823" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1231" cy="926.818" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1208.33" cy="967.055" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1261.3" cy="849.559" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1192.67" cy="852.646" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1274.73" cy="800.2" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1206.64" cy="716.06" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1276.9" cy="977.616" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1216.56" cy="1032.82" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1105.12" cy="755.006" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1244.37" cy="451.17" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1187.63" cy="1076.88" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1175.29" cy="891.742" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1212.1" cy="711.803" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1203.31" cy="1023.53" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1231.01" cy="713.662" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1248.1" cy="811.079" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1215.21" cy="781.069" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1062.22" cy="627.937" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1089.18" cy="641.163" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1107.28" cy="702.687" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1116.56" cy="781.888" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1151.45" cy="284.8" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1106.18" cy="730.726" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1033.7" cy="678.349" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1130.71" cy="381.891" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1171.55" cy="368.515" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1073.18" cy="1010.52" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1065.27" cy="512.073" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1145.2" cy="756.131" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1163.53" cy="654.961" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1155.5" cy="583.089" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1175.58" cy="793.194" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1226.16" cy="469.212" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1151.69" cy="788.307" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1152.78" cy="683.659" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1130.13" cy="573.136" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1170.95" cy="584.089" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1190.71" cy="538.113" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1106.69" cy="445.024" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1151.76" cy="540.403" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1150.25" cy="589.674" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip022)" cx="1111.82" cy="534.78" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<path clip-path="url(#clip020)" d="
M2001.41 300.469 L2283.91 300.469 L2283.91 93.1086 L2001.41 93.1086  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip020)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  2001.41,300.469 2283.91,300.469 2283.91,93.1086 2001.41,93.1086 2001.41,300.469 
  "/>
<circle clip-path="url(#clip020)" cx="2093.21" cy="144.949" r="23.04" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="5.12"/>
<path clip-path="url(#clip020)" d="M2198.85 164.636 Q2197.04 169.266 2195.33 170.678 Q2193.61 172.09 2190.74 172.09 L2187.34 172.09 L2187.34 168.525 L2189.84 168.525 Q2191.6 168.525 2192.57 167.692 Q2193.54 166.858 2194.73 163.756 L2195.49 161.812 L2185 136.303 L2189.52 136.303 L2197.62 156.581 L2205.72 136.303 L2210.23 136.303 L2198.85 164.636 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M2217.53 158.293 L2225.16 158.293 L2225.16 131.928 L2216.85 133.595 L2216.85 129.335 L2225.12 127.669 L2229.79 127.669 L2229.79 158.293 L2237.43 158.293 L2237.43 162.229 L2217.53 162.229 L2217.53 158.293 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><circle clip-path="url(#clip020)" cx="2093.21" cy="196.789" r="23.04" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="5.12"/>
<path clip-path="url(#clip020)" d="M2198.85 216.476 Q2197.04 221.106 2195.33 222.518 Q2193.61 223.93 2190.74 223.93 L2187.34 223.93 L2187.34 220.365 L2189.84 220.365 Q2191.6 220.365 2192.57 219.532 Q2193.54 218.698 2194.73 215.596 L2195.49 213.652 L2185 188.143 L2189.52 188.143 L2197.62 208.421 L2205.72 188.143 L2210.23 188.143 L2198.85 216.476 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M2220.74 210.133 L2237.06 210.133 L2237.06 214.069 L2215.12 214.069 L2215.12 210.133 Q2217.78 207.379 2222.36 202.749 Q2226.97 198.096 2228.15 196.754 Q2230.4 194.231 2231.28 192.495 Q2232.18 190.735 2232.18 189.046 Q2232.18 186.291 2230.23 184.555 Q2228.31 182.819 2225.21 182.819 Q2223.01 182.819 2220.56 183.583 Q2218.13 184.347 2215.35 185.897 L2215.35 181.175 Q2218.17 180.041 2220.63 179.462 Q2223.08 178.884 2225.12 178.884 Q2230.49 178.884 2233.68 181.569 Q2236.88 184.254 2236.88 188.745 Q2236.88 190.874 2236.07 192.796 Q2235.28 194.694 2233.17 197.286 Q2232.6 197.958 2229.49 201.175 Q2226.39 204.37 2220.74 210.133 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><circle clip-path="url(#clip020)" cx="2093.21" cy="248.629" r="23.04" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="5.12"/>
<path clip-path="url(#clip020)" d="M2198.85 268.316 Q2197.04 272.946 2195.33 274.358 Q2193.61 275.77 2190.74 275.77 L2187.34 275.77 L2187.34 272.205 L2189.84 272.205 Q2191.6 272.205 2192.57 271.372 Q2193.54 270.538 2194.73 267.436 L2195.49 265.492 L2185 239.983 L2189.52 239.983 L2197.62 260.261 L2205.72 239.983 L2210.23 239.983 L2198.85 268.316 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip020)" d="M2230.88 247.274 Q2234.24 247.992 2236.11 250.261 Q2238.01 252.529 2238.01 255.862 Q2238.01 260.978 2234.49 263.779 Q2230.98 266.58 2224.49 266.58 Q2222.32 266.58 2220 266.14 Q2217.71 265.723 2215.26 264.867 L2215.26 260.353 Q2217.2 261.487 2219.52 262.066 Q2221.83 262.645 2224.35 262.645 Q2228.75 262.645 2231.04 260.909 Q2233.36 259.173 2233.36 255.862 Q2233.36 252.807 2231.21 251.094 Q2229.08 249.358 2225.26 249.358 L2221.23 249.358 L2221.23 245.515 L2225.44 245.515 Q2228.89 245.515 2230.72 244.149 Q2232.55 242.761 2232.55 240.168 Q2232.55 237.506 2230.65 236.094 Q2228.78 234.659 2225.26 234.659 Q2223.34 234.659 2221.14 235.075 Q2218.94 235.492 2216.3 236.372 L2216.3 232.205 Q2218.96 231.464 2221.28 231.094 Q2223.61 230.724 2225.67 230.724 Q2231 230.724 2234.1 233.154 Q2237.2 235.562 2237.2 239.682 Q2237.2 242.552 2235.56 244.543 Q2233.91 246.511 2230.88 247.274 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<h2 id="Linear-Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Linear-Principal-Component-Analysis">Linear Principal Component Analysis</a><a id="Linear-Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Principal-Component-Analysis" title="Permalink"></a></h2><p>This package uses the <a href="#MultivariateStats.PCA"><code>PCA</code></a> type to define a linear PCA model:</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.PCA" href="#MultivariateStats.PCA"><code>MultivariateStats.PCA</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Linear Principal Component Analysis</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L3-L5">source</a></section></article><p>This type comes with several methods where <span>$M$</span> be an instance of  <a href="#MultivariateStats.PCA"><code>PCA</code></a>, <span>$d$</span> be the dimension of observations, and <span>$p$</span> be the output dimension (<em>i.e</em> the dimension of the principal subspace).</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit-Union{Tuple{T}, Tuple{Type{PCA}, AbstractMatrix{T}}} where T&lt;:Real" href="#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{PCA}, AbstractMatrix{T}}} where T&lt;:Real"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(PCA, X; ...)</code></pre><p>Perform PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an <strong>observation</strong>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:auto</code>: use <code>:cov</code> when <code>d &lt; n</code> or <code>:svd</code> otherwise (<em>default</em>).</li><li><code>:cov</code>: based on covariance matrix decomposition.</li><li><code>:svd</code>: based on SVD of the input data.</li></ul></li><li><code>maxoutdim</code>: The output dimension, i.e. dimension of the transformed space (<em>min(d, nc-1)</em>)</li><li><code>pratio</code>: The ratio of variances preserved in the principal subspace (<em>0.99</em>)</li><li><code>mean</code>: The mean vector, which can be either of<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Notes:</strong></p><ul><li><p>The output dimension <code>p</code> depends on both <code>maxoutdim</code> and <code>pratio</code>, as follows. Suppose the first <code>k</code> principal components preserve at least <code>pratio</code> of the total variance, while the first <code>k-1</code> preserves less than <code>pratio</code>, then the actual output dimension will be <span>$\min(k, maxoutdim)$</span>.</p></li><li><p>This function calls <a href="#MultivariateStats.pcacov"><code>pcacov</code></a> or <a href="#MultivariateStats.pcasvd"><code>pcasvd</code></a> internally, depending on the choice of method.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L255-L280">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#StatsAPI.predict-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::PCA, x::AbstractVecOrMat{&lt;:Real})</code></pre><p>Given a PCA model <code>M</code>, transform observations <code>x</code> into principal components space, as</p><p class="math-container">\[\mathbf{y} = \mathbf{P}^T (\mathbf{x} - \boldsymbol{\mu})\]</p><p>Here, <code>x</code> can be either a vector of length <code>d</code> or a matrix where each column is an observation, and <code>\mathbf{P}</code> is the projection matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L112-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>MultivariateStats.reconstruct</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">reconstruct(M::PCA, y::AbstractVecOrMat{&lt;:Real})</code></pre><p>Given a PCA model <code>M</code>, returns a (approximately) reconstructed observations from principal components space, as</p><p class="math-container">\[\tilde{\mathbf{x}} = \mathbf{P} \mathbf{y} + \boldsymbol{\mu}\]</p><p>Here, <code>y</code> can be either a vector of length <code>p</code> or a matrix where each column gives the principal components for an observation, and <span>$\mathbf{P}$</span> is the projection matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L124-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{PCA}" href="#Base.size-Tuple{PCA}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(M)</code></pre><p>Returns a tuple with the dimensions of input (the dimension of the observation space) and output (the dimension of the principal subspace).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L28-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.mean-Tuple{PCA}" href="#Statistics.mean-Tuple{PCA}"><code>Statistics.mean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mean(M::PCA)</code></pre><p>Returns the mean vector (of length <code>d</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L36-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.projection-Tuple{PCA}" href="#MultivariateStats.projection-Tuple{PCA}"><code>MultivariateStats.projection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">projection(M::PCA)</code></pre><p>Returns the projection matrix (of size <code>(d, p)</code>). Each column of the projection matrix corresponds to a principal component. The principal components are arranged in descending order of the corresponding variances.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L43-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.var-Tuple{PCA}" href="#Statistics.var-Tuple{PCA}"><code>Statistics.var</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">var(M::PCA)</code></pre><p>Returns the total observation variance, which is equal to <code>tprincipalvar(M) + tresidualvar(M)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L87-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.principalvars-Tuple{PCA}" href="#MultivariateStats.principalvars-Tuple{PCA}"><code>MultivariateStats.principalvars</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">principalvars(M::PCA)</code></pre><p>Returns the variances of principal components.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L58-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.tprincipalvar-Tuple{PCA}" href="#MultivariateStats.tprincipalvar-Tuple{PCA}"><code>MultivariateStats.tprincipalvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tprincipalvar(M::PCA)</code></pre><p>Returns the total variance of principal components, which is equal to <code>sum(principalvars(M))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L73-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.tresidualvar-Tuple{PCA}" href="#MultivariateStats.tresidualvar-Tuple{PCA}"><code>MultivariateStats.tresidualvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tresidualvar(M::PCA)</code></pre><p>Returns the total residual variance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L80-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.r2-Tuple{PCA}" href="#StatsAPI.r2-Tuple{PCA}"><code>StatsAPI.r2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">r2(M::PCA)
principalratio(M::PCA)</code></pre><p>Returns the ratio of variance preserved in the principal subspace, which is equal to <code>tprincipalvar(M) / var(M)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L94-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.loadings-Tuple{PCA}" href="#MultivariateStats.loadings-Tuple{PCA}"><code>MultivariateStats.loadings</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">loadings(M::PCA)</code></pre><p>Returns model loadings, i.e. the weights for each original variable when calculating the principal component.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L103-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvals-Tuple{PCA}" href="#LinearAlgebra.eigvals-Tuple{PCA}"><code>LinearAlgebra.eigvals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvals(M::PCA)</code></pre><p>Get the eigenvalues of the PCA model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L66-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvecs-Tuple{PCA}" href="#LinearAlgebra.eigvecs-Tuple{PCA}"><code>LinearAlgebra.eigvecs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvecs(M::PCA)</code></pre><p>Get the eigenvalues of the PCA model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L51-L55">source</a></section></article><p>Auxiliary functions:</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.pcacov" href="#MultivariateStats.pcacov"><code>MultivariateStats.pcacov</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pcacov(C, mean; ...)</code></pre><p>Compute and return a PCA model based on eigenvalue decomposition of a given covariance matrix <code>C</code>.</p><p><strong>Parameters:</strong></p><ul><li><code>C</code>: The covariance matrix of the samples.</li><li><code>mean</code>: The mean vector of original samples, which can be a vector of length <code>d</code>,          or an empty vector <code>Float64[]</code> indicating a zero mean.</li></ul><p><em>Note:</em> This function accepts two keyword arguments: <code>maxoutdim</code> and <code>pratio</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L197-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.pcasvd" href="#MultivariateStats.pcasvd"><code>MultivariateStats.pcasvd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pcasvd(Z, mean, tw; ...)</code></pre><p>Compute and return a PCA model based on singular value decomposition of a centralized sample matrix <code>Z</code>.</p><p><strong>Parameters:</strong></p><ul><li><code>Z</code>: a matrix of centralized samples.</li><li><code>mean</code>: The mean vector of the <strong>original</strong> samples, which can be a vector of length <code>d</code>,         or an empty vector <code>Float64[]</code> indicating a zero mean.</li><li><code>n</code>: a number of samples.</li></ul><p><em>Note:</em> This function accepts two keyword arguments: <code>maxoutdim</code> and <code>pratio</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L223-L235">source</a></section></article><h2 id="Kernel-Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Kernel-Principal-Component-Analysis">Kernel Principal Component Analysis</a><a id="Kernel-Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Principal-Component-Analysis" title="Permalink"></a></h2><p><a href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis&gt;">Kernel Principal Component Analysis</a> (kernel PCA) is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.</p><p>This package defines a <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a> type to represent a kernel PCA model.</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.KernelPCA" href="#MultivariateStats.KernelPCA"><code>MultivariateStats.KernelPCA</code></a> — <span class="docstring-category">Type</span></header><section><div><p>This type contains kernel PCA model parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L31-L33">source</a></section></article><p>The package provides a set of methods to access the properties of the kernel PCA model. Let <span>$M$</span> be an instance of <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a>, <span>$d$</span> be the dimension of observations, and <span>$p$</span> be the output dimension (<em>i.e</em> the dimension of the principal subspace).</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit-Union{Tuple{T}, Tuple{Type{KernelPCA}, AbstractMatrix{T}}} where T&lt;:Real" href="#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{KernelPCA}, AbstractMatrix{T}}} where T&lt;:Real"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(KernelPCA, X; ...)</code></pre><p>Perform kernel PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation.</p><p>This method returns an instance of <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>kernel</code>: The kernel function. This functions accepts two vector arguments <code>x</code> and <code>y</code>,</li></ul><p>and returns a scalar value (<em>default:</em> <code>(x,y)-&gt;x&#39;y</code>)</p><ul><li><code>solver</code>: The choice of solver:<ul><li><code>:eig</code>: uses <code>LinearAlgebra.eigen</code> (<em>default</em>)</li><li><code>:eigs</code>: uses <code>Arpack.eigs</code> (always used for sparse data)</li></ul></li><li><code>maxoutdim</code>:  Maximum output dimension (<em>default</em> <code>min(d, n)</code>)</li><li><code>inverse</code>: Whether to perform calculation for inverse transform for non-precomputed kernels (<em>default</em> <code>false</code>)</li><li><code>β</code>: Hyperparameter of the ridge regression that learns the inverse transform (<em>default</em> <code>1</code> when <code>inverse</code> is <code>true</code>).</li><li><code>tol</code>: Convergence tolerance for <code>eigs</code> solver (<em>default</em> <code>0.0</code>)</li><li><code>maxiter</code>: Maximum number of iterations for <code>eigs</code> solver (<em>default</em> <code>300</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L123-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Tuple{KernelPCA}" href="#StatsAPI.predict-Tuple{KernelPCA}"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::KernelPCA)</code></pre><p>Transform the data fitted to the model <code>M</code> to a kernel space of the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L91-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#StatsAPI.predict-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::KernelPCA, x)</code></pre><p>Transform out-of-sample transformation of <code>x</code> into a kernel space of the model <code>M</code>.</p><p>Here, <code>x</code> can be either a vector of length <code>d</code> or a matrix where each column is an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L77-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>MultivariateStats.reconstruct</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">reconstruct(M::KernelPCA, y)</code></pre><p>Approximately reconstruct observations, given in <code>y</code>, to the original space using the kernel PCA model <code>M</code>.</p><p>Here, <code>y</code> can be either a vector of length <code>p</code> or a matrix where each column gives the principal components for an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L99-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{KernelPCA}" href="#Base.size-Tuple{KernelPCA}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(M::KernelPCA)</code></pre><p>Returns a tuple with the input dimension <span>$d$</span>, <em>i.e</em> the dimension of the observation space, and the output dimension <span>$p$</span>, <em>i.e</em> the dimension of the principal subspace.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L44-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.projection-Tuple{KernelPCA}" href="#MultivariateStats.projection-Tuple{KernelPCA}"><code>MultivariateStats.projection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">projection(M::KernelPCA)</code></pre><p>Return the projection matrix (of size <span>$n \times p$</span>). Each column of the projection matrix corresponds to an eigenvector, and <span>$n$</span> is a number of observations.</p><p>The principal components are arranged in descending order of the corresponding eigenvalues.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L65-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvals-Tuple{KernelPCA}" href="#LinearAlgebra.eigvals-Tuple{KernelPCA}"><code>LinearAlgebra.eigvals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvals(M::KernelPCA)</code></pre><p>Return eigenvalues of the kernel matrix of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L51-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvecs-Tuple{KernelPCA}" href="#LinearAlgebra.eigvecs-Tuple{KernelPCA}"><code>LinearAlgebra.eigvecs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvecs(M::KernelPCA)</code></pre><p>Return eigenvectors of the kernel matrix of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L58-L62">source</a></section></article><h3 id="Kernels"><a class="docs-heading-anchor" href="#Kernels">Kernels</a><a id="Kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Kernels" title="Permalink"></a></h3><p>List of the commonly used kernels:</p><table><tr><th style="text-align: right">function</th><th style="text-align: right">description</th></tr><tr><td style="text-align: right"><code>(x,y)-&gt;x&#39;y</code></td><td style="text-align: right">Linear</td></tr><tr><td style="text-align: right"><code>(x,y)-&gt;(x&#39;y+c)^d</code></td><td style="text-align: right">Polynomial</td></tr><tr><td style="text-align: right"><code>(x,y)-&gt;exp(-γ*norm(x-y)^2.0)</code></td><td style="text-align: right">Radial basis function (RBF)</td></tr></table><p>This package has a separate interface for adjusting kernel matrices.</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.KernelCenter" href="#MultivariateStats.KernelCenter"><code>MultivariateStats.KernelCenter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Center a kernel matrix</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit-Tuple{Type{MultivariateStats.KernelCenter}, AbstractMatrix{&lt;:Real}}" href="#StatsAPI.fit-Tuple{Type{MultivariateStats.KernelCenter}, AbstractMatrix{&lt;:Real}}"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Fit <code>KernelCenter</code> object</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.transform!-Tuple{MultivariateStats.KernelCenter, AbstractMatrix{&lt;:Real}}" href="#MultivariateStats.transform!-Tuple{MultivariateStats.KernelCenter, AbstractMatrix{&lt;:Real}}"><code>MultivariateStats.transform!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Center kernel matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L18">source</a></section></article><h2 id="Probabilistic-Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Probabilistic-Principal-Component-Analysis">Probabilistic Principal Component Analysis</a><a id="Probabilistic-Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-Principal-Component-Analysis" title="Permalink"></a></h2><p><a href="https://www.microsoft.com/en-us/research/publication/probabilistic-principal-component-analysis">Probabilistic Principal Component Analysis</a> (PPCA) represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. It is expressed as the maximum likelihood solution of a probabilistic latent variable model<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>.</p><p>This package defines a <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> type to represent a probabilistic PCA model, and provides a set of methods to access the properties.</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.PPCA" href="#MultivariateStats.PPCA"><code>MultivariateStats.PPCA</code></a> — <span class="docstring-category">Type</span></header><section><div><p>This type contains probabilistic PCA model parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L3-L5">source</a></section></article><p>Let <span>$M$</span> be an instance of <a href="#MultivariateStats.PPCA"><code>PPCA</code></a>, <span>$d$</span> be the dimension of observations, and <span>$p$</span> be the output dimension (<em>i.e</em> the dimension of the principal subspace).</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit" href="#StatsAPI.fit"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit(Whitening, X::AbstractMatrix{T}; kwargs...)</code></pre><p>Estimate a whitening transform from the data given in <code>X</code>.</p><p>This function returns an instance of <a href="../whiten/#MultivariateStats.Whitening"><code>Whitening</code></a></p><p><strong>Keyword Arguments:</strong></p><ul><li><p><code>regcoef</code>: The regularization coefficient. The covariance will be regularized as follows when <code>regcoef</code> is positive <code>C + (eigmax(C) * regcoef) * eye(d)</code>. Default values is <code>zero(T)</code>.</p></li><li><p><code>dims</code>: if <code>1</code> the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if <code>2</code> the transformation calculated from the column samples. The default is <code>nothing</code>, which is equivalent to <code>dims=2</code> with a deprecation warning.</p></li><li><p><code>mean</code>: The mean vector, which can be either of:</p><ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<strong>default</strong>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Note:</strong> This function internally relies on <a href="../whiten/#MultivariateStats.cov_whitening"><code>cov_whitening</code></a> to derive the transformation <code>W</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/whiten.jl#L104-L123">source</a></section><section><div><pre><code class="language-julia hljs">fit(PCA, X; ...)</code></pre><p>Perform PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an <strong>observation</strong>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:auto</code>: use <code>:cov</code> when <code>d &lt; n</code> or <code>:svd</code> otherwise (<em>default</em>).</li><li><code>:cov</code>: based on covariance matrix decomposition.</li><li><code>:svd</code>: based on SVD of the input data.</li></ul></li><li><code>maxoutdim</code>: The output dimension, i.e. dimension of the transformed space (<em>min(d, nc-1)</em>)</li><li><code>pratio</code>: The ratio of variances preserved in the principal subspace (<em>0.99</em>)</li><li><code>mean</code>: The mean vector, which can be either of<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Notes:</strong></p><ul><li><p>The output dimension <code>p</code> depends on both <code>maxoutdim</code> and <code>pratio</code>, as follows. Suppose the first <code>k</code> principal components preserve at least <code>pratio</code> of the total variance, while the first <code>k-1</code> preserves less than <code>pratio</code>, then the actual output dimension will be <span>$\min(k, maxoutdim)$</span>.</p></li><li><p>This function calls <a href="#MultivariateStats.pcacov"><code>pcacov</code></a> or <a href="#MultivariateStats.pcasvd"><code>pcasvd</code></a> internally, depending on the choice of method.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/pca.jl#L255-L280">source</a></section><section><div><pre><code class="nohighlight hljs">fit(PPCA, X; ...)</code></pre><p>Perform probabilistic PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation. This method returns an instance of <a href="#MultivariateStats.PPCA"><code>PPCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:ml</code>: use maximum likelihood version of probabilistic PCA (<em>default</em>)</li><li><code>:em</code>: use EM version of probabilistic PCA</li><li><code>:bayes</code>: use Bayesian PCA</li></ul></li><li><code>maxoutdim</code>: Maximum output dimension (<em>default</em> <code>d-1</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li><li><code>tol</code>: Convergence tolerance (<em>default</em> <code>1.0e-6</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>1000</code>)</li></ul><p><strong>Notes:</strong> This function calls <a href="#MultivariateStats.ppcaml"><code>ppcaml</code></a>, <a href="#MultivariateStats.ppcaem"><code>ppcaem</code></a> or <a href="#MultivariateStats.bayespca"><code>bayespca</code></a> internally, depending on the choice of method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L280-L305">source</a></section><section><div><p>Fit <code>KernelCenter</code> object</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L11">source</a></section><section><div><pre><code class="language-julia hljs">fit(KernelPCA, X; ...)</code></pre><p>Perform kernel PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation.</p><p>This method returns an instance of <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>kernel</code>: The kernel function. This functions accepts two vector arguments <code>x</code> and <code>y</code>,</li></ul><p>and returns a scalar value (<em>default:</em> <code>(x,y)-&gt;x&#39;y</code>)</p><ul><li><code>solver</code>: The choice of solver:<ul><li><code>:eig</code>: uses <code>LinearAlgebra.eigen</code> (<em>default</em>)</li><li><code>:eigs</code>: uses <code>Arpack.eigs</code> (always used for sparse data)</li></ul></li><li><code>maxoutdim</code>:  Maximum output dimension (<em>default</em> <code>min(d, n)</code>)</li><li><code>inverse</code>: Whether to perform calculation for inverse transform for non-precomputed kernels (<em>default</em> <code>false</code>)</li><li><code>β</code>: Hyperparameter of the ridge regression that learns the inverse transform (<em>default</em> <code>1</code> when <code>inverse</code> is <code>true</code>).</li><li><code>tol</code>: Convergence tolerance for <code>eigs</code> solver (<em>default</em> <code>0.0</code>)</li><li><code>maxiter</code>: Maximum number of iterations for <code>eigs</code> solver (<em>default</em> <code>300</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/kpca.jl#L123-L144">source</a></section><section><div><pre><code class="language-julia hljs">fit(CCA, X, Y; ...)</code></pre><p>Perform CCA over the data given in matrices <code>X</code> and <code>Y</code>. Each column of <code>X</code> and <code>Y</code> is an observation.</p><p><code>X</code> and <code>Y</code> should have the same number of columns (denoted by <code>n</code> below).</p><p>This method returns an instance of <a href="../cca/#MultivariateStats.CCA"><code>CCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:cov</code>: based on covariance matrices</li><li><code>:svd</code>: based on SVD of the input data (<em>default</em>)</li></ul></li><li><code>outdim</code>: The output dimension, <em>i.e</em> dimension of the common space (<em>default</em>: <code>min(dx, dy, n)</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Notes:</strong> This function calls <a href="../cca/#MultivariateStats.ccacov"><code>ccacov</code></a> or <a href="../cca/#MultivariateStats.ccasvd"><code>ccasvd</code></a> internally, depending on the choice of method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/cca.jl#L283-L304">source</a></section><section><div><pre><code class="language-julia hljs">fit(MDS, X; kwargs...)</code></pre><p>Compute an embedding of <code>X</code> points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument <code>distances</code>:</p><pre><code class="nohighlight hljs">mds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)</code></pre><p>where <code>X</code> is the data matrix. Distances between pairs of columns of <code>X</code> are computed using the Euclidean norm. This is equivalent to performing PCA on <code>X</code>.</p><pre><code class="nohighlight hljs">mds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)</code></pre><p>where <code>D</code> is a symmetric matrix <code>D</code> of distances between points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/cmds.jl#L217-L231">source</a></section><section><div><pre><code class="language-julia hljs">fit(MetricMDS, X; kwargs...)</code></pre><p>Compute an embedding of <code>X</code> points by (non)metric multidimensional scaling (MDS).</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>distances</code>: The choice of input (<em>required</em>):<ul><li><code>false</code>: use <code>X</code> to calculate dissimilarity matrix using Euclidean distance</li><li><code>true</code>: use <code>X</code> input as precomputed dissimilarity symmetric matrix (distances)</li></ul></li><li><code>maxoutdim</code>: Maximum output dimension (<em>default</em> <code>d-1</code>)</li><li><code>metric</code> : a function for calculation of disparity values<ul><li><code>nothing</code>: use dissimilarity values as the disparities to perform the metric MDS (<em>default</em>)</li><li><code>isotonic</code>: converts dissimilarity values to ordinal disparities to perform non-metric MDS</li><li>any two parameter disparity transformation function, where the first parameter is a vector of proximities (i.e. dissimilarities) and the second parameter is a vector of distances, e.g. <code>(p,d)-&gt;b*p</code> for some <code>b</code> is a transformation function for <em>ratio</em> MDS.</li></ul></li><li><code>tol</code>: Convergence tolerance (<em>default</em> <code>1.0e-3</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>300</code>)</li><li><code>initial</code>: an initial reduced space point configuration<ul><li><code>nothing</code>: then an initial configuration is randomly generated (<em>default</em>)</li><li>pre-defined matrix</li></ul></li><li><code>weights</code>: a weight matrix<ul><li><code>nothing</code>: then weights are set to one, <span>$w_{ij} = 1$</span> (<em>default</em>)</li><li>pre-defined matrix</li></ul></li></ul><p><em>Note:</em> if the algorithm is unable to converge then <code>ConvergenceException</code> is thrown.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/mmds.jl#L97-L124">source</a></section><section><div><pre><code class="language-julia hljs">fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())</code></pre><p>Performs LDA given both positive and negative samples. The function accepts follwing parameters:</p><p><strong>Parameters</strong></p><ul><li><code>Xp</code>: The sample matrix of the positive class.</li><li><code>Xn</code>: The sample matrix of the negative class.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>covestimator</code>: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as <code>cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)</code>. Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/lda.jl#L128-L139">source</a></section><section><div><pre><code class="nohighlight hljs">fit(MulticlassLDA, X, y; ...)</code></pre><p>Perform multi-class LDA over a given data set <code>X</code> with corresponding labels <code>y</code> with <code>nc</code> number of classes.</p><p>This function returns the resultant multi-class LDA model as an instance of <a href="../lda/#MultivariateStats.MulticlassLDA"><code>MulticlassLDA</code></a>.</p><p><em>Parameters</em></p><ul><li><code>X</code>:   the matrix of input samples, of size <code>(d, n)</code>. Each column in <code>X</code> is an observation.</li><li><code>y</code>:   the vector of class labels, of length <code>n</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:gevd</code>: based on generalized eigenvalue decomposition (<em>default</em>).</li><li><code>:whiten</code>: first derive a whitening transform from <code>Sw</code> and then solve the problem based on eigenvalue</li></ul>decomposition of the whiten <code>Sb</code>.</li><li><code>outdim</code>: The output dimension, i.e. dimension of the transformed space <code>min(d, nc-1)</code></li><li><code>regcoef</code>: The regularization coefficient (<em>default:</em> <code>1.0e-6</code>). A positive value <code>regcoef * eigmax(Sw)</code>   is added to the diagonal of <code>Sw</code> to improve numerical stability.</li><li><code>covestimator_between</code>: Custom covariance estimator for between-class covariance (<em>default:</em> <code>SimpleCovariance()</code>).   The covariance matrix will be calculated as <code>cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#))</code>.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.</li><li><code>covestimator_within</code>:  Custom covariance estimator for within-class covariance (<em>default:</em> <code>SimpleCovariance()</code>).   The covariance matrix will be calculated as <code>cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc))</code>.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.</li></ul><p><strong>Notes:</strong></p><p>The resultant projection matrix <span>$P$</span> satisfies:</p><p class="math-container">\[\mathbf{P}^T (\mathbf{S}_w + \kappa \mathbf{I}) \mathbf{P} = \mathbf{I}\]</p><p>Here, <span>$\kappa$</span> equals <code>regcoef * eigmax(Sw)</code>. The columns of <span>$P$</span> are arranged in descending order of the corresponding generalized eigenvalues.</p><p>Note that <a href="../lda/#MultivariateStats.MulticlassLDA"><code>MulticlassLDA</code></a> does not currently support the normalized version using <span>$\mathbf{S}_w^*$</span> and <span>$\mathbf{S}_b^*$</span> (see <a href="../lda/#MultivariateStats.SubspaceLDA"><code>SubspaceLDA</code></a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/lda.jl#L286-L328">source</a></section><section><div><pre><code class="nohighlight hljs">fit(SubspaceLDA, X, y; normalize=true)</code></pre><p>Fit an subspace projection of LDA model over a given data set <code>X</code> with corresponding labels <code>y</code> using the equivalent of <span>$\mathbf{S}_w^*$</span> and <span>$\mathbf{S}_b^*$</span>`.</p><p>Note: Subspace LDA also supports the normalized version of LDA via the <code>normalize</code> keyword.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/lda.jl#L453-L460">source</a></section><section><div><pre><code class="language-julia hljs">fit(ICA, X, k; ...)</code></pre><p>Perform ICA over the data set given in <code>X</code>.</p><p><strong>Parameters:</strong> -<code>X</code>: The data matrix, of size <span>$(m, n)$</span>. Each row corresponds to a mixed signal, while each column corresponds to an observation (<em>e.g</em> all signal value at a particular time step). -<code>k</code>: The number of independent components to recover.</p><p><strong>Keyword Arguments:</strong></p><ul><li><code>alg</code>: The choice of algorithm (<em>default</em> <code>:fastica</code>)</li><li><code>fun</code>: The approx neg-entropy functor (<em>default</em> <a href="../ica/#MultivariateStats.Tanh"><code>Tanh</code></a>)</li><li><code>do_whiten</code>: Whether to perform pre-whitening (<em>default</em> <code>true</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>100</code>)</li><li><code>tol</code>: Tolerable change of <span>$W$</span> at convergence (<em>default</em> <code>1.0e-6</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li><li><code>winit</code>: Initial guess of <span>$W$</span>, which should be either of:<ul><li>empty matrix: the function will perform random initialization (<em>default</em>)</li><li>a matrix of size <span>$(k, k)$</span> (when <code>do_whiten</code>)</li><li>a matrix of size <span>$(m, k)$</span> (when <code>!do_whiten</code>)</li></ul></li></ul><p>Returns the resultant ICA model, an instance of type <a href="../ica/#MultivariateStats.ICA"><code>ICA</code></a>.</p><p><strong>Note:</strong> If <code>do_whiten</code> is <code>true</code>, the return <code>W</code> satisfies <span>$\mathbf{W}^T \mathbf{C} \mathbf{W} = \mathbf{I}$</span>, otherwise <span>$W$</span> is orthonormal, <em>i.e</em> <span>$\mathbf{W}^T \mathbf{W} = \mathbf{I}$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ica.jl#L181-L211">source</a></section><section><div><pre><code class="language-julia hljs">fit(FactorAnalysis, X; ...)</code></pre><p>Perform factor analysis over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation. This method returns an instance of <a href="../fa/#MultivariateStats.FactorAnalysis"><code>FactorAnalysis</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:em</code>: use EM version of factor analysis</li><li><code>:cm</code>: use CM version of factor analysis (<em>default</em>)</li></ul></li><li><code>maxoutdim</code>: Maximum output dimension (<em>default</em> <code>d-1</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li><li><code>tol</code>: Convergence tolerance (<em>default</em> <code>1.0e-6</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>1000</code>)</li><li><code>η</code>: Variance low bound (<em>default</em> <code>1.0e-6</code>)</li></ul><p><strong>Notes:</strong> This function calls <a href="../fa/#MultivariateStats.facm"><code>facm</code></a> or <a href="../fa/#MultivariateStats.faem"><code>faem</code></a> internally, depending on the choice of method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/fa.jl#L232-L256">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{PPCA}" href="#Base.size-Tuple{PPCA}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(M::PPCA)</code></pre><p>Returns a tuple with values of the input dimension <span>$d$</span>, <em>i.e</em> the dimension of the observation space, and the output dimension <span>$p$</span>, <em>i.e</em> the dimension of the principal subspace.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L13-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.mean-Tuple{PPCA}" href="#Statistics.mean-Tuple{PPCA}"><code>Statistics.mean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mean(M::PPCA)</code></pre><p>Get the mean vector (of length <span>$d$</span>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.var-Tuple{PPCA}" href="#Statistics.var-Tuple{PPCA}"><code>Statistics.var</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">var(M::PPCA)</code></pre><p>Returns the total residual variance of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L40-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.cov-Tuple{PPCA}" href="#Statistics.cov-Tuple{PPCA}"><code>Statistics.cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cov(M::PPCA)</code></pre><p>Returns the covariance of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L54-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.projection-Tuple{PPCA}" href="#MultivariateStats.projection-Tuple{PPCA}"><code>MultivariateStats.projection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">projection(M::PPCA)</code></pre><p>Returns the projection matrix (of size <span>$(d, p)$</span>). Each column of the projection matrix corresponds to a principal component.</p><p>The principal components are arranged in descending order of the corresponding variances.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L29-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.loadings-Tuple{PPCA}" href="#MultivariateStats.loadings-Tuple{PPCA}"><code>MultivariateStats.loadings</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">loadings(M::PPCA)</code></pre><p>Returns the factor loadings matrix (of size <span>$(d, p)$</span>) of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L47-L51">source</a></section></article><p>Given a probabilistic PCA model <span>$M$</span>, one can use it to transform observations into latent variables, as</p><pre><code class="language- hljs">\mathbf{z} = (\mathbf{W}^T \mathbf{W} + \sigma^2 \mathbf{I}) \mathbf{W}^T (\mathbf{x} - \boldsymbol{\mu})</code></pre><p>or use it to reconstruct (approximately) the observations from latent variables, as</p><pre><code class="language- hljs">\tilde{\mathbf{x}} = \mathbf{W} \mathbb{E}[\mathbf{z}] + \boldsymbol{\mu}</code></pre><p>Here, <span>$\mathbf{W}$</span> is the factor loadings or weight matrix.</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#StatsAPI.predict-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::PPCA, x)</code></pre><p>Transform observations <code>x</code> into latent variables. Here, <code>x</code> can be either a vector of length <code>d</code> or a matrix where each column is an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L62-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>MultivariateStats.reconstruct</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">reconstruct(M::PPCA, z)</code></pre><p>Approximately reconstruct observations from the latent variable given in <code>z</code>. Here, <code>z</code> can be either a vector of length <code>p</code> or a matrix where each column gives the latent variables for an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L74-L80">source</a></section></article><p>Auxiliary functions:</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.ppcaml" href="#MultivariateStats.ppcaml"><code>MultivariateStats.ppcaml</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ppcaml(Z, mean; ...)</code></pre><p>Compute probabilistic PCA using on maximum likelihood formulation for a centralized sample matrix <code>Z</code>.</p><p><em>Parameters</em>:</p><ul><li><code>Z</code>: a centralized samples matrix</li><li><code>mean</code>: The mean vector of the <strong>original</strong> samples, which can be a vector of</li></ul><p>length <code>d</code>, or an empty vector indicating a zero mean.</p><p>Returns the resultant <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> model.</p><p><strong>Note:</strong> This function accepts two keyword arguments: <code>maxoutdim</code> and <code>tol</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L97-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.ppcaem" href="#MultivariateStats.ppcaem"><code>MultivariateStats.ppcaem</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ppcaem(S, mean, n; ...)</code></pre><p>Compute probabilistic PCA based on expectation-maximization algorithm for a given sample covariance matrix <code>S</code>.</p><p><em>Parameters</em>:</p><ul><li><code>S</code>: The sample covariance matrix.</li><li><code>mean</code>: The mean vector of original samples, which can be a vector of length <code>d</code>,</li></ul><p>or an empty vector indicating a zero mean.</p><ul><li><code>n</code>: The number of observations.</li></ul><p>Returns the resultant <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> model.</p><p><strong>Note:</strong> This function accepts three keyword arguments: <code>maxoutdim</code>, <code>tol</code>, and <code>maxiter</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L143-L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.bayespca" href="#MultivariateStats.bayespca"><code>MultivariateStats.bayespca</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bayespca(S, mean, n; ...)</code></pre><p>Compute probabilistic PCA using a Bayesian algorithm for a given sample covariance matrix <code>S</code>.</p><p><em>Parameters</em>:</p><ul><li><code>S</code>: The sample covariance matrix.</li><li><code>mean</code>: The mean vector of original samples, which can be a vector of length <code>d</code>,</li></ul><p>or an empty vector indicating a zero mean.</p><ul><li><code>n</code>: The number of observations.</li></ul><p>Returns the resultant <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> model.</p><p><strong>Notes:</strong></p><ul><li>This function accepts three keyword arguments: <code>maxoutdim</code>, <code>tol</code>, and <code>maxiter</code>.</li><li>Function uses the <code>maxoutdim</code> parameter as an upper boundary when it automatically</li></ul><p>determines the latent space dimensionality.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/7f3673a349be9b7e4365324de55ec91c830359dc/src/ppca.jl#L206-L223">source</a></section></article><hr/><h3 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h3><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Bishop, C. M. Pattern Recognition and Machine Learning, 2006.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../lda/">« Linear Discriminant Analysis</a><a class="docs-footer-nextpage" href="../ica/">Independent Component Analysis »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Saturday 6 August 2022 02:20">Saturday 6 August 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
