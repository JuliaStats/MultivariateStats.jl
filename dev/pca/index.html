<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Principal Component Analysis · MultivariateStats.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultivariateStats.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../whiten/">Data Transformation</a></li><li><a class="tocitem" href="../lreg/">Regression</a></li><li><a class="tocitem" href="../lda/">Linear Discriminant Analysis</a></li><li class="is-active"><a class="tocitem" href>Principal Component Analysis</a><ul class="internal"><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Linear-Principal-Component-Analysis"><span>Linear Principal Component Analysis</span></a></li><li><a class="tocitem" href="#Kernel-Principal-Component-Analysis"><span>Kernel Principal Component Analysis</span></a></li><li><a class="tocitem" href="#Probabilistic-Principal-Component-Analysis"><span>Probabilistic Principal Component Analysis</span></a></li></ul></li><li><a class="tocitem" href="../ica/">Independent Component Analysis</a></li><li><a class="tocitem" href="../cca/">Canonical Correlation Analysis</a></li><li><a class="tocitem" href="../fa/">Factor Analysis</a></li><li><a class="tocitem" href="../mds/">Multidimensional Scaling</a></li><li><a class="tocitem" href="../api/">Development</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Principal Component Analysis</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Principal Component Analysis</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/master/docs/src/pca.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Principal-Component-Analysis">Principal Component Analysis</a><a id="Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Principal-Component-Analysis" title="Permalink"></a></h1><p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA) derives an orthogonal projection to convert a given set of observations to linearly uncorrelated variables, called <em>principal components</em>.</p><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>Performing <a href="#MultivariateStats.PCA"><code>PCA</code></a> on <em>Iris</em> data set:</p><pre><code class="language-julia hljs">using MultivariateStats, RDatasets, Plots

# load iris dataset
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)

# split half to training set
Xtr = Matrix(iris[1:2:end,1:4])&#39;
Xtr_labels = Vector(iris[1:2:end,5])

# split other half to testing set
Xte = Matrix(iris[2:2:end,1:4])&#39;
Xte_labels = Vector(iris[2:2:end,5])</code></pre><p>Suppose <code>Xtr</code> and <code>Xte</code> are training and testing data matrix, with each observation in a column. We train a PCA model, allowing up to 3 dimensions:</p><pre><code class="language-julia hljs">M = fit(PCA, Xtr; maxoutdim=3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PCA(indim = 4, outdim = 3, principalratio = 0.9957325846529409)

Pattern matrix (unstandardized loadings):
────────────────────────────────────
         PC1         PC2         PC3
────────────────────────────────────
1   0.70954    0.344711   -0.160106
2  -0.227592   0.29865     0.215417
3   1.77976   -0.0797511   0.0197705
4   0.764206  -0.0453779   0.166764
────────────────────────────────────

Importance of components:
─────────────────────────────────────────────────────────
                                PC1        PC2        PC3
─────────────────────────────────────────────────────────
SS Loadings (Eigenvalues)  4.3068    0.216437   0.100239
Variance explained         0.927532  0.0466128  0.021588
Cumulative variance        0.927532  0.974145   0.995733
Proportion explained       0.931507  0.0468125  0.0216805
Cumulative proportion      0.931507  0.978319   1.0
─────────────────────────────────────────────────────────</code></pre><p>Then, apply PCA model to the testing set</p><pre><code class="language-julia hljs">Yte = predict(M, Xte)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3×75 Matrix{Float64}:
  2.72714    2.75491     2.32396   …  -1.92047   -1.74161   -1.37706
 -0.230916  -0.406149    0.646374      0.246554   0.127625  -0.280295
  0.253119   0.0271266  -0.230469     -0.180044  -0.123165  -0.314992</code></pre><p>And, reconstruct testing observations (approximately) to the original space</p><pre><code class="language-julia hljs">Xr = reconstruct(M, Yte)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4×75 Matrix{Float64}:
 4.86449  4.61087   5.40782   5.00775   …  6.79346  6.58825  6.46774  5.94384
 3.04262  3.08695   3.89061   3.39069      3.20785  3.13416  3.03873  2.94737
 1.46099  1.48132   1.68656   1.48668      5.91124  5.39197  5.25542  5.02469
 0.10362  0.229519  0.421233  0.221041     2.28224  1.99665  1.91243  1.91901</code></pre><p>Now, we group results by testing set labels for color coding and visualize first 3 principal components in 3D plot</p><pre><code class="language-julia hljs">setosa = Yte[:,Xte_labels.==&quot;setosa&quot;]
versicolor = Yte[:,Xte_labels.==&quot;versicolor&quot;]
virginica = Yte[:,Xte_labels.==&quot;virginica&quot;]

p = scatter(setosa[1,:],setosa[2,:],setosa[3,:],marker=:circle,linewidth=0)
scatter!(versicolor[1,:],versicolor[2,:],versicolor[3,:],marker=:circle,linewidth=0)
scatter!(virginica[1,:],virginica[2,:],virginica[3,:],marker=:circle,linewidth=0)
plot!(p,xlabel=&quot;PC1&quot;,ylabel=&quot;PC2&quot;,zlabel=&quot;PC3&quot;)</code></pre><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip330">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip330)" d="
M0 1600 L2400 1600 L2400 0 L0 0  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip331">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<defs>
  <clipPath id="clip332">
    <rect x="287" y="47" width="2066" height="1377"/>
  </clipPath>
</defs>
<path clip-path="url(#clip332)" d="
M1204.31 1149.71 L1179.68 627.219 L1312.89 444.958 L1501.91 539.937 L1481.55 1029.94 L1393.41 1354.16 L1204.31 1149.71  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  862.042,1148.1 1254.23,805.522 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1254.23,805.522 1245.21,205.842 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  948.11,1182.98 1330.4,828.853 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1330.4,828.853 1331.84,222.157 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1037.4,1219.16 1408.89,852.895 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1408.89,852.895 1421.49,239.04 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1130.11,1256.73 1489.82,877.682 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1489.82,877.682 1514.31,256.522 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1226.43,1295.76 1573.29,903.249 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1573.29,903.249 1610.49,274.636 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1326.58,1336.34 1659.43,929.634 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1659.43,929.634 1710.2,293.415 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1430.78,1378.57 1748.37,956.876 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1748.37,956.876 1813.65,312.897 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  809.917,1126.98 1476.45,1397.07 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  862.042,1148.1 867.771,1143.09 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  948.11,1182.98 953.713,1177.79 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1037.4,1219.16 1042.87,1213.77 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1130.11,1256.73 1135.42,1251.13 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1226.43,1295.76 1231.57,1289.94 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1326.58,1336.34 1331.52,1330.29 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1430.78,1378.57 1435.52,1372.27 
  "/>
<path clip-path="url(#clip330)" d="M779.834 1168.11 L809.51 1168.11 L809.51 1172.04 L779.834 1172.04 L779.834 1168.11 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M833.769 1166.3 Q837.126 1167.02 839.001 1169.29 Q840.899 1171.55 840.899 1174.89 Q840.899 1180 837.38 1182.8 Q833.862 1185.61 827.38 1185.61 Q825.205 1185.61 822.89 1185.17 Q820.598 1184.75 818.144 1183.89 L818.144 1179.38 Q820.089 1180.51 822.404 1181.09 Q824.718 1181.67 827.242 1181.67 Q831.64 1181.67 833.931 1179.93 Q836.246 1178.2 836.246 1174.89 Q836.246 1171.83 834.093 1170.12 Q831.964 1168.38 828.144 1168.38 L824.117 1168.38 L824.117 1164.54 L828.33 1164.54 Q831.779 1164.54 833.607 1163.18 Q835.436 1161.79 835.436 1159.19 Q835.436 1156.53 833.538 1155.12 Q831.663 1153.68 828.144 1153.68 Q826.223 1153.68 824.024 1154.1 Q821.825 1154.52 819.186 1155.4 L819.186 1151.23 Q821.848 1150.49 824.163 1150.12 Q826.501 1149.75 828.561 1149.75 Q833.885 1149.75 836.987 1152.18 Q840.089 1154.59 840.089 1158.71 Q840.089 1161.58 838.445 1163.57 Q836.802 1165.54 833.769 1166.3 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M866.851 1202.98 L896.527 1202.98 L896.527 1206.92 L866.851 1206.92 L866.851 1202.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M910.647 1215.88 L926.967 1215.88 L926.967 1219.81 L905.022 1219.81 L905.022 1215.88 Q907.685 1213.12 912.268 1208.49 Q916.874 1203.84 918.055 1202.5 Q920.3 1199.97 921.18 1198.24 Q922.083 1196.48 922.083 1194.79 Q922.083 1192.03 920.138 1190.3 Q918.217 1188.56 915.115 1188.56 Q912.916 1188.56 910.462 1189.33 Q908.032 1190.09 905.254 1191.64 L905.254 1186.92 Q908.078 1185.78 910.532 1185.2 Q912.985 1184.63 915.022 1184.63 Q920.393 1184.63 923.587 1187.31 Q926.782 1190 926.782 1194.49 Q926.782 1196.62 925.971 1198.54 Q925.184 1200.44 923.078 1203.03 Q922.499 1203.7 919.397 1206.92 Q916.296 1210.11 910.647 1215.88 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M955.776 1239.17 L985.452 1239.17 L985.452 1243.1 L955.776 1243.1 L955.776 1239.17 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M996.354 1252.06 L1003.99 1252.06 L1003.99 1225.69 L995.683 1227.36 L995.683 1223.1 L1003.95 1221.44 L1008.62 1221.44 L1008.62 1252.06 L1016.26 1252.06 L1016.26 1256 L996.354 1256 L996.354 1252.06 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1097.02 1262.08 Q1093.41 1262.08 1091.58 1265.65 Q1089.78 1269.19 1089.78 1276.32 Q1089.78 1283.42 1091.58 1286.99 Q1093.41 1290.53 1097.02 1290.53 Q1100.66 1290.53 1102.46 1286.99 Q1104.29 1283.42 1104.29 1276.32 Q1104.29 1269.19 1102.46 1265.65 Q1100.66 1262.08 1097.02 1262.08 M1097.02 1258.38 Q1102.83 1258.38 1105.89 1262.98 Q1108.97 1267.57 1108.97 1276.32 Q1108.97 1285.04 1105.89 1289.65 Q1102.83 1294.23 1097.02 1294.23 Q1091.21 1294.23 1088.14 1289.65 Q1085.08 1285.04 1085.08 1276.32 Q1085.08 1267.57 1088.14 1262.98 Q1091.21 1258.38 1097.02 1258.38 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1185.38 1328.66 L1193.02 1328.66 L1193.02 1302.29 L1184.71 1303.96 L1184.71 1299.7 L1192.97 1298.03 L1197.65 1298.03 L1197.65 1328.66 L1205.29 1328.66 L1205.29 1332.59 L1185.38 1332.59 L1185.38 1328.66 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1289.11 1369.24 L1305.43 1369.24 L1305.43 1373.17 L1283.49 1373.17 L1283.49 1369.24 Q1286.15 1366.48 1290.73 1361.86 Q1295.34 1357.2 1296.52 1355.86 Q1298.77 1353.34 1299.65 1351.6 Q1300.55 1349.84 1300.55 1348.15 Q1300.55 1345.4 1298.6 1343.66 Q1296.68 1341.92 1293.58 1341.92 Q1291.38 1341.92 1288.93 1342.69 Q1286.5 1343.45 1283.72 1345 L1283.72 1340.28 Q1286.54 1339.15 1289 1338.57 Q1291.45 1337.99 1293.49 1337.99 Q1298.86 1337.99 1302.05 1340.67 Q1305.25 1343.36 1305.25 1347.85 Q1305.25 1349.98 1304.44 1351.9 Q1303.65 1353.8 1301.54 1356.39 Q1300.96 1357.06 1297.86 1360.28 Q1294.76 1363.48 1289.11 1369.24 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1402.51 1396.77 Q1405.86 1397.48 1407.74 1399.75 Q1409.64 1402.02 1409.64 1405.35 Q1409.64 1410.47 1406.12 1413.27 Q1402.6 1416.07 1396.12 1416.07 Q1393.94 1416.07 1391.63 1415.63 Q1389.34 1415.22 1386.88 1414.36 L1386.88 1409.85 Q1388.83 1410.98 1391.14 1411.56 Q1393.46 1412.14 1395.98 1412.14 Q1400.38 1412.14 1402.67 1410.4 Q1404.98 1408.66 1404.98 1405.35 Q1404.98 1402.3 1402.83 1400.59 Q1400.7 1398.85 1396.88 1398.85 L1392.85 1398.85 L1392.85 1395.01 L1397.07 1395.01 Q1400.52 1395.01 1402.34 1393.64 Q1404.17 1392.25 1404.17 1389.66 Q1404.17 1387 1402.28 1385.59 Q1400.4 1384.15 1396.88 1384.15 Q1394.96 1384.15 1392.76 1384.57 Q1390.56 1384.98 1387.92 1385.86 L1387.92 1381.7 Q1390.59 1380.96 1392.9 1380.59 Q1395.24 1380.22 1397.3 1380.22 Q1402.62 1380.22 1405.72 1382.65 Q1408.83 1385.05 1408.83 1389.17 Q1408.83 1392.04 1407.18 1394.03 Q1405.54 1396 1402.51 1396.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M947.152 1390.86 L947.152 1408.71 L955.237 1408.71 Q959.725 1408.71 962.176 1406.39 Q964.626 1404.06 964.626 1399.77 Q964.626 1395.5 962.176 1393.18 Q959.725 1390.86 955.237 1390.86 L947.152 1390.86 M940.723 1385.57 L955.237 1385.57 Q963.226 1385.57 967.3 1389.2 Q971.406 1392.8 971.406 1399.77 Q971.406 1406.8 967.3 1410.4 Q963.226 1413.99 955.237 1413.99 L947.152 1413.99 L947.152 1433.09 L940.723 1433.09 L940.723 1385.57 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1015.62 1389.23 L1015.62 1396.01 Q1012.37 1392.99 1008.68 1391.49 Q1005.02 1390 1000.88 1390 Q992.731 1390 988.402 1394.99 Q984.074 1399.96 984.074 1409.38 Q984.074 1418.77 988.402 1423.77 Q992.731 1428.73 1000.88 1428.73 Q1005.02 1428.73 1008.68 1427.24 Q1012.37 1425.74 1015.62 1422.72 L1015.62 1429.43 Q1012.24 1431.72 1008.45 1432.87 Q1004.7 1434.01 1000.5 1434.01 Q989.707 1434.01 983.501 1427.43 Q977.294 1420.81 977.294 1409.38 Q977.294 1397.92 983.501 1391.33 Q989.707 1384.71 1000.5 1384.71 Q1004.76 1384.71 1008.52 1385.86 Q1012.31 1386.97 1015.62 1389.23 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1027.23 1427.68 L1037.74 1427.68 L1037.74 1391.43 L1026.31 1393.72 L1026.31 1387.86 L1037.67 1385.57 L1044.1 1385.57 L1044.1 1427.68 L1054.61 1427.68 L1054.61 1433.09 L1027.23 1433.09 L1027.23 1427.68 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1490.23,1378.08 827.138,1112.45 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  827.138,1112.45 743.998,426.001 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1559.17,1283.03 913.901,1039.27 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  913.901,1039.27 848.525,372.4 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1622.22,1196.12 994.093,971.623 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  994.093,971.623 943.917,323.483 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1680.08,1116.34 1068.43,908.916 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1068.43,908.916 1031.32,278.662 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1733.38,1042.85 1137.54,850.624 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1137.54,850.624 1111.7,237.442 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1782.64,974.944 1201.95,796.296 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1201.95,796.296 1185.87,199.408 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1476.45,1397.07 1787.13,968.749 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1490.23,1378.08 1481.15,1374.44 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1559.17,1283.03 1550.39,1279.71 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1622.22,1196.12 1613.71,1193.07 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1680.08,1116.34 1671.84,1113.54 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1733.38,1042.85 1725.39,1040.27 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  1782.64,974.944 1774.88,972.555 
  "/>
<path clip-path="url(#clip330)" d="M1518.02 1386.09 L1547.7 1386.09 L1547.7 1390.02 L1518.02 1390.02 L1518.02 1386.09 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1558.6 1398.98 L1566.24 1398.98 L1566.24 1372.61 L1557.93 1374.28 L1557.93 1370.02 L1566.19 1368.36 L1570.87 1368.36 L1570.87 1398.98 L1578.51 1398.98 L1578.51 1402.92 L1558.6 1402.92 L1558.6 1398.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1587.95 1397.04 L1592.84 1397.04 L1592.84 1402.92 L1587.95 1402.92 L1587.95 1397.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1613.02 1371.43 Q1609.41 1371.43 1607.58 1375 Q1605.78 1378.54 1605.78 1385.67 Q1605.78 1392.78 1607.58 1396.34 Q1609.41 1399.88 1613.02 1399.88 Q1616.66 1399.88 1618.46 1396.34 Q1620.29 1392.78 1620.29 1385.67 Q1620.29 1378.54 1618.46 1375 Q1616.66 1371.43 1613.02 1371.43 M1613.02 1367.73 Q1618.83 1367.73 1621.89 1372.34 Q1624.97 1376.92 1624.97 1385.67 Q1624.97 1394.4 1621.89 1399 Q1618.83 1403.59 1613.02 1403.59 Q1607.21 1403.59 1604.13 1399 Q1601.08 1394.4 1601.08 1385.67 Q1601.08 1376.92 1604.13 1372.34 Q1607.21 1367.73 1613.02 1367.73 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1586.97 1291.04 L1616.64 1291.04 L1616.64 1294.97 L1586.97 1294.97 L1586.97 1291.04 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1636.73 1276.38 Q1633.12 1276.38 1631.29 1279.95 Q1629.49 1283.49 1629.49 1290.62 Q1629.49 1297.72 1631.29 1301.29 Q1633.12 1304.83 1636.73 1304.83 Q1640.37 1304.83 1642.17 1301.29 Q1644 1297.72 1644 1290.62 Q1644 1283.49 1642.17 1279.95 Q1640.37 1276.38 1636.73 1276.38 M1636.73 1272.68 Q1642.54 1272.68 1645.6 1277.29 Q1648.68 1281.87 1648.68 1290.62 Q1648.68 1299.35 1645.6 1303.95 Q1642.54 1308.54 1636.73 1308.54 Q1630.92 1308.54 1627.85 1303.95 Q1624.79 1299.35 1624.79 1290.62 Q1624.79 1281.87 1627.85 1277.29 Q1630.92 1272.68 1636.73 1272.68 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1656.9 1301.98 L1661.78 1301.98 L1661.78 1307.86 L1656.9 1307.86 L1656.9 1301.98 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1672.01 1273.3 L1690.37 1273.3 L1690.37 1277.24 L1676.29 1277.24 L1676.29 1285.71 Q1677.31 1285.36 1678.33 1285.2 Q1679.35 1285.02 1680.37 1285.02 Q1686.16 1285.02 1689.54 1288.19 Q1692.91 1291.36 1692.91 1296.78 Q1692.91 1302.35 1689.44 1305.46 Q1685.97 1308.54 1679.65 1308.54 Q1677.48 1308.54 1675.21 1308.16 Q1672.96 1307.79 1670.55 1307.05 L1670.55 1302.35 Q1672.64 1303.49 1674.86 1304.04 Q1677.08 1304.6 1679.56 1304.6 Q1683.56 1304.6 1685.9 1302.49 Q1688.24 1300.39 1688.24 1296.78 Q1688.24 1293.16 1685.9 1291.06 Q1683.56 1288.95 1679.56 1288.95 Q1677.68 1288.95 1675.81 1289.37 Q1673.96 1289.79 1672.01 1290.66 L1672.01 1273.3 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1661.95 1189.47 Q1658.34 1189.47 1656.51 1193.03 Q1654.71 1196.58 1654.71 1203.71 Q1654.71 1210.81 1656.51 1214.38 Q1658.34 1217.92 1661.95 1217.92 Q1665.59 1217.92 1667.39 1214.38 Q1669.22 1210.81 1669.22 1203.71 Q1669.22 1196.58 1667.39 1193.03 Q1665.59 1189.47 1661.95 1189.47 M1661.95 1185.77 Q1667.76 1185.77 1670.82 1190.37 Q1673.9 1194.96 1673.9 1203.71 Q1673.9 1212.43 1670.82 1217.04 Q1667.76 1221.62 1661.95 1221.62 Q1656.14 1221.62 1653.06 1217.04 Q1650.01 1212.43 1650.01 1203.71 Q1650.01 1194.96 1653.06 1190.37 Q1656.14 1185.77 1661.95 1185.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1682.11 1215.07 L1687 1215.07 L1687 1220.95 L1682.11 1220.95 L1682.11 1215.07 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1707.18 1189.47 Q1703.57 1189.47 1701.74 1193.03 Q1699.94 1196.58 1699.94 1203.71 Q1699.94 1210.81 1701.74 1214.38 Q1703.57 1217.92 1707.18 1217.92 Q1710.82 1217.92 1712.62 1214.38 Q1714.45 1210.81 1714.45 1203.71 Q1714.45 1196.58 1712.62 1193.03 Q1710.82 1189.47 1707.18 1189.47 M1707.18 1185.77 Q1712.99 1185.77 1716.05 1190.37 Q1719.13 1194.96 1719.13 1203.71 Q1719.13 1212.43 1716.05 1217.04 Q1712.99 1221.62 1707.18 1221.62 Q1701.37 1221.62 1698.29 1217.04 Q1695.24 1212.43 1695.24 1203.71 Q1695.24 1194.96 1698.29 1190.37 Q1701.37 1185.77 1707.18 1185.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1719.82 1109.69 Q1716.21 1109.69 1714.38 1113.26 Q1712.57 1116.8 1712.57 1123.93 Q1712.57 1131.03 1714.38 1134.6 Q1716.21 1138.14 1719.82 1138.14 Q1723.45 1138.14 1725.26 1134.6 Q1727.09 1131.03 1727.09 1123.93 Q1727.09 1116.8 1725.26 1113.26 Q1723.45 1109.69 1719.82 1109.69 M1719.82 1105.99 Q1725.63 1105.99 1728.68 1110.6 Q1731.76 1115.18 1731.76 1123.93 Q1731.76 1132.66 1728.68 1137.26 Q1725.63 1141.84 1719.82 1141.84 Q1714.01 1141.84 1710.93 1137.26 Q1707.87 1132.66 1707.87 1123.93 Q1707.87 1115.18 1710.93 1110.6 Q1714.01 1105.99 1719.82 1105.99 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1739.98 1135.29 L1744.86 1135.29 L1744.86 1141.17 L1739.98 1141.17 L1739.98 1135.29 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1755.1 1106.61 L1773.45 1106.61 L1773.45 1110.55 L1759.38 1110.55 L1759.38 1119.02 Q1760.4 1118.67 1761.41 1118.51 Q1762.43 1118.33 1763.45 1118.33 Q1769.24 1118.33 1772.62 1121.5 Q1776 1124.67 1776 1130.09 Q1776 1135.66 1772.53 1138.77 Q1769.05 1141.84 1762.73 1141.84 Q1760.56 1141.84 1758.29 1141.47 Q1756.04 1141.1 1753.64 1140.36 L1753.64 1135.66 Q1755.72 1136.8 1757.94 1137.35 Q1760.16 1137.91 1762.64 1137.91 Q1766.65 1137.91 1768.98 1135.8 Q1771.32 1133.7 1771.32 1130.09 Q1771.32 1126.47 1768.98 1124.37 Q1766.65 1122.26 1762.64 1122.26 Q1760.77 1122.26 1758.89 1122.68 Q1757.04 1123.1 1755.1 1123.97 L1755.1 1106.61 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1761.85 1063.75 L1769.49 1063.75 L1769.49 1037.39 L1761.18 1039.05 L1761.18 1034.8 L1769.44 1033.13 L1774.11 1033.13 L1774.11 1063.75 L1781.75 1063.75 L1781.75 1067.69 L1761.85 1067.69 L1761.85 1063.75 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1791.2 1061.81 L1796.08 1061.81 L1796.08 1067.69 L1791.2 1067.69 L1791.2 1061.81 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1816.27 1036.21 Q1812.66 1036.21 1810.83 1039.77 Q1809.02 1043.31 1809.02 1050.44 Q1809.02 1057.55 1810.83 1061.11 Q1812.66 1064.66 1816.27 1064.66 Q1819.9 1064.66 1821.71 1061.11 Q1823.54 1057.55 1823.54 1050.44 Q1823.54 1043.31 1821.71 1039.77 Q1819.9 1036.21 1816.27 1036.21 M1816.27 1032.5 Q1822.08 1032.5 1825.13 1037.11 Q1828.21 1041.69 1828.21 1050.44 Q1828.21 1059.17 1825.13 1063.78 Q1822.08 1068.36 1816.27 1068.36 Q1810.46 1068.36 1807.38 1063.78 Q1804.32 1059.17 1804.32 1050.44 Q1804.32 1041.69 1807.38 1037.11 Q1810.46 1032.5 1816.27 1032.5 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1811.1 995.844 L1818.74 995.844 L1818.74 969.478 L1810.43 971.145 L1810.43 966.886 L1818.7 965.219 L1823.37 965.219 L1823.37 995.844 L1831.01 995.844 L1831.01 999.779 L1811.1 999.779 L1811.1 995.844 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1840.46 993.899 L1845.34 993.899 L1845.34 999.779 L1840.46 999.779 L1840.46 993.899 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1855.57 965.219 L1873.93 965.219 L1873.93 969.154 L1859.85 969.154 L1859.85 977.626 Q1860.87 977.279 1861.89 977.117 Q1862.91 976.932 1863.93 976.932 Q1869.71 976.932 1873.09 980.103 Q1876.47 983.275 1876.47 988.691 Q1876.47 994.27 1873 997.372 Q1869.53 1000.45 1863.21 1000.45 Q1861.03 1000.45 1858.77 1000.08 Q1856.52 999.71 1854.11 998.969 L1854.11 994.27 Q1856.2 995.404 1858.42 995.96 Q1860.64 996.515 1863.12 996.515 Q1867.12 996.515 1869.46 994.409 Q1871.8 992.302 1871.8 988.691 Q1871.8 985.08 1869.46 982.974 Q1867.12 980.867 1863.12 980.867 Q1861.24 980.867 1859.37 981.284 Q1857.52 981.7 1855.57 982.58 L1855.57 965.219 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1721.89 1297.05 L1721.89 1314.91 L1729.98 1314.91 Q1734.47 1314.91 1736.92 1312.59 Q1739.37 1310.26 1739.37 1305.97 Q1739.37 1301.7 1736.92 1299.38 Q1734.47 1297.05 1729.98 1297.05 L1721.89 1297.05 M1715.46 1291.77 L1729.98 1291.77 Q1737.97 1291.77 1742.04 1295.4 Q1746.15 1299 1746.15 1305.97 Q1746.15 1313 1742.04 1316.6 Q1737.97 1320.19 1729.98 1320.19 L1721.89 1320.19 L1721.89 1339.29 L1715.46 1339.29 L1715.46 1291.77 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1790.36 1295.43 L1790.36 1302.21 Q1787.11 1299.19 1783.42 1297.69 Q1779.76 1296.2 1775.62 1296.2 Q1767.47 1296.2 1763.14 1301.19 Q1758.81 1306.16 1758.81 1315.58 Q1758.81 1324.97 1763.14 1329.97 Q1767.47 1334.93 1775.62 1334.93 Q1779.76 1334.93 1783.42 1333.44 Q1787.11 1331.94 1790.36 1328.92 L1790.36 1335.63 Q1786.98 1337.92 1783.2 1339.07 Q1779.44 1340.21 1775.24 1340.21 Q1764.45 1340.21 1758.24 1333.63 Q1752.04 1327.01 1752.04 1315.58 Q1752.04 1304.12 1758.24 1297.53 Q1764.45 1290.91 1775.24 1290.91 Q1779.5 1290.91 1783.26 1292.06 Q1787.05 1293.17 1790.36 1295.43 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M1806.4 1333.88 L1828.84 1333.88 L1828.84 1339.29 L1798.66 1339.29 L1798.66 1333.88 Q1802.32 1330.09 1808.63 1323.73 Q1814.96 1317.33 1816.58 1315.48 Q1819.67 1312.01 1820.88 1309.63 Q1822.12 1307.21 1822.12 1304.88 Q1822.12 1301.1 1819.45 1298.71 Q1816.81 1296.32 1812.54 1296.32 Q1809.52 1296.32 1806.14 1297.37 Q1802.8 1298.42 1798.98 1300.56 L1798.98 1294.06 Q1802.87 1292.5 1806.24 1291.71 Q1809.61 1290.91 1812.41 1290.91 Q1819.8 1290.91 1824.19 1294.6 Q1828.58 1298.3 1828.58 1304.47 Q1828.58 1307.4 1827.47 1310.04 Q1826.39 1312.65 1823.49 1316.22 Q1822.69 1317.14 1818.43 1321.56 Q1814.16 1325.96 1806.4 1333.88 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  807.991,1111.66 1207.5,777.717 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1207.5,777.717 1788.75,954.187 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  792.43,987.969 1204.71,668.515 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1204.71,668.515 1801.75,836.893 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  775.895,856.521 1201.78,553.762 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1201.78,553.762 1815.5,712.906 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  758.289,716.567 1198.69,433.022 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1198.69,433.022 1830.05,581.636 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  739.506,567.254 1195.44,305.813 
  "/>
<polyline clip-path="url(#clip332)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="
  1195.44,305.813 1845.48,442.421 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  809.917,1126.98 723.086,436.725 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  807.991,1111.66 813.82,1106.79 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  792.43,987.969 798.478,983.284 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  775.895,856.521 782.177,852.055 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  758.289,716.567 764.825,712.359 
  "/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  739.506,567.254 746.316,563.35 
  "/>
<path clip-path="url(#clip330)" d="M642.085 1112.12 L671.761 1112.12 L671.761 1116.05 L642.085 1116.05 L642.085 1112.12 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M691.853 1097.46 Q688.242 1097.46 686.413 1101.03 Q684.608 1104.57 684.608 1111.7 Q684.608 1118.81 686.413 1122.37 Q688.242 1125.91 691.853 1125.91 Q695.488 1125.91 697.293 1122.37 Q699.122 1118.81 699.122 1111.7 Q699.122 1104.57 697.293 1101.03 Q695.488 1097.46 691.853 1097.46 M691.853 1093.76 Q697.663 1093.76 700.719 1098.37 Q703.798 1102.95 703.798 1111.7 Q703.798 1120.43 700.719 1125.03 Q697.663 1129.62 691.853 1129.62 Q686.043 1129.62 682.964 1125.03 Q679.909 1120.43 679.909 1111.7 Q679.909 1102.95 682.964 1098.37 Q686.043 1093.76 691.853 1093.76 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M712.015 1123.06 L716.899 1123.06 L716.899 1128.94 L712.015 1128.94 L712.015 1123.06 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M727.131 1094.38 L745.487 1094.38 L745.487 1098.32 L731.413 1098.32 L731.413 1106.79 Q732.432 1106.44 733.45 1106.28 Q734.469 1106.1 735.487 1106.1 Q741.274 1106.1 744.654 1109.27 Q748.034 1112.44 748.034 1117.86 Q748.034 1123.44 744.561 1126.54 Q741.089 1129.62 734.77 1129.62 Q732.594 1129.62 730.325 1129.25 Q728.08 1128.88 725.673 1128.13 L725.673 1123.44 Q727.756 1124.57 729.978 1125.13 Q732.2 1125.68 734.677 1125.68 Q738.682 1125.68 741.02 1123.57 Q743.358 1121.47 743.358 1117.86 Q743.358 1114.25 741.02 1112.14 Q738.682 1110.03 734.677 1110.03 Q732.802 1110.03 730.927 1110.45 Q729.075 1110.87 727.131 1111.75 L727.131 1094.38 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M767.246 1097.46 Q763.635 1097.46 761.807 1101.03 Q760.001 1104.57 760.001 1111.7 Q760.001 1118.81 761.807 1122.37 Q763.635 1125.91 767.246 1125.91 Q770.881 1125.91 772.686 1122.37 Q774.515 1118.81 774.515 1111.7 Q774.515 1104.57 772.686 1101.03 Q770.881 1097.46 767.246 1097.46 M767.246 1093.76 Q773.057 1093.76 776.112 1098.37 Q779.191 1102.95 779.191 1111.7 Q779.191 1120.43 776.112 1125.03 Q773.057 1129.62 767.246 1129.62 Q761.436 1129.62 758.358 1125.03 Q755.302 1120.43 755.302 1111.7 Q755.302 1102.95 758.358 1098.37 Q761.436 1093.76 767.246 1093.76 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M627.52 988.421 L657.196 988.421 L657.196 992.356 L627.52 992.356 L627.52 988.421 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M677.288 973.768 Q673.677 973.768 671.848 977.333 Q670.043 980.874 670.043 988.004 Q670.043 995.11 671.848 998.675 Q673.677 1002.22 677.288 1002.22 Q680.923 1002.22 682.728 998.675 Q684.557 995.11 684.557 988.004 Q684.557 980.874 682.728 977.333 Q680.923 973.768 677.288 973.768 M677.288 970.064 Q683.098 970.064 686.154 974.671 Q689.233 979.254 689.233 988.004 Q689.233 996.731 686.154 1001.34 Q683.098 1005.92 677.288 1005.92 Q671.478 1005.92 668.399 1001.34 Q665.344 996.731 665.344 988.004 Q665.344 979.254 668.399 974.671 Q671.478 970.064 677.288 970.064 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M697.45 999.37 L702.334 999.37 L702.334 1005.25 L697.45 1005.25 L697.45 999.37 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M716.547 1001.31 L732.867 1001.31 L732.867 1005.25 L710.922 1005.25 L710.922 1001.31 Q713.584 998.56 718.168 993.93 Q722.774 989.277 723.955 987.935 Q726.2 985.411 727.08 983.675 Q727.982 981.916 727.982 980.226 Q727.982 977.472 726.038 975.736 Q724.117 973.999 721.015 973.999 Q718.816 973.999 716.362 974.763 Q713.932 975.527 711.154 977.078 L711.154 972.356 Q713.978 971.222 716.432 970.643 Q718.885 970.064 720.922 970.064 Q726.293 970.064 729.487 972.75 Q732.682 975.435 732.682 979.925 Q732.682 982.055 731.871 983.976 Q731.084 985.874 728.978 988.467 Q728.399 989.138 725.297 992.356 Q722.195 995.55 716.547 1001.31 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M742.728 970.689 L761.084 970.689 L761.084 974.624 L747.01 974.624 L747.01 983.097 Q748.029 982.749 749.047 982.587 Q750.066 982.402 751.084 982.402 Q756.871 982.402 760.251 985.574 Q763.63 988.745 763.63 994.161 Q763.63 999.74 760.158 1002.84 Q756.686 1005.92 750.367 1005.92 Q748.191 1005.92 745.922 1005.55 Q743.677 1005.18 741.269 1004.44 L741.269 999.74 Q743.353 1000.87 745.575 1001.43 Q747.797 1001.99 750.274 1001.99 Q754.279 1001.99 756.617 999.879 Q758.955 997.773 758.955 994.161 Q758.955 990.55 756.617 988.444 Q754.279 986.337 750.274 986.337 Q748.399 986.337 746.524 986.754 Q744.672 987.171 742.728 988.05 L742.728 970.689 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M659.757 842.32 Q656.146 842.32 654.317 845.884 Q652.512 849.426 652.512 856.556 Q652.512 863.662 654.317 867.227 Q656.146 870.769 659.757 870.769 Q663.391 870.769 665.197 867.227 Q667.026 863.662 667.026 856.556 Q667.026 849.426 665.197 845.884 Q663.391 842.32 659.757 842.32 M659.757 838.616 Q665.567 838.616 668.623 843.222 Q671.702 847.806 671.702 856.556 Q671.702 865.283 668.623 869.889 Q665.567 874.472 659.757 874.472 Q653.947 874.472 650.868 869.889 Q647.813 865.283 647.813 856.556 Q647.813 847.806 650.868 843.222 Q653.947 838.616 659.757 838.616 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M679.919 867.921 L684.803 867.921 L684.803 873.801 L679.919 873.801 L679.919 867.921 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M704.988 842.32 Q701.377 842.32 699.549 845.884 Q697.743 849.426 697.743 856.556 Q697.743 863.662 699.549 867.227 Q701.377 870.769 704.988 870.769 Q708.623 870.769 710.428 867.227 Q712.257 863.662 712.257 856.556 Q712.257 849.426 710.428 845.884 Q708.623 842.32 704.988 842.32 M704.988 838.616 Q710.799 838.616 713.854 843.222 Q716.933 847.806 716.933 856.556 Q716.933 865.283 713.854 869.889 Q710.799 874.472 704.988 874.472 Q699.178 874.472 696.1 869.889 Q693.044 865.283 693.044 856.556 Q693.044 847.806 696.1 843.222 Q699.178 838.616 704.988 838.616 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M735.15 842.32 Q731.539 842.32 729.711 845.884 Q727.905 849.426 727.905 856.556 Q727.905 863.662 729.711 867.227 Q731.539 870.769 735.15 870.769 Q738.785 870.769 740.59 867.227 Q742.419 863.662 742.419 856.556 Q742.419 849.426 740.59 845.884 Q738.785 842.32 735.15 842.32 M735.15 838.616 Q740.961 838.616 744.016 843.222 Q747.095 847.806 747.095 856.556 Q747.095 865.283 744.016 869.889 Q740.961 874.472 735.15 874.472 Q729.34 874.472 726.262 869.889 Q723.206 865.283 723.206 856.556 Q723.206 847.806 726.262 843.222 Q729.34 838.616 735.15 838.616 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M643.147 702.366 Q639.536 702.366 637.707 705.931 Q635.902 709.472 635.902 716.602 Q635.902 723.708 637.707 727.273 Q639.536 730.815 643.147 730.815 Q646.781 730.815 648.587 727.273 Q650.415 723.708 650.415 716.602 Q650.415 709.472 648.587 705.931 Q646.781 702.366 643.147 702.366 M643.147 698.662 Q648.957 698.662 652.013 703.269 Q655.091 707.852 655.091 716.602 Q655.091 725.329 652.013 729.935 Q648.957 734.518 643.147 734.518 Q637.337 734.518 634.258 729.935 Q631.203 725.329 631.203 716.602 Q631.203 707.852 634.258 703.269 Q637.337 698.662 643.147 698.662 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M663.309 727.967 L668.193 727.967 L668.193 733.847 L663.309 733.847 L663.309 727.967 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M682.406 729.912 L698.725 729.912 L698.725 733.847 L676.781 733.847 L676.781 729.912 Q679.443 727.157 684.026 722.528 Q688.633 717.875 689.813 716.532 Q692.059 714.009 692.938 712.273 Q693.841 710.514 693.841 708.824 Q693.841 706.069 691.897 704.333 Q689.975 702.597 686.874 702.597 Q684.675 702.597 682.221 703.361 Q679.79 704.125 677.013 705.676 L677.013 700.954 Q679.837 699.819 682.29 699.241 Q684.744 698.662 686.781 698.662 Q692.151 698.662 695.346 701.347 Q698.54 704.032 698.54 708.523 Q698.54 710.653 697.73 712.574 Q696.943 714.472 694.837 717.065 Q694.258 717.736 691.156 720.954 Q688.054 724.148 682.406 729.912 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M708.586 699.287 L726.943 699.287 L726.943 703.222 L712.869 703.222 L712.869 711.694 Q713.887 711.347 714.906 711.185 Q715.924 711 716.943 711 Q722.73 711 726.11 714.171 Q729.489 717.343 729.489 722.759 Q729.489 728.338 726.017 731.44 Q722.545 734.518 716.225 734.518 Q714.049 734.518 711.781 734.148 Q709.536 733.778 707.128 733.037 L707.128 728.338 Q709.211 729.472 711.434 730.028 Q713.656 730.583 716.133 730.583 Q720.137 730.583 722.475 728.477 Q724.813 726.37 724.813 722.759 Q724.813 719.148 722.475 717.042 Q720.137 714.935 716.133 714.935 Q714.258 714.935 712.383 715.352 Q710.531 715.768 708.586 716.648 L708.586 699.287 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M623.369 553.053 Q619.758 553.053 617.929 556.618 Q616.123 560.16 616.123 567.289 Q616.123 574.396 617.929 577.96 Q619.758 581.502 623.369 581.502 Q627.003 581.502 628.808 577.96 Q630.637 574.396 630.637 567.289 Q630.637 560.16 628.808 556.618 Q627.003 553.053 623.369 553.053 M623.369 549.349 Q629.179 549.349 632.234 553.956 Q635.313 558.539 635.313 567.289 Q635.313 576.016 632.234 580.622 Q629.179 585.206 623.369 585.206 Q617.559 585.206 614.48 580.622 Q611.424 576.016 611.424 567.289 Q611.424 558.539 614.48 553.956 Q617.559 549.349 623.369 549.349 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M643.531 578.655 L648.415 578.655 L648.415 584.534 L643.531 584.534 L643.531 578.655 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M658.646 549.974 L677.003 549.974 L677.003 553.91 L662.929 553.91 L662.929 562.382 Q663.947 562.035 664.966 561.872 Q665.984 561.687 667.003 561.687 Q672.79 561.687 676.169 564.859 Q679.549 568.03 679.549 573.446 Q679.549 579.025 676.077 582.127 Q672.605 585.206 666.285 585.206 Q664.109 585.206 661.841 584.835 Q659.595 584.465 657.188 583.724 L657.188 579.025 Q659.271 580.159 661.493 580.715 Q663.716 581.271 666.193 581.271 Q670.197 581.271 672.535 579.164 Q674.873 577.058 674.873 573.446 Q674.873 569.835 672.535 567.729 Q670.197 565.622 666.193 565.622 Q664.318 565.622 662.443 566.039 Q660.591 566.456 658.646 567.335 L658.646 549.974 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M698.762 553.053 Q695.151 553.053 693.322 556.618 Q691.516 560.16 691.516 567.289 Q691.516 574.396 693.322 577.96 Q695.151 581.502 698.762 581.502 Q702.396 581.502 704.202 577.96 Q706.03 574.396 706.03 567.289 Q706.03 560.16 704.202 556.618 Q702.396 553.053 698.762 553.053 M698.762 549.349 Q704.572 549.349 707.627 553.956 Q710.706 558.539 710.706 567.289 Q710.706 576.016 707.627 580.622 Q704.572 585.206 698.762 585.206 Q692.952 585.206 689.873 580.622 Q686.817 576.016 686.817 567.289 Q686.817 558.539 689.873 553.956 Q692.952 549.349 698.762 549.349 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M504.308 859.828 L522.164 859.828 L522.164 851.744 Q522.164 847.256 519.841 844.805 Q517.517 842.355 513.22 842.355 Q508.955 842.355 506.632 844.805 Q504.308 847.256 504.308 851.744 L504.308 859.828 M499.025 866.258 L499.025 851.744 Q499.025 843.755 502.653 839.681 Q506.25 835.575 513.22 835.575 Q520.254 835.575 523.851 839.681 Q527.448 843.755 527.448 851.744 L527.448 859.828 L546.545 859.828 L546.545 866.258 L499.025 866.258 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M502.685 791.365 L509.465 791.365 Q506.441 794.612 504.945 798.304 Q503.449 801.964 503.449 806.102 Q503.449 814.25 508.446 818.579 Q513.411 822.907 522.833 822.907 Q532.222 822.907 537.219 818.579 Q542.184 814.25 542.184 806.102 Q542.184 801.964 540.688 798.304 Q539.192 794.612 536.169 791.365 L542.885 791.365 Q545.176 794.739 546.322 798.527 Q547.468 802.282 547.468 806.484 Q547.468 817.274 540.879 823.48 Q534.259 829.687 522.833 829.687 Q511.374 829.687 504.786 823.48 Q498.165 817.274 498.165 806.484 Q498.165 802.219 499.311 798.463 Q500.425 794.675 502.685 791.365 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M520.923 761.383 Q521.91 756.768 525.029 754.19 Q528.148 751.58 532.731 751.58 Q539.765 751.58 543.617 756.418 Q547.468 761.255 547.468 770.167 Q547.468 773.159 546.863 776.342 Q546.29 779.493 545.113 782.867 L538.906 782.867 Q540.466 780.193 541.261 777.011 Q542.057 773.828 542.057 770.358 Q542.057 764.311 539.67 761.16 Q537.283 757.977 532.731 757.977 Q528.53 757.977 526.175 760.937 Q523.787 763.865 523.787 769.117 L523.787 774.655 L518.504 774.655 L518.504 768.862 Q518.504 764.12 516.626 761.606 Q514.716 759.091 511.151 759.091 Q507.491 759.091 505.55 761.701 Q503.576 764.279 503.576 769.117 Q503.576 771.759 504.149 774.783 Q504.722 777.806 505.932 781.435 L500.202 781.435 Q499.184 777.774 498.675 774.592 Q498.165 771.377 498.165 768.544 Q498.165 761.224 501.507 756.959 Q504.818 752.694 510.483 752.694 Q514.43 752.694 517.167 754.953 Q519.873 757.213 520.923 761.383 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><circle clip-path="url(#clip332)" cx="1544.94" cy="785.972" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1516.64" cy="945.242" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1601.14" cy="913.453" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1572.36" cy="868.518" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1548.11" cy="789.331" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1545.39" cy="951.221" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1538.62" cy="999.619" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1670.78" cy="925.411" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1591.68" cy="883.113" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1600.91" cy="965.994" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1587.69" cy="967.417" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1529.76" cy="894.428" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1529.04" cy="751.009" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1594.89" cy="818.456" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1521.38" cy="935.804" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1589.46" cy="769.941" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1670.06" cy="892.306" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1592.53" cy="799.572" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1597.09" cy="931.496" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1580.11" cy="828.852" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1449.31" cy="758.753" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1542.22" cy="1011.55" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1529.7" cy="859.332" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1536.84" cy="970.347" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1576.48" cy="836.252" r="14.4" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1264.49" cy="745.487" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1145" cy="752.657" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1172.81" cy="839.347" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1203.29" cy="919.214" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1181.33" cy="1018.13" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1236.02" cy="865.491" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1191.11" cy="749.792" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1290.72" cy="576.079" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1225.55" cy="666.384" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1201.47" cy="708.118" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1258.83" cy="667.644" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1187.04" cy="661.115" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1272.48" cy="582.428" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1205" cy="651.406" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1273.58" cy="653.471" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1207.7" cy="677.754" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1107.34" cy="800.73" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1252.61" cy="966.235" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1166.91" cy="434.432" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1172.7" cy="814.811" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1213.63" cy="775.155" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1198.3" cy="845.176" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1233.87" cy="857.266" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1245.77" cy="669.709" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1216.29" cy="826.506" r="14.4" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1081.07" cy="960.891" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1098.05" cy="815.566" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1097.42" cy="495.125" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1102.39" cy="475.351" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1171.72" cy="873.665" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1106.7" cy="741.486" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1052.34" cy="971.459" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1154.18" cy="976.887" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1181.99" cy="706.958" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1049.58" cy="611.733" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1096.35" cy="1081.68" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1144.47" cy="737.71" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1161.3" cy="590.543" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1166.63" cy="894.17" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1164.87" cy="467.799" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1228.5" cy="586.514" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1147.79" cy="686.629" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1145.97" cy="500.537" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1140.7" cy="829.644" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1175.68" cy="730.075" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1197.28" cy="774.027" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1125.22" cy="856.476" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1161.92" cy="820.654" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1158.47" cy="812.071" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<circle clip-path="url(#clip332)" cx="1131.45" cy="973.013" r="14.4" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="3.2"/>
<path clip-path="url(#clip330)" d="
M2001.41 300.469 L2283.91 300.469 L2283.91 93.1086 L2001.41 93.1086  Z
  " fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<polyline clip-path="url(#clip330)" style="stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="
  2001.41,300.469 2283.91,300.469 2283.91,93.1086 2001.41,93.1086 2001.41,300.469 
  "/>
<circle clip-path="url(#clip330)" cx="2093.21" cy="144.949" r="23.04" fill="#009af9" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="5.12"/>
<path clip-path="url(#clip330)" d="M2198.85 164.636 Q2197.04 169.266 2195.33 170.678 Q2193.61 172.09 2190.74 172.09 L2187.34 172.09 L2187.34 168.525 L2189.84 168.525 Q2191.6 168.525 2192.57 167.692 Q2193.54 166.858 2194.73 163.756 L2195.49 161.812 L2185 136.303 L2189.52 136.303 L2197.62 156.581 L2205.72 136.303 L2210.23 136.303 L2198.85 164.636 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M2217.53 158.293 L2225.16 158.293 L2225.16 131.928 L2216.85 133.595 L2216.85 129.335 L2225.12 127.669 L2229.79 127.669 L2229.79 158.293 L2237.43 158.293 L2237.43 162.229 L2217.53 162.229 L2217.53 158.293 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><circle clip-path="url(#clip330)" cx="2093.21" cy="196.789" r="23.04" fill="#e26f46" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="5.12"/>
<path clip-path="url(#clip330)" d="M2198.85 216.476 Q2197.04 221.106 2195.33 222.518 Q2193.61 223.93 2190.74 223.93 L2187.34 223.93 L2187.34 220.365 L2189.84 220.365 Q2191.6 220.365 2192.57 219.532 Q2193.54 218.698 2194.73 215.596 L2195.49 213.652 L2185 188.143 L2189.52 188.143 L2197.62 208.421 L2205.72 188.143 L2210.23 188.143 L2198.85 216.476 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M2220.74 210.133 L2237.06 210.133 L2237.06 214.069 L2215.12 214.069 L2215.12 210.133 Q2217.78 207.379 2222.36 202.749 Q2226.97 198.096 2228.15 196.754 Q2230.4 194.231 2231.28 192.495 Q2232.18 190.735 2232.18 189.046 Q2232.18 186.291 2230.23 184.555 Q2228.31 182.819 2225.21 182.819 Q2223.01 182.819 2220.56 183.583 Q2218.13 184.347 2215.35 185.897 L2215.35 181.175 Q2218.17 180.041 2220.63 179.462 Q2223.08 178.884 2225.12 178.884 Q2230.49 178.884 2233.68 181.569 Q2236.88 184.254 2236.88 188.745 Q2236.88 190.874 2236.07 192.796 Q2235.28 194.694 2233.17 197.286 Q2232.6 197.958 2229.49 201.175 Q2226.39 204.37 2220.74 210.133 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><circle clip-path="url(#clip330)" cx="2093.21" cy="248.629" r="23.04" fill="#3da44d" fill-rule="evenodd" fill-opacity="1" stroke="#000000" stroke-opacity="1" stroke-width="5.12"/>
<path clip-path="url(#clip330)" d="M2198.85 268.316 Q2197.04 272.946 2195.33 274.358 Q2193.61 275.77 2190.74 275.77 L2187.34 275.77 L2187.34 272.205 L2189.84 272.205 Q2191.6 272.205 2192.57 271.372 Q2193.54 270.538 2194.73 267.436 L2195.49 265.492 L2185 239.983 L2189.52 239.983 L2197.62 260.261 L2205.72 239.983 L2210.23 239.983 L2198.85 268.316 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /><path clip-path="url(#clip330)" d="M2230.88 247.274 Q2234.24 247.992 2236.11 250.261 Q2238.01 252.529 2238.01 255.862 Q2238.01 260.978 2234.49 263.779 Q2230.98 266.58 2224.49 266.58 Q2222.32 266.58 2220 266.14 Q2217.71 265.723 2215.26 264.867 L2215.26 260.353 Q2217.2 261.487 2219.52 262.066 Q2221.83 262.645 2224.35 262.645 Q2228.75 262.645 2231.04 260.909 Q2233.36 259.173 2233.36 255.862 Q2233.36 252.807 2231.21 251.094 Q2229.08 249.358 2225.26 249.358 L2221.23 249.358 L2221.23 245.515 L2225.44 245.515 Q2228.89 245.515 2230.72 244.149 Q2232.55 242.761 2232.55 240.168 Q2232.55 237.506 2230.65 236.094 Q2228.78 234.659 2225.26 234.659 Q2223.34 234.659 2221.14 235.075 Q2218.94 235.492 2216.3 236.372 L2216.3 232.205 Q2218.96 231.464 2221.28 231.094 Q2223.61 230.724 2225.67 230.724 Q2231 230.724 2234.1 233.154 Q2237.2 235.562 2237.2 239.682 Q2237.2 242.552 2235.56 244.543 Q2233.91 246.511 2230.88 247.274 Z" fill="#000000" fill-rule="evenodd" fill-opacity="1" /></svg>
<h2 id="Linear-Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Linear-Principal-Component-Analysis">Linear Principal Component Analysis</a><a id="Linear-Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Principal-Component-Analysis" title="Permalink"></a></h2><p>This package uses the <a href="#MultivariateStats.PCA"><code>PCA</code></a> type to define a linear PCA model:</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.PCA" href="#MultivariateStats.PCA"><code>MultivariateStats.PCA</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Linear Principal Component Analysis</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L3-L5">source</a></section></article><p>This type comes with several methods where <span>$M$</span> be an instance of  <a href="#MultivariateStats.PCA"><code>PCA</code></a>, <span>$d$</span> be the dimension of observations, and <span>$p$</span> be the output dimension (<em>i.e</em> the dimension of the principal subspace).</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit-Union{Tuple{T}, Tuple{Type{PCA}, AbstractMatrix{T}}} where T&lt;:Real" href="#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{PCA}, AbstractMatrix{T}}} where T&lt;:Real"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(PCA, X; ...)</code></pre><p>Perform PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an <strong>observation</strong>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:auto</code>: use <code>:cov</code> when <code>d &lt; n</code> or <code>:svd</code> otherwise (<em>default</em>).</li><li><code>:cov</code>: based on covariance matrix decomposition.</li><li><code>:svd</code>: based on SVD of the input data.</li></ul></li><li><code>maxoutdim</code>: The output dimension, i.e. dimension of the transformed space (<em>min(d, nc-1)</em>)</li><li><code>pratio</code>: The ratio of variances preserved in the principal subspace (<em>0.99</em>)</li><li><code>mean</code>: The mean vector, which can be either of<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Notes:</strong></p><ul><li><p>The output dimension <code>p</code> depends on both <code>maxoutdim</code> and <code>pratio</code>, as follows. Suppose the first <code>k</code> principal components preserve at least <code>pratio</code> of the total variance, while the first <code>k-1</code> preserves less than <code>pratio</code>, then the actual output dimension will be <span>$\min(k, maxoutdim)$</span>.</p></li><li><p>This function calls <a href="#MultivariateStats.pcacov"><code>pcacov</code></a> or <a href="#MultivariateStats.pcasvd"><code>pcasvd</code></a> internally, depending on the choice of method.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L255-L280">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#StatsAPI.predict-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::PCA, x::AbstractVecOrMat{&lt;:Real})</code></pre><p>Given a PCA model <code>M</code>, transform observations <code>x</code> into principal components space, as</p><p class="math-container">\[\mathbf{y} = \mathbf{P}^T (\mathbf{x} - \boldsymbol{\mu})\]</p><p>Here, <code>x</code> can be either a vector of length <code>d</code> or a matrix where each column is an observation, and <code>\mathbf{P}</code> is the projection matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L112-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>MultivariateStats.reconstruct</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">reconstruct(M::PCA, y::AbstractVecOrMat{&lt;:Real})</code></pre><p>Given a PCA model <code>M</code>, returns a (approximately) reconstructed observations from principal components space, as</p><p class="math-container">\[\tilde{\mathbf{x}} = \mathbf{P} \mathbf{y} + \boldsymbol{\mu}\]</p><p>Here, <code>y</code> can be either a vector of length <code>p</code> or a matrix where each column gives the principal components for an observation, and <span>$\mathbf{P}$</span> is the projection matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L124-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{PCA}" href="#Base.size-Tuple{PCA}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(M)</code></pre><p>Returns a tuple with the dimensions of input (the dimension of the observation space) and output (the dimension of the principal subspace).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L28-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.mean-Tuple{PCA}" href="#Statistics.mean-Tuple{PCA}"><code>Statistics.mean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mean(M::PCA)</code></pre><p>Returns the mean vector (of length <code>d</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L36-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.projection-Tuple{PCA}" href="#MultivariateStats.projection-Tuple{PCA}"><code>MultivariateStats.projection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">projection(M::PCA)</code></pre><p>Returns the projection matrix (of size <code>(d, p)</code>). Each column of the projection matrix corresponds to a principal component. The principal components are arranged in descending order of the corresponding variances.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L43-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.var-Tuple{PCA}" href="#Statistics.var-Tuple{PCA}"><code>Statistics.var</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">var(M::PCA)</code></pre><p>Returns the total observation variance, which is equal to <code>tprincipalvar(M) + tresidualvar(M)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L87-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.principalvars-Tuple{PCA}" href="#MultivariateStats.principalvars-Tuple{PCA}"><code>MultivariateStats.principalvars</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">principalvars(M::PCA)</code></pre><p>Returns the variances of principal components.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L58-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.tprincipalvar-Tuple{PCA}" href="#MultivariateStats.tprincipalvar-Tuple{PCA}"><code>MultivariateStats.tprincipalvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tprincipalvar(M::PCA)</code></pre><p>Returns the total variance of principal components, which is equal to <code>sum(principalvars(M))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L73-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.tresidualvar-Tuple{PCA}" href="#MultivariateStats.tresidualvar-Tuple{PCA}"><code>MultivariateStats.tresidualvar</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">tresidualvar(M::PCA)</code></pre><p>Returns the total residual variance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L80-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.r2-Tuple{PCA}" href="#StatsAPI.r2-Tuple{PCA}"><code>StatsAPI.r2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">r2(M::PCA)
principalratio(M::PCA)</code></pre><p>Returns the ratio of variance preserved in the principal subspace, which is equal to <code>tprincipalvar(M) / var(M)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L94-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.loadings-Tuple{PCA}" href="#MultivariateStats.loadings-Tuple{PCA}"><code>MultivariateStats.loadings</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">loadings(M::PCA)</code></pre><p>Returns model loadings, i.e. the weights for each original variable when calculating the principal component.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L103-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvals-Tuple{PCA}" href="#LinearAlgebra.eigvals-Tuple{PCA}"><code>LinearAlgebra.eigvals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvals(M::PCA)</code></pre><p>Get the eigenvalues of the PCA model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L66-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvecs-Tuple{PCA}" href="#LinearAlgebra.eigvecs-Tuple{PCA}"><code>LinearAlgebra.eigvecs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvecs(M::PCA)</code></pre><p>Get the eigenvalues of the PCA model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L51-L55">source</a></section></article><p>Auxiliary functions:</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.pcacov" href="#MultivariateStats.pcacov"><code>MultivariateStats.pcacov</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pcacov(C, mean; ...)</code></pre><p>Compute and return a PCA model based on eigenvalue decomposition of a given covariance matrix <code>C</code>.</p><p><strong>Parameters:</strong></p><ul><li><code>C</code>: The covariance matrix of the samples.</li><li><code>mean</code>: The mean vector of original samples, which can be a vector of length <code>d</code>,          or an empty vector <code>Float64[]</code> indicating a zero mean.</li></ul><p><em>Note:</em> This function accepts two keyword arguments: <code>maxoutdim</code> and <code>pratio</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L197-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.pcasvd" href="#MultivariateStats.pcasvd"><code>MultivariateStats.pcasvd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pcasvd(Z, mean, tw; ...)</code></pre><p>Compute and return a PCA model based on singular value decomposition of a centralized sample matrix <code>Z</code>.</p><p><strong>Parameters:</strong></p><ul><li><code>Z</code>: a matrix of centralized samples.</li><li><code>mean</code>: The mean vector of the <strong>original</strong> samples, which can be a vector of length <code>d</code>,         or an empty vector <code>Float64[]</code> indicating a zero mean.</li><li><code>n</code>: a number of samples.</li></ul><p><em>Note:</em> This function accepts two keyword arguments: <code>maxoutdim</code> and <code>pratio</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L223-L235">source</a></section></article><h2 id="Kernel-Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Kernel-Principal-Component-Analysis">Kernel Principal Component Analysis</a><a id="Kernel-Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Principal-Component-Analysis" title="Permalink"></a></h2><p><a href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis&gt;">Kernel Principal Component Analysis</a> (kernel PCA) is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.</p><p>This package defines a <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a> type to represent a kernel PCA model.</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.KernelPCA" href="#MultivariateStats.KernelPCA"><code>MultivariateStats.KernelPCA</code></a> — <span class="docstring-category">Type</span></header><section><div><p>This type contains kernel PCA model parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L31-L33">source</a></section></article><p>The package provides a set of methods to access the properties of the kernel PCA model. Let <span>$M$</span> be an instance of <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a>, <span>$d$</span> be the dimension of observations, and <span>$p$</span> be the output dimension (<em>i.e</em> the dimension of the principal subspace).</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit-Union{Tuple{T}, Tuple{Type{KernelPCA}, AbstractMatrix{T}}} where T&lt;:Real" href="#StatsAPI.fit-Union{Tuple{T}, Tuple{Type{KernelPCA}, AbstractMatrix{T}}} where T&lt;:Real"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit(KernelPCA, X; ...)</code></pre><p>Perform kernel PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation.</p><p>This method returns an instance of <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>kernel</code>: The kernel function. This functions accepts two vector arguments <code>x</code> and <code>y</code>,</li></ul><p>and returns a scalar value (<em>default:</em> <code>(x,y)-&gt;x&#39;y</code>)</p><ul><li><code>solver</code>: The choice of solver:<ul><li><code>:eig</code>: uses <code>LinearAlgebra.eigen</code> (<em>default</em>)</li><li><code>:eigs</code>: uses <code>Arpack.eigs</code> (always used for sparse data)</li></ul></li><li><code>maxoutdim</code>:  Maximum output dimension (<em>default</em> <code>min(d, n)</code>)</li><li><code>inverse</code>: Whether to perform calculation for inverse transform for non-precomputed kernels (<em>default</em> <code>false</code>)</li><li><code>β</code>: Hyperparameter of the ridge regression that learns the inverse transform (<em>default</em> <code>1</code> when <code>inverse</code> is <code>true</code>).</li><li><code>tol</code>: Convergence tolerance for <code>eigs</code> solver (<em>default</em> <code>0.0</code>)</li><li><code>maxiter</code>: Maximum number of iterations for <code>eigs</code> solver (<em>default</em> <code>300</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L123-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Tuple{KernelPCA}" href="#StatsAPI.predict-Tuple{KernelPCA}"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::KernelPCA)</code></pre><p>Transform the data fitted to the model <code>M</code> to a kernel space of the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L91-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#StatsAPI.predict-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::KernelPCA, x)</code></pre><p>Transform out-of-sample transformation of <code>x</code> into a kernel space of the model <code>M</code>.</p><p>Here, <code>x</code> can be either a vector of length <code>d</code> or a matrix where each column is an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L77-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{KernelPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>MultivariateStats.reconstruct</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">reconstruct(M::KernelPCA, y)</code></pre><p>Approximately reconstruct observations, given in <code>y</code>, to the original space using the kernel PCA model <code>M</code>.</p><p>Here, <code>y</code> can be either a vector of length <code>p</code> or a matrix where each column gives the principal components for an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L99-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{KernelPCA}" href="#Base.size-Tuple{KernelPCA}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(M::KernelPCA)</code></pre><p>Returns a tuple with the input dimension <span>$d$</span>, <em>i.e</em> the dimension of the observation space, and the output dimension <span>$p$</span>, <em>i.e</em> the dimension of the principal subspace.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L44-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.projection-Tuple{KernelPCA}" href="#MultivariateStats.projection-Tuple{KernelPCA}"><code>MultivariateStats.projection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">projection(M::KernelPCA)</code></pre><p>Return the projection matrix (of size <span>$n \times p$</span>). Each column of the projection matrix corresponds to an eigenvector, and <span>$n$</span> is a number of observations.</p><p>The principal components are arranged in descending order of the corresponding eigenvalues.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L65-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvals-Tuple{KernelPCA}" href="#LinearAlgebra.eigvals-Tuple{KernelPCA}"><code>LinearAlgebra.eigvals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvals(M::KernelPCA)</code></pre><p>Return eigenvalues of the kernel matrix of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L51-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.eigvecs-Tuple{KernelPCA}" href="#LinearAlgebra.eigvecs-Tuple{KernelPCA}"><code>LinearAlgebra.eigvecs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eigvecs(M::KernelPCA)</code></pre><p>Return eigenvectors of the kernel matrix of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L58-L62">source</a></section></article><h3 id="Kernels"><a class="docs-heading-anchor" href="#Kernels">Kernels</a><a id="Kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Kernels" title="Permalink"></a></h3><p>List of the commonly used kernels:</p><table><tr><th style="text-align: right">function</th><th style="text-align: right">description</th></tr><tr><td style="text-align: right"><code>(x,y)-&gt;x&#39;y</code></td><td style="text-align: right">Linear</td></tr><tr><td style="text-align: right"><code>(x,y)-&gt;(x&#39;y+c)^d</code></td><td style="text-align: right">Polynomial</td></tr><tr><td style="text-align: right"><code>(x,y)-&gt;exp(-γ*norm(x-y)^2.0)</code></td><td style="text-align: right">Radial basis function (RBF)</td></tr></table><p>This package has a separate interface for adjusting kernel matrices.</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.KernelCenter" href="#MultivariateStats.KernelCenter"><code>MultivariateStats.KernelCenter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Center a kernel matrix</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit-Tuple{Type{MultivariateStats.KernelCenter}, AbstractMatrix{&lt;:Real}}" href="#StatsAPI.fit-Tuple{Type{MultivariateStats.KernelCenter}, AbstractMatrix{&lt;:Real}}"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Fit <code>KernelCenter</code> object</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.transform!-Tuple{MultivariateStats.KernelCenter, AbstractMatrix{&lt;:Real}}" href="#MultivariateStats.transform!-Tuple{MultivariateStats.KernelCenter, AbstractMatrix{&lt;:Real}}"><code>MultivariateStats.transform!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Center kernel matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L18">source</a></section></article><h2 id="Probabilistic-Principal-Component-Analysis"><a class="docs-heading-anchor" href="#Probabilistic-Principal-Component-Analysis">Probabilistic Principal Component Analysis</a><a id="Probabilistic-Principal-Component-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-Principal-Component-Analysis" title="Permalink"></a></h2><p><a href="https://www.microsoft.com/en-us/research/publication/probabilistic-principal-component-analysis">Probabilistic Principal Component Analysis</a> (PPCA) represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. It is expressed as the maximum likelihood solution of a probabilistic latent variable model<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>.</p><p>This package defines a <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> type to represent a probabilistic PCA model, and provides a set of methods to access the properties.</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.PPCA" href="#MultivariateStats.PPCA"><code>MultivariateStats.PPCA</code></a> — <span class="docstring-category">Type</span></header><section><div><p>This type contains probabilistic PCA model parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L3-L5">source</a></section></article><p>Let <span>$M$</span> be an instance of <a href="#MultivariateStats.PPCA"><code>PPCA</code></a>, <span>$d$</span> be the dimension of observations, and <span>$p$</span> be the output dimension (<em>i.e</em> the dimension of the principal subspace).</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit" href="#StatsAPI.fit"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit(Whitening, X::AbstractMatrix{T}; kwargs...)</code></pre><p>Estimate a whitening transform from the data given in <code>X</code>.</p><p>This function returns an instance of <a href="../whiten/#MultivariateStats.Whitening"><code>Whitening</code></a></p><p><strong>Keyword Arguments:</strong></p><ul><li><p><code>regcoef</code>: The regularization coefficient. The covariance will be regularized as follows when <code>regcoef</code> is positive <code>C + (eigmax(C) * regcoef) * eye(d)</code>. Default values is <code>zero(T)</code>.</p></li><li><p><code>dims</code>: if <code>1</code> the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if <code>2</code> the transformation calculated from the column samples. The default is <code>nothing</code>, which is equivalent to <code>dims=2</code> with a deprecation warning.</p></li><li><p><code>mean</code>: The mean vector, which can be either of:</p><ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<strong>default</strong>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Note:</strong> This function internally relies on <a href="../whiten/#MultivariateStats.cov_whitening"><code>cov_whitening</code></a> to derive the transformation <code>W</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/whiten.jl#L104-L123">source</a></section><section><div><pre><code class="language-julia hljs">fit(PCA, X; ...)</code></pre><p>Perform PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an <strong>observation</strong>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:auto</code>: use <code>:cov</code> when <code>d &lt; n</code> or <code>:svd</code> otherwise (<em>default</em>).</li><li><code>:cov</code>: based on covariance matrix decomposition.</li><li><code>:svd</code>: based on SVD of the input data.</li></ul></li><li><code>maxoutdim</code>: The output dimension, i.e. dimension of the transformed space (<em>min(d, nc-1)</em>)</li><li><code>pratio</code>: The ratio of variances preserved in the principal subspace (<em>0.99</em>)</li><li><code>mean</code>: The mean vector, which can be either of<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Notes:</strong></p><ul><li><p>The output dimension <code>p</code> depends on both <code>maxoutdim</code> and <code>pratio</code>, as follows. Suppose the first <code>k</code> principal components preserve at least <code>pratio</code> of the total variance, while the first <code>k-1</code> preserves less than <code>pratio</code>, then the actual output dimension will be <span>$\min(k, maxoutdim)$</span>.</p></li><li><p>This function calls <a href="#MultivariateStats.pcacov"><code>pcacov</code></a> or <a href="#MultivariateStats.pcasvd"><code>pcasvd</code></a> internally, depending on the choice of method.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/pca.jl#L255-L280">source</a></section><section><div><pre><code class="nohighlight hljs">fit(PPCA, X; ...)</code></pre><p>Perform probabilistic PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation. This method returns an instance of <a href="#MultivariateStats.PPCA"><code>PPCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:ml</code>: use maximum likelihood version of probabilistic PCA (<em>default</em>)</li><li><code>:em</code>: use EM version of probabilistic PCA</li><li><code>:bayes</code>: use Bayesian PCA</li></ul></li><li><code>maxoutdim</code>: Maximum output dimension (<em>default</em> <code>d-1</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li><li><code>tol</code>: Convergence tolerance (<em>default</em> <code>1.0e-6</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>1000</code>)</li></ul><p><strong>Notes:</strong> This function calls <a href="#MultivariateStats.ppcaml"><code>ppcaml</code></a>, <a href="#MultivariateStats.ppcaem"><code>ppcaem</code></a> or <a href="#MultivariateStats.bayespca"><code>bayespca</code></a> internally, depending on the choice of method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L280-L305">source</a></section><section><div><p>Fit <code>KernelCenter</code> object</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L11">source</a></section><section><div><pre><code class="language-julia hljs">fit(KernelPCA, X; ...)</code></pre><p>Perform kernel PCA over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation.</p><p>This method returns an instance of <a href="#MultivariateStats.KernelPCA"><code>KernelPCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>kernel</code>: The kernel function. This functions accepts two vector arguments <code>x</code> and <code>y</code>,</li></ul><p>and returns a scalar value (<em>default:</em> <code>(x,y)-&gt;x&#39;y</code>)</p><ul><li><code>solver</code>: The choice of solver:<ul><li><code>:eig</code>: uses <code>LinearAlgebra.eigen</code> (<em>default</em>)</li><li><code>:eigs</code>: uses <code>Arpack.eigs</code> (always used for sparse data)</li></ul></li><li><code>maxoutdim</code>:  Maximum output dimension (<em>default</em> <code>min(d, n)</code>)</li><li><code>inverse</code>: Whether to perform calculation for inverse transform for non-precomputed kernels (<em>default</em> <code>false</code>)</li><li><code>β</code>: Hyperparameter of the ridge regression that learns the inverse transform (<em>default</em> <code>1</code> when <code>inverse</code> is <code>true</code>).</li><li><code>tol</code>: Convergence tolerance for <code>eigs</code> solver (<em>default</em> <code>0.0</code>)</li><li><code>maxiter</code>: Maximum number of iterations for <code>eigs</code> solver (<em>default</em> <code>300</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/kpca.jl#L123-L144">source</a></section><section><div><pre><code class="language-julia hljs">fit(CCA, X, Y; ...)</code></pre><p>Perform CCA over the data given in matrices <code>X</code> and <code>Y</code>. Each column of <code>X</code> and <code>Y</code> is an observation.</p><p><code>X</code> and <code>Y</code> should have the same number of columns (denoted by <code>n</code> below).</p><p>This method returns an instance of <a href="../cca/#MultivariateStats.CCA"><code>CCA</code></a>.</p><p><strong>Keyword arguments:</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:cov</code>: based on covariance matrices</li><li><code>:svd</code>: based on SVD of the input data (<em>default</em>)</li></ul></li><li><code>outdim</code>: The output dimension, <em>i.e</em> dimension of the common space (<em>default</em>: <code>min(dx, dy, n)</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li></ul><p><strong>Notes:</strong> This function calls <a href="../cca/#MultivariateStats.ccacov"><code>ccacov</code></a> or <a href="../cca/#MultivariateStats.ccasvd"><code>ccasvd</code></a> internally, depending on the choice of method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/cca.jl#L283-L304">source</a></section><section><div><pre><code class="language-julia hljs">fit(MDS, X; kwargs...)</code></pre><p>Compute an embedding of <code>X</code> points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument <code>distances</code>:</p><pre><code class="nohighlight hljs">mds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)</code></pre><p>where <code>X</code> is the data matrix. Distances between pairs of columns of <code>X</code> are computed using the Euclidean norm. This is equivalent to performing PCA on <code>X</code>.</p><pre><code class="nohighlight hljs">mds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)</code></pre><p>where <code>D</code> is a symmetric matrix <code>D</code> of distances between points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/cmds.jl#L217-L231">source</a></section><section><div><pre><code class="language-julia hljs">fit(MetricMDS, X; kwargs...)</code></pre><p>Compute an embedding of <code>X</code> points by (non)metric multidimensional scaling (MDS).</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>distances</code>: The choice of input (<em>required</em>):<ul><li><code>false</code>: use <code>X</code> to calculate dissimilarity matrix using Euclidean distance</li><li><code>true</code>: use <code>X</code> input as precomputed dissimilarity symmetric matrix (distances)</li></ul></li><li><code>maxoutdim</code>: Maximum output dimension (<em>default</em> <code>d-1</code>)</li><li><code>metric</code> : a function for calculation of disparity values<ul><li><code>nothing</code>: use dissimilarity values as the disparities to perform the metric MDS (<em>default</em>)</li><li><code>isotonic</code>: converts dissimilarity values to ordinal disparities to perform non-metric MDS</li><li>any two parameter disparity transformation function, where the first parameter is a vector of proximities (i.e. dissimilarities) and the second parameter is a vector of distances, e.g. <code>(p,d)-&gt;b*p</code> for some <code>b</code> is a transformation function for <em>ratio</em> MDS.</li></ul></li><li><code>tol</code>: Convergence tolerance (<em>default</em> <code>1.0e-3</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>300</code>)</li><li><code>initial</code>: an initial reduced space point configuration<ul><li><code>nothing</code>: then an initial configuration is randomly generated (<em>default</em>)</li><li>pre-defined matrix</li></ul></li><li><code>weights</code>: a weight matrix<ul><li><code>nothing</code>: then weights are set to one, <span>$w_{ij} = 1$</span> (<em>default</em>)</li><li>pre-defined matrix</li></ul></li></ul><p><em>Note:</em> if the algorithm is unable to converge then <code>ConvergenceException</code> is thrown.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/mmds.jl#L97-L124">source</a></section><section><div><pre><code class="language-julia hljs">fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())</code></pre><p>Performs LDA given both positive and negative samples. The function accepts follwing parameters:</p><p><strong>Parameters</strong></p><ul><li><code>Xp</code>: The sample matrix of the positive class.</li><li><code>Xn</code>: The sample matrix of the negative class.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>covestimator</code>: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as <code>cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)</code>. Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/lda.jl#L128-L139">source</a></section><section><div><pre><code class="nohighlight hljs">fit(MulticlassLDA, X, y; ...)</code></pre><p>Perform multi-class LDA over a given data set <code>X</code> with corresponding labels <code>y</code> with <code>nc</code> number of classes.</p><p>This function returns the resultant multi-class LDA model as an instance of <a href="../lda/#MultivariateStats.MulticlassLDA"><code>MulticlassLDA</code></a>.</p><p><em>Parameters</em></p><ul><li><code>X</code>:   the matrix of input samples, of size <code>(d, n)</code>. Each column in <code>X</code> is an observation.</li><li><code>y</code>:   the vector of class labels, of length <code>n</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:gevd</code>: based on generalized eigenvalue decomposition (<em>default</em>).</li><li><code>:whiten</code>: first derive a whitening transform from <code>Sw</code> and then solve the problem based on eigenvalue</li></ul>decomposition of the whiten <code>Sb</code>.</li><li><code>outdim</code>: The output dimension, i.e. dimension of the transformed space <code>min(d, nc-1)</code></li><li><code>regcoef</code>: The regularization coefficient (<em>default:</em> <code>1.0e-6</code>). A positive value <code>regcoef * eigmax(Sw)</code>   is added to the diagonal of <code>Sw</code> to improve numerical stability.</li><li><code>covestimator_between</code>: Custom covariance estimator for between-class covariance (<em>default:</em> <code>SimpleCovariance()</code>).   The covariance matrix will be calculated as <code>cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#))</code>.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.</li><li><code>covestimator_within</code>:  Custom covariance estimator for within-class covariance (<em>default:</em> <code>SimpleCovariance()</code>).   The covariance matrix will be calculated as <code>cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc))</code>.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.</li></ul><p><strong>Notes:</strong></p><p>The resultant projection matrix <span>$P$</span> satisfies:</p><p class="math-container">\[\mathbf{P}^T (\mathbf{S}_w + \kappa \mathbf{I}) \mathbf{P} = \mathbf{I}\]</p><p>Here, <span>$\kappa$</span> equals <code>regcoef * eigmax(Sw)</code>. The columns of <span>$P$</span> are arranged in descending order of the corresponding generalized eigenvalues.</p><p>Note that <a href="../lda/#MultivariateStats.MulticlassLDA"><code>MulticlassLDA</code></a> does not currently support the normalized version using <span>$\mathbf{S}_w^*$</span> and <span>$\mathbf{S}_b^*$</span> (see <a href="../lda/#MultivariateStats.SubspaceLDA"><code>SubspaceLDA</code></a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/lda.jl#L286-L328">source</a></section><section><div><pre><code class="nohighlight hljs">fit(SubspaceLDA, X, y; normalize=true)</code></pre><p>Fit an subspace projection of LDA model over a given data set <code>X</code> with corresponding labels <code>y</code> using the equivalent of <span>$\mathbf{S}_w^*$</span> and <span>$\mathbf{S}_b^*$</span>`.</p><p>Note: Subspace LDA also supports the normalized version of LDA via the <code>normalize</code> keyword.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/lda.jl#L453-L460">source</a></section><section><div><pre><code class="language-julia hljs">fit(ICA, X, k; ...)</code></pre><p>Perform ICA over the data set given in <code>X</code>.</p><p><strong>Parameters:</strong> -<code>X</code>: The data matrix, of size <span>$(m, n)$</span>. Each row corresponds to a mixed signal, while each column corresponds to an observation (<em>e.g</em> all signal value at a particular time step). -<code>k</code>: The number of independent components to recover.</p><p><strong>Keyword Arguments:</strong></p><ul><li><code>alg</code>: The choice of algorithm (<em>default</em> <code>:fastica</code>)</li><li><code>fun</code>: The approx neg-entropy functor (<em>default</em> <a href="../ica/#MultivariateStats.Tanh"><code>Tanh</code></a>)</li><li><code>do_whiten</code>: Whether to perform pre-whitening (<em>default</em> <code>true</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>100</code>)</li><li><code>tol</code>: Tolerable change of <span>$W$</span> at convergence (<em>default</em> <code>1.0e-6</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li><li><code>winit</code>: Initial guess of <span>$W$</span>, which should be either of:<ul><li>empty matrix: the function will perform random initialization (<em>default</em>)</li><li>a matrix of size <span>$(k, k)$</span> (when <code>do_whiten</code>)</li><li>a matrix of size <span>$(m, k)$</span> (when <code>!do_whiten</code>)</li></ul></li></ul><p>Returns the resultant ICA model, an instance of type <a href="../ica/#MultivariateStats.ICA"><code>ICA</code></a>.</p><p><strong>Note:</strong> If <code>do_whiten</code> is <code>true</code>, the return <code>W</code> satisfies <span>$\mathbf{W}^T \mathbf{C} \mathbf{W} = \mathbf{I}$</span>, otherwise <span>$W$</span> is orthonormal, <em>i.e</em> <span>$\mathbf{W}^T \mathbf{W} = \mathbf{I}$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ica.jl#L181-L211">source</a></section><section><div><pre><code class="language-julia hljs">fit(FactorAnalysis, X; ...)</code></pre><p>Perform factor analysis over the data given in a matrix <code>X</code>. Each column of <code>X</code> is an observation. This method returns an instance of <a href="../fa/#MultivariateStats.FactorAnalysis"><code>FactorAnalysis</code></a>.</p><p><strong>Keyword arguments:</strong></p><p>Let <code>(d, n) = size(X)</code> be respectively the input dimension and the number of observations:</p><ul><li><code>method</code>: The choice of methods:<ul><li><code>:em</code>: use EM version of factor analysis</li><li><code>:cm</code>: use CM version of factor analysis (<em>default</em>)</li></ul></li><li><code>maxoutdim</code>: Maximum output dimension (<em>default</em> <code>d-1</code>)</li><li><code>mean</code>: The mean vector, which can be either of:<ul><li><code>0</code>: the input data has already been centralized</li><li><code>nothing</code>: this function will compute the mean (<em>default</em>)</li><li>a pre-computed mean vector</li></ul></li><li><code>tol</code>: Convergence tolerance (<em>default</em> <code>1.0e-6</code>)</li><li><code>maxiter</code>: Maximum number of iterations (<em>default</em> <code>1000</code>)</li><li><code>η</code>: Variance low bound (<em>default</em> <code>1.0e-6</code>)</li></ul><p><strong>Notes:</strong> This function calls <a href="../fa/#MultivariateStats.facm"><code>facm</code></a> or <a href="../fa/#MultivariateStats.faem"><code>faem</code></a> internally, depending on the choice of method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/fa.jl#L232-L256">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.size-Tuple{PPCA}" href="#Base.size-Tuple{PPCA}"><code>Base.size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">size(M::PPCA)</code></pre><p>Returns a tuple with values of the input dimension <span>$d$</span>, <em>i.e</em> the dimension of the observation space, and the output dimension <span>$p$</span>, <em>i.e</em> the dimension of the principal subspace.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L13-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.mean-Tuple{PPCA}" href="#Statistics.mean-Tuple{PPCA}"><code>Statistics.mean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mean(M::PPCA)</code></pre><p>Get the mean vector (of length <span>$d$</span>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.var-Tuple{PPCA}" href="#Statistics.var-Tuple{PPCA}"><code>Statistics.var</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">var(M::PPCA)</code></pre><p>Returns the total residual variance of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L40-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Statistics.cov-Tuple{PPCA}" href="#Statistics.cov-Tuple{PPCA}"><code>Statistics.cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cov(M::PPCA)</code></pre><p>Returns the covariance of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L54-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.projection-Tuple{PPCA}" href="#MultivariateStats.projection-Tuple{PPCA}"><code>MultivariateStats.projection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">projection(M::PPCA)</code></pre><p>Returns the projection matrix (of size <span>$(d, p)$</span>). Each column of the projection matrix corresponds to a principal component.</p><p>The principal components are arranged in descending order of the corresponding variances.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L29-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.loadings-Tuple{PPCA}" href="#MultivariateStats.loadings-Tuple{PPCA}"><code>MultivariateStats.loadings</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">loadings(M::PPCA)</code></pre><p>Returns the factor loadings matrix (of size <span>$(d, p)$</span>) of the model <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L47-L51">source</a></section></article><p>Given a probabilistic PCA model <span>$M$</span>, one can use it to transform observations into latent variables, as</p><pre><code class="language- hljs">\mathbf{z} = (\mathbf{W}^T \mathbf{W} + \sigma^2 \mathbf{I}) \mathbf{W}^T (\mathbf{x} - \boldsymbol{\mu})</code></pre><p>or use it to reconstruct (approximately) the observations from latent variables, as</p><pre><code class="language- hljs">\tilde{\mathbf{x}} = \mathbf{W} \mathbb{E}[\mathbf{z}] + \boldsymbol{\mu}</code></pre><p>Here, <span>$\mathbf{W}$</span> is the factor loadings or weight matrix.</p><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.predict-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#StatsAPI.predict-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">predict(M::PPCA, x)</code></pre><p>Transform observations <code>x</code> into latent variables. Here, <code>x</code> can be either a vector of length <code>d</code> or a matrix where each column is an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L62-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real" href="#MultivariateStats.reconstruct-Union{Tuple{T}, Tuple{PPCA, AbstractVecOrMat{T}}} where T&lt;:Real"><code>MultivariateStats.reconstruct</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">reconstruct(M::PPCA, z)</code></pre><p>Approximately reconstruct observations from the latent variable given in <code>z</code>. Here, <code>z</code> can be either a vector of length <code>p</code> or a matrix where each column gives the latent variables for an observation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L74-L80">source</a></section></article><p>Auxiliary functions:</p><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.ppcaml" href="#MultivariateStats.ppcaml"><code>MultivariateStats.ppcaml</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ppcaml(Z, mean; ...)</code></pre><p>Compute probabilistic PCA using on maximum likelihood formulation for a centralized sample matrix <code>Z</code>.</p><p><em>Parameters</em>:</p><ul><li><code>Z</code>: a centralized samples matrix</li><li><code>mean</code>: The mean vector of the <strong>original</strong> samples, which can be a vector of</li></ul><p>length <code>d</code>, or an empty vector indicating a zero mean.</p><p>Returns the resultant <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> model.</p><p><strong>Note:</strong> This function accepts two keyword arguments: <code>maxoutdim</code> and <code>tol</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L97-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.ppcaem" href="#MultivariateStats.ppcaem"><code>MultivariateStats.ppcaem</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ppcaem(S, mean, n; ...)</code></pre><p>Compute probabilistic PCA based on expectation-maximization algorithm for a given sample covariance matrix <code>S</code>.</p><p><em>Parameters</em>:</p><ul><li><code>S</code>: The sample covariance matrix.</li><li><code>mean</code>: The mean vector of original samples, which can be a vector of length <code>d</code>,</li></ul><p>or an empty vector indicating a zero mean.</p><ul><li><code>n</code>: The number of observations.</li></ul><p>Returns the resultant <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> model.</p><p><strong>Note:</strong> This function accepts three keyword arguments: <code>maxoutdim</code>, <code>tol</code>, and <code>maxiter</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L143-L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MultivariateStats.bayespca" href="#MultivariateStats.bayespca"><code>MultivariateStats.bayespca</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bayespca(S, mean, n; ...)</code></pre><p>Compute probabilistic PCA using a Bayesian algorithm for a given sample covariance matrix <code>S</code>.</p><p><em>Parameters</em>:</p><ul><li><code>S</code>: The sample covariance matrix.</li><li><code>mean</code>: The mean vector of original samples, which can be a vector of length <code>d</code>,</li></ul><p>or an empty vector indicating a zero mean.</p><ul><li><code>n</code>: The number of observations.</li></ul><p>Returns the resultant <a href="#MultivariateStats.PPCA"><code>PPCA</code></a> model.</p><p><strong>Notes:</strong></p><ul><li>This function accepts three keyword arguments: <code>maxoutdim</code>, <code>tol</code>, and <code>maxiter</code>.</li><li>Function uses the <code>maxoutdim</code> parameter as an upper boundary when it automatically</li></ul><p>determines the latent space dimensionality.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MultivariateStats.jl/blob/1c95f82f634174cfac4919a5c406908815c0d573/src/ppca.jl#L206-L223">source</a></section></article><hr/><h3 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h3><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Bishop, C. M. Pattern Recognition and Machine Learning, 2006.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../lda/">« Linear Discriminant Analysis</a><a class="docs-footer-nextpage" href="../ica/">Independent Component Analysis »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Saturday 6 August 2022 01:28">Saturday 6 August 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
